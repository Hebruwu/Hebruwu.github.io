[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog is lead as part of a machine learning class I am currently taking at Virginia Tech. In this blog I will be exploring machine learning related topics. There will be 3 posts on this website. Each post will be discussing a different machine learning related topic. Each post will include some code and visualizations. The code is executed on my laptop, so you should be able to replecate the results fairly easily without requiring an extremly powerful machine."
  },
  {
    "objectID": "posts/Regression-wine/index.html",
    "href": "posts/Regression-wine/index.html",
    "title": "Computer Learns the Beautiful Intricacies of Wine",
    "section": "",
    "text": "# Computer as a Sommelier Wine occupies an exceptional place in the fabric of human culture, serving as an enduring symbol of tradition, refinement, and celebration spanning countless millennia. Its rich history spans across continents and epochs, with deep-seated connections in both the Old World and the New. Wine’s production and consumption have long been integral to diverse societies, providing a window into the evolution of craftsmanship, agricultural practices, and culinary artistry. In its intricate spectrum of flavors and aromas, wine encapsulates the harmonious fusion of nature, culture, and human innovation, standing as a cherished and enduring emblem of our shared heritage. The cultural significance of wine has even given rise to a specific class of experts known as sommeliers, responsible for curating wine lists at restaurants and evaluating the quality of wine produced by vineyards.\nA sommelier, often regarded as a professional wine connoisseur, plays a pivotal role in selecting and presenting wines in restaurant settings, as well as assessing the quality of wines produced by various wineries. The quality of wine is dependent on measurable attributes, and if these qualities can be quantified, it stands to reason that we can develop models capable of predicting wine quality, thus making the task of currating wine somewhat simpler. However, the first step on this journey is acquiring the necessary data."
  },
  {
    "objectID": "posts/Regression-wine/index.html#data",
    "href": "posts/Regression-wine/index.html#data",
    "title": "Computer Learns the Beautiful Intricacies of Wine",
    "section": "Data",
    "text": "Data\nThe data we will analyze is sourced from UC Irvine’s website and originates from the study “Modeling wine preferences by data mining from physicochemical properties” [1]. This dataset showcases wines of the Vinho Verde variety, produced in the breathtaking Minho region of Portugal. Data collection for this dataset took place between May 2004 and February 2007.\nThis dataset comprises two separate files: one exclusively dedicated to white wines and the other specifically detailing red wines. Both files contain 11 fields that provide comprehensive information about the physical properties of the wines, complemented by an additional field that characterizes the wine’s quality. The quality of the wine was evaluated through a series of blind tests, yielding ratings ranging from 0 to 10.\nLet’s take a quick look at the dataset:\n\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nwhite_wine_df = pd.read_csv(\"winequality-white.csv\", sep=\";\")\nred_wine_df = pd.read_csv(\"winequality-red.csv\", sep=\";\")\nwhite_wine_df.head()\n\n\n\n\n\n\n\n\nfixed acidity\nvolatile acidity\ncitric acid\nresidual sugar\nchlorides\nfree sulfur dioxide\ntotal sulfur dioxide\ndensity\npH\nsulphates\nalcohol\nquality\n\n\n\n\n0\n7.0\n0.27\n0.36\n20.7\n0.045\n45.0\n170.0\n1.0010\n3.00\n0.45\n8.8\n6\n\n\n1\n6.3\n0.30\n0.34\n1.6\n0.049\n14.0\n132.0\n0.9940\n3.30\n0.49\n9.5\n6\n\n\n2\n8.1\n0.28\n0.40\n6.9\n0.050\n30.0\n97.0\n0.9951\n3.26\n0.44\n10.1\n6\n\n\n3\n7.2\n0.23\n0.32\n8.5\n0.058\n47.0\n186.0\n0.9956\n3.19\n0.40\n9.9\n6\n\n\n4\n7.2\n0.23\n0.32\n8.5\n0.058\n47.0\n186.0\n0.9956\n3.19\n0.40\n9.9\n6\n\n\n\n\n\n\n\n\nred_wine_df.head()\n\n\n\n\n\n\n\n\nfixed acidity\nvolatile acidity\ncitric acid\nresidual sugar\nchlorides\nfree sulfur dioxide\ntotal sulfur dioxide\ndensity\npH\nsulphates\nalcohol\nquality\n\n\n\n\n0\n7.4\n0.70\n0.00\n1.9\n0.076\n11.0\n34.0\n0.9978\n3.51\n0.56\n9.4\n5\n\n\n1\n7.8\n0.88\n0.00\n2.6\n0.098\n25.0\n67.0\n0.9968\n3.20\n0.68\n9.8\n5\n\n\n2\n7.8\n0.76\n0.04\n2.3\n0.092\n15.0\n54.0\n0.9970\n3.26\n0.65\n9.8\n5\n\n\n3\n11.2\n0.28\n0.56\n1.9\n0.075\n17.0\n60.0\n0.9980\n3.16\n0.58\n9.8\n6\n\n\n4\n7.4\n0.70\n0.00\n1.9\n0.076\n11.0\n34.0\n0.9978\n3.51\n0.56\n9.4\n5\n\n\n\n\n\n\n\n\nimport numpy as np\nplt.figure(figsize=(10,5))\nbin_edges = np.arange(11) - 0.5\nwhite_wine_df[\"quality\"].plot(kind=\"hist\", alpha=0.5, bins=bin_edges, edgecolor=\"k\", density=True)\nred_wine_df[\"quality\"].plot(kind=\"hist\", alpha=0.5, bins=bin_edges, edgecolor=\"k\", density=True)\nplt.legend([\"White\", \"Red\"])\nplt.xticks(range(10))\nplt.xlim([-1, 10])\n\n(-1.0, 10.0)\n\n\n\n\n\nThe quality distribution of the wines seems to approximate a normal distribution. Additionally, there is a noticeable rightward shift in the distribution of white wine quality compared to red wine, suggesting that, on average, white wine may receive higher rankings. To examine the distributions of other columns in the data—given their continuous nature—it would be more appropriate to utilize Kernel Density Estimation (KDE) instead.\n\nfor column in red_wine_df.columns:\n    if column != \"quality\":\n        plt.figure()    \n        white_wine_df[column].plot(kind=\"kde\", label=\"White\")\n        red_wine_df[column].plot(kind=\"kde\", label=\"Red\")\n        plt.legend()\n        plt.title(column)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe distributions of the independent variables related to wine quality show noticeable variations. Hence, it would be more sensible to develop separate models for each type of wine rather than trying to merge the data into a single dataframe, at least when we are dealing with simpler models."
  },
  {
    "objectID": "posts/Regression-wine/index.html#predicting-wine-quality",
    "href": "posts/Regression-wine/index.html#predicting-wine-quality",
    "title": "Computer Learns the Beautiful Intricacies of Wine",
    "section": "Predicting Wine Quality",
    "text": "Predicting Wine Quality\n\nLinear Model\nHaving reviewed the data, our next step is to construct a model that provides a reliable estimate of wine quality. Beginning with the simplest technique, let’s explore simple linear regression. To accomplish this, we’ll identify the variables that show the strongest correlation with wine quality. For visualization purposes, we’ll leverage Seaborn, a high-level API for the Matplotlib library, streamlining the process of plotting specific graphs.\n\nimport seaborn as sns\nplt.figure(figsize=(10,10))\nsns.heatmap(white_wine_df.corr(), annot=True).set(title=\"White Wine Correlation\")\nplt.show()\nplt.figure(figsize=(10,10))\nsns.heatmap(red_wine_df.corr(), annot=True).set(title=\"Red Wine Correlation\")\nplt.show()\n\n\n\n\n\n\n\nInterestingly, it seems that alcohol concentration is the variable most correlated with wine quality. However, given the relatively weak correlation, I’m not overly optimistic about the prospects of this model. Before moving forward, it’s crucial to confirm the presence of a linear relationship between the variables.\n\nplt.scatter(\"alcohol\", \"quality\", data=white_wine_df, alpha=0.01)\nplt.title(\"White Wine Alchohol Content vs. Quality\")\n\nplt.figure()\nplt.scatter(\"alcohol\", \"quality\", data=red_wine_df, alpha=0.01)\nplt.title(\"Red Wine Alchohol Content vs. Quality\")\n\nText(0.5, 1.0, 'Red Wine Alchohol Content vs. Quality')\n\n\n\n\n\n\n\n\nThe relationship between the variables doesn’t seem to follow a linear pattern. Nevertheless, for the sake of exploration, we can attempt to fit a linear model to it. To start, we’ll divide the data into a training set and a test set.\n\nfrom sklearn.model_selection import train_test_split\nX_white = white_wine_df[\"alcohol\"].to_numpy().reshape(-1, 1) # Reshaped for a single feature\ny_white = white_wine_df[\"quality\"].to_numpy().reshape(-1, 1)\nX_train_white, X_test_white, y_train_white, y_test_white = train_test_split(X_white, y_white, test_size=0.2, random_state=42)\n\nX_red = red_wine_df[\"alcohol\"].to_numpy().reshape(-1, 1)\ny_red = red_wine_df[\"quality\"].to_numpy().reshape(-1, 1)\n\nX_train_red, X_test_red, y_train_red, y_test_red = train_test_split(X_red, y_red, test_size=0.2, random_state=42)\n\n\nfrom sklearn.linear_model import LinearRegression\nwhite_linear_regression_model = LinearRegression()\nwhite_linear_regression_model.fit(X_train_white, y_train_white)\n\nred_linear_regression_model = LinearRegression()\nred_linear_regression_model.fit(X_train_red, y_train_red)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nHaving fitted the model, let’s now assess its predictive performance. In this case, we’ll evaluate its accuracy using metrics such as Mean Squared Error (MSE) and Root Mean Squared Error (RMSE).\n\nfrom sklearn.metrics import mean_squared_error\nwhite_predictions = white_linear_regression_model.predict(X_test_white)\nred_predictions = red_linear_regression_model.predict(X_test_red)\n\nwhite_error = mean_squared_error(y_test_white, white_predictions)\nred_error = mean_squared_error(y_test_red, red_predictions)\n\nprint(f\"The MSE for white wine is {white_error} and RMSE of {white_error ** 0.5}\")\nprint(f\"The MSE for red wine is {red_error} and RMSE of {red_error ** 0.5}\")\n\nThe MSE for white wine is 0.6177518759003805 and RMSE of 0.7859719307331404\nThe MSE for red wine is 0.4995281340730445 and RMSE of 0.7067730428313211\n\n\nAt first glance, the RMSE of 0.78 may seem promising. However, it’s important to consider that the data is heavily concentrated on a few data points. Let’s explore a more robust technique—multiple linear regression.\nMultiple linear regression, akin to linear regression, involves using multiple variables to model the data. The next significant variables correlating with red wine quality are volatile acidity and density in white wine. We can fit a model in a similar fashion.\nFirstly, we divide the data into a test and train set, mirroring the previous procedure. However, this time, we include “volatile acidity” as part of our predictor variables (X) for red wine and “density” for white wine.\n\nX_white = white_wine_df[[\"alcohol\", \"density\"]].to_numpy()\ny_white = white_wine_df[\"quality\"].to_numpy()\nX_train_white, X_test_white, y_train_white, y_test_white = train_test_split(X_white, y_white, test_size=0.2, random_state=42)\n\nX_red = red_wine_df[[\"alcohol\", \"volatile acidity\"]].to_numpy()\ny_red = red_wine_df[\"quality\"].to_numpy()\n\nX_train_red, X_test_red, y_train_red, y_test_red = train_test_split(X_red, y_red, test_size=0.2, random_state=42)\n\nThe sklearn linear regression model supports multiple linear regression right out of the box. Therefore, the next code block doesn’t introduce any new, ground-breaking, concepts. We simply supply it with a slightly modified X, incorporating the additional predictor variables.\n\nwhite_linear_regression_model = LinearRegression()\nwhite_linear_regression_model.fit(X_train_white, y_train_white)\n\nred_linear_regression_model = LinearRegression()\nred_linear_regression_model.fit(X_train_red, y_train_red)\n\nwhite_predictions = white_linear_regression_model.predict(X_test_white)\nred_predictions = red_linear_regression_model.predict(X_test_red)\n\nwhite_error = mean_squared_error(y_test_white, white_predictions)\nred_error = mean_squared_error(y_test_red, red_predictions)\n\nprint(f\"The MSE for white wine is {white_error} and RMSE of {white_error ** 0.5}\")\nprint(f\"The MSE for red wine is {red_error} and RMSE of {red_error ** 0.5}\")\n\nThe MSE for white wine is 0.6160746073023867 and RMSE of 0.7849042026275479\nThe MSE for red wine is 0.41775613260831734 and RMSE of 0.646340570139549\n\n\nWhen provided with more than one variable, the linear regression model demonstrated improved fitting capabilities on the red wine dataset. However, the RMSE for the white wine dataset remained unaltered, and overall, the RMSE remains relatively high. Perhaps it’s worth considering a more potent approach to address the issue.\n\n\nRandom Forrest\nThe random forest model is significantly more potent than linear regression, albeit at the expense of managing more hyperparameters and being more computationally expensive. To achieve the optimal model, we’ll need to fine-tune some of these hyperparameters through a grid search. Although computationally more intensive, this approach ensures the selection of the best hyperparameters for the specific dataset.\n\nX_white = white_wine_df.loc[:, white_wine_df.columns != \"quality\"].to_numpy()\ny_white = white_wine_df[\"quality\"].to_numpy()\nX_train_white, X_test_white, y_train_white, y_test_white = train_test_split(X_white, y_white, test_size=0.2, random_state=42)\n\nX_red = red_wine_df.loc[:, white_wine_df.columns != \"quality\"].to_numpy()\ny_red = red_wine_df[\"quality\"].to_numpy()\nX_train_red, X_test_red, y_train_red, y_test_red = train_test_split(X_red, y_red, test_size=0.2, random_state=42)\n\nHaving re-split our data, we can now perform the grid search. The parameters slated for tuning are n_estimators and max_depth. n_estimators governs the number of trees in the forest, while max_depth regulates the maximum depth of each tree.\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nparameters = {\"n_estimators\": list(range(1,14)), \n              \"max_depth\": list(range(5, 20)) + [None]} # None corresponds to no limit on the depth.\nclf_white = GridSearchCV(RandomForestRegressor(), param_grid=parameters, n_jobs=8)\nwhite_tuned_random_forrest = clf_white.fit(X_train_white, y_train_white)\n\nclf_red = GridSearchCV(RandomForestRegressor(), param_grid=parameters, n_jobs=8)\nred_tuned_random_forrest = clf_red.fit(X_train_red, y_train_red)\n\nNow that we have a model for each dataset, let’s assess their performance.\n\nwhite_predictions = white_tuned_random_forrest.predict(X_test_white)\nred_predictions = red_tuned_random_forrest.predict(X_test_red)\n\nwhite_error = mean_squared_error(y_test_white, white_predictions)\nred_error = mean_squared_error(y_test_red, red_predictions)\n\nprint(f\"The MSE for white wine is {white_error} and RMSE of {white_error ** 0.5}\")\nprint(f\"The MSE for red wine is {red_error} and RMSE of {red_error ** 0.5}\")\n\nThe MSE for white wine is 0.3725697379543533 and RMSE of 0.610384909671228\nThe MSE for red wine is 0.31858549155336 and RMSE of 0.5644337795998393\n\n\nAt the cost of higher training time, the model we’ve developed is substantially more accurate than the linear regression model. To the extent that I’m quite confident we could produce a robust model without separating the white and red wine datasets.\nLet’s combine our datasets and then split them for training.\n\ncombined_df = pd.concat([white_wine_df, red_wine_df])\nX_combined = combined_df.loc[:, combined_df.columns != \"quality\"].to_numpy()\ny_combined = combined_df[\"quality\"].to_numpy()\n\nX_train_combined, X_test_combined, y_train_combined, y_test_combined = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)\n\n\nparameters = {\"n_estimators\": list(range(1,14)), \n              \"max_depth\": list(range(5, 20)) + [None]} # None corresponds to no limit on the depth.\nclf_combined = GridSearchCV(RandomForestRegressor(), param_grid=parameters, n_jobs=8)\ncombined_tuned_random_forrest = clf_combined.fit(X_train_combined, y_train_combined)\n\n\ncombined_prediction = combined_tuned_random_forrest.predict(X_test_combined)\ncombined_error = mean_squared_error(y_test_combined, combined_prediction)\nprint(f\"The MSE for white wine is {combined_error} and RMSE of {combined_error ** 0.5}\")\n\nThe MSE for white wine is 0.3524850101553075 and RMSE of 0.5937044804911846\n\n\nEven with the combined dataset, it’s apparent that our model can effectively distinguish between white and red wine, assigning a quality score that outperforms the simple linear regression model."
  },
  {
    "objectID": "posts/Regression-wine/index.html#references",
    "href": "posts/Regression-wine/index.html#references",
    "title": "Computer Learns the Beautiful Intricacies of Wine",
    "section": "References",
    "text": "References\n[1] P. Cortez, et al. “Modeling wine preferences by data mining from physicochemical properties,” in Decis. Support Syst., vol. 47, pp. 547-553, 2009."
  },
  {
    "objectID": "posts/KNN-text-classification/index.html",
    "href": "posts/KNN-text-classification/index.html",
    "title": "KNN for Text Classification",
    "section": "",
    "text": "Taken From Mac Business Solutions\n\n\n\n\nNeural networks (NN), deep learning (DL), and natural language processing (NLP) are all the rage today, and not without reason. In 2017, Google published “Attention Is All You Need” [1]. This paper presented the transformer architecture, which would later be used in a large variety of NLP models such as Google’s BERT and OpenAI’s GPT, which use transformers within their architecture.\nTransformer architectures show great promise as they tend to be more parallelizable, require less time to train, and tend to be of higher quality [1]. However, neural networks, especially deep neural networks, tend to be computationally expensive and require large datasets to train. Thus, a team from the University of Waterloo proposed an alternative method for classifying text using a GZIP compressor and the K-Nearest Neighbor clustering algorithm.\nThe team demonstrated the viability of the method on a variety of datasets, testing the accuracy compared to non-pretrained datasets. As it turns out, their model is able to compete with the larger models and was even capable of outperforming BERT in some cases.\nIn the following blog post, I plan to try and recreate the said model on a different dataset to see how accurate the model is and to learn to apply it myself. Specifically, I want to apply the model to the most classic example of outlier detection - spam or ham!\n\n\n\nThe algorithm is simple and consists of KNN as the classification method and GZIP as the distance metric, but what does it mean?\n\n\nKNN (also known as KNN) is a simple and intuitive machine learning algorithm used for classification and regression tasks. It works by finding the K closest data points (neighbors) to a given input data point in a dataset and then classifying or predicting the target variable based on the majority class or average value of those K neighbors.\n\n\n\nGZIP is a file compression utility that is typically used for compressing files to be transfered over network or for storage. You most likely have used GZIP before in order to do that. An interesting detail of the GZIP compression is that repetative texts will be compressed into smaller sizes than texts that are not repetative. This is due to the fact that the algorithm uses Huffman coding, which replaces the most repeated sequences with shorter sequences, and LZ77, which stores references to repeated sequnces instead of using the sequences themselves (think of it like having a variable assigned to sequence and then using that variable later instead of the sequence).\n\n\n\nKNN can be used in order to classifiy different sentences. However, to classify something when using KNN we need a way to also measure the distance between two datapoints. This is where GZIP comes in. Since GZIP compresses repetative sequences into smaller sizes - similar sequences that are concatonated together will compress to smaller sizes as well. This allows us to formulate a distance metric based on how well a sequence is compressed.\nConsider an example: We have sequences x1, x2, and x3. Assume that x1 and x2 are similar sequences while x3 is less similar. Let C(x, y) be our GZIP compressor. Then it follows that: \\[\nsize(C(x1 + x2)) &lt; size(C(x1 + x3))\n\\]\nThis is the intuition behind the method. We will first begin by implementing the metric.\n\n\n\nThe most basic implementation of the metric would look like this, but the keen eyed may notice that it could be optimized.\n\nimport gzip as gz\n\n\ndef gzip_metric(x1, x2):\n    Cx1 = len(gz.compress(x1.encode()))\n    Cx2 = len(gz.compress(x2.encode()))\n\n    x1x2 = \" \".join([x1, x2])\n    Cx1x2 = len(gz.compress(x1x2.encode()))\n    ncd = (Cx1x2 - min(Cx1, Cx2)) / max(Cx1, Cx2)\n    return ncd\n\n\n\n\nNormally, we would use an already existing library such as Scikit-Learn and simply import the KNN algorithm from there (neighbors.KNeighborsClassifier). However, when trying to do that, I run into an interesting problem. Turns out that Scikit’s KNeighborsClassifier does not operate with strings and expects the user to encode them beforehand. We do not want to encode these strings since we want our GZIP metric to interact with the strings directly. Luckily, KNN is simple to implement by hand and is also implemented in the paper.\n\nimport pandas as pd\nimport numpy as np\n\n\ndef knn_classify(known_df: pd.DataFrame, unknown_sr: pd.Series, k: int = 3):\n    knowns: np.ndarray = known_df.to_numpy(copy=True)\n    unknowns: np.ndarray = unknown_sr.to_numpy(copy=True)\n    predictions = []\n    for x1 in unknowns:\n        distances_from_x1: np.array = np.array(\n            [gzip_metric(x1, x2) for x2, _ in knowns]\n        )\n        sorted_idx = np.argsort(distances_from_x1)\n        top_k_classes = knowns[sorted_idx[:k], 1]\n        values, counts = np.unique(top_k_classes, return_counts=True)\n        predicted_class = values[np.argmax(counts)]\n        predictions.append(predicted_class)\n\n    return predictions\n\n\n\n\nAs you have seen in the KNN section, x2 is computed multiple times for every x1. Therefore, when we use the gzip metric, we unecessarily compress x1 multiple times, which is time-consuming. So, instead, we could precompute that value inside our knn_function and pass the precomputed value into our metric.\n\ndef improved_gzip_metric(Cx1, x1, x2):\n    Cx2 = len(gz.compress(x2.encode()))\n    x1x2 = \" \".join([x1, x2])\n    Cx1x2 = len(gz.compress(x1x2.encode()))\n    ncd = (Cx1x2 - min(Cx1, Cx2)) / max(Cx1, Cx2)\n    return ncd\n\n\ndef improved_knn_classify(known_df: pd.DataFrame, unknown_sr: pd.Series, k: int = 3):\n    knowns: np.ndarray = known_df.to_numpy(copy=True)\n    unknowns: np.ndarray = unknown_sr.to_numpy(copy=True)\n    predictions = []\n    for x1 in unknowns:\n        Cx1 = len(gz.compress(x1.encode()))\n        distances_from_x1: np.array = np.array(\n            [improved_gzip_metric(Cx1, x1, x2) for x2, _ in knowns]\n        )\n        sorted_idx = np.argsort(distances_from_x1)\n        top_k_classes = knowns[sorted_idx[:k], 1]\n        values, counts = np.unique(top_k_classes, return_counts=True)\n        predicted_class = values[np.argmax(counts)]\n        predictions.append(predicted_class)\n\n    return predictions\n\n\n\n\n\nThe dataset I would like to look at was taken from kaggle and includes a collection of messages labeled as spam or not spam (also called ham).\nLet’s take a quick look at the data using the pandas library.\n\n\n\n# The dataset comes with a single file named train.csv. Interestingly, there is no test.csv.\ndf_spam = pd.read_csv(\"Spam_dataset/train.csv\") \ndf_spam[\"sms\"] = df_spam[\"sms\"].values.astype(\"str\")\ndf_spam.head()\n\n\n\n\n\n\n\n\nsms\nlabel\n\n\n\n\n0\nGo until jurong point, crazy.. Available only ...\n0\n\n\n1\nOk lar... Joking wif u oni...\\n\n0\n\n\n2\nFree entry in 2 a wkly comp to win FA Cup fina...\n1\n\n\n3\nU dun say so early hor... U c already then say...\n0\n\n\n4\nNah I don't think he goes to usf, he lives aro...\n0\n\n\n\n\n\n\n\nYou can see that the dataset is split into two columns. One containing the message, the second containing the label. Conviniently for us, the labels are already hot-encoded, so we do not need to go through the encoding step. Something to note, if the label is ‘0’ then the message is considered ham, if it is labled as ‘1’ the message is spam.\nLet’s quickly look at the distribution of the data.\n\ngrouped_counts = df_spam.groupby(\"label\").size()\ngrouped_counts.plot.bar(x=\"label\")\ngrouped_counts\n\nlabel\n0    4827\n1     747\ndtype: int64\n\n\n\n\n\nWe can see that the overwhelming number of labels belong to ‘ham’. When we split this data into a train and test dataset, we will need to pay special attention that both of the labels are present in both the testing and training datasets. To do that we can use sklearn’s train_test_split.\n\nfrom sklearn.model_selection import train_test_split\nX = df_spam[\"sms\"]\ny = df_spam[\"label\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\ntrain_df = pd.concat([X_train, y_train], axis=1)\ntest_df = pd.concat([X_test, y_test], axis=1)\n\nNow that the data has been split, we can use the algorithm we built to try and classify the text messages.\n\n\n\nFirst let’s try the unoptimized algorithm.\n\ny_bar_slow = knn_classify(train_df, X_test)\n\nThis cell took 3 minutes and 50 seconds to execute on my system. Now, we will run the improved implementation.\n\ny_bar = improved_knn_classify(train_df, X_test)\n\n\ny_bar == y_bar_slow\n\nTrue\n\n\nWe can see that the results are identical, yet the improved version runs faster. So, from now on, we will simply use the improved version. Let’s see how the model has performed.\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nconf_mat = confusion_matrix(y_test, y_bar)\nConfusionMatrixDisplay(conf_mat).plot()\n\nprint(\"f1:\", f1_score(y_test, y_bar))\nprint(\"accuracy:\", accuracy_score(y_test, y_bar))\n\nf1: 0.9547038327526132\naccuracy: 0.9883408071748879\n\n\n\n\n\nThese results are pretty impressive. However, this could be a result of a particularly lucky split. We will now try to perform cross validation while also experimenting with different sizes of K.\n\n\n\nFirst, need to consider the possible size of K. A good rule of thumb for KNN is to select a K that is equal to \\(\\sqrt(n)\\), where \\(n\\) is the number of datapoints in the training set. We have 4459 data points in our training set. That means that our K should be approximately \\(66\\). We could try multiple K values as well. We can try a list of \\(64, 65, 66, 67, 68\\) since these values are in the vicinity of our rule of thumb and also \\(3\\), since we have recieved good results from it.\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom multiprocess import Pool\n\nf1_scores = []\naccuracy_scores = []\nKs = [3, 64, 65, 66, 67, 68]\nskf = StratifiedKFold(n_splits=5)\nwith Pool() as pool:\n    for train_idx, test_idx in skf.split(X, y):\n        train_df = df_spam.iloc[train_idx]\n        test_df = df_spam.iloc[test_idx]\n        args = [(train_df, test_df[\"sms\"], K) for K in Ks]\n        y_bars = pool.starmap(improved_knn_classify, args)\n        accuracies = []\n        f1s = []\n        for y_bar in y_bars:\n            accuracies.append(accuracy_score(test_df[\"label\"], y_bar))\n            f1s.append(f1_score(test_df[\"label\"], y_bar))\n        \n        accuracy_scores.append(accuracies)\n        f1_scores.append(f1s)\n\nYou may notice that the above code uses a new 3rd party library names “multiprocess” (not to be confused with Python’s “multiprocessing” library). It is a fork of the “multiprocessing” built-in Python library that performs better in the Jupyter environment. We are using this library in order to be able to process multiple K values in parallel. Otherwise, this process would have taken significantly longer.\nNow, lets save our metric results as dataframes for further processing.\n\nf1_df = pd.DataFrame(f1_scores, columns=Ks)\naccuracy_df = pd.DataFrame(accuracy_scores, columns=Ks)\nprint(f1_df)\nprint(accuracy_df)\n\n         3         64        65        66        67        68\n0  0.944444  0.892989  0.897059  0.897059  0.897059  0.897059\n1  0.951389  0.897059  0.901099  0.897059  0.897059  0.897059\n2  0.936170  0.888889  0.888889  0.884758  0.884758  0.880597\n3  0.932862  0.872180  0.872180  0.867925  0.872180  0.867925\n4  0.954386  0.900369  0.900369  0.900369  0.900369  0.900369\n         3         64        65        66        67        68\n0  0.985650  0.973991  0.974888  0.974888  0.974888  0.974888\n1  0.987444  0.974888  0.975785  0.974888  0.974888  0.974888\n2  0.983857  0.973094  0.973094  0.972197  0.972197  0.971300\n3  0.982960  0.969507  0.969507  0.968610  0.969507  0.968610\n4  0.988330  0.975763  0.975763  0.975763  0.975763  0.975763\n\n\nWe can create a bar chart in order to understand the results a bit better. We will first look at the \\(F1\\) score.\n\nax = f1_df.plot.bar(rot=0, title=\"F1 Scores of Different Splits and Different K Values\")\nax.legend(loc='center left',bbox_to_anchor=(1.0, 0.5))\nax.set_xlabel(\"Split Index\")\nax.set_ylabel(\"F1 Score\")\n\nText(0, 0.5, 'F1 Score')\n\n\n\n\n\n\nax = accuracy_df.plot.bar(rot=0, title=\"Accuracy Scores of Different Splits and Different K Values\")\nax.legend(loc='center left',bbox_to_anchor=(1.0, 0.5))\nax.set_xlabel(\"Split Index\")\nax.set_ylabel(\"Accuracy Score\")\n\nText(0, 0.5, 'Accuracy Score')\n\n\n\n\n\nFrom both of these plots we can see that there is no significant difference between different splits of the dataset. Additionally, there does not appear to be a significant difference in performance of the K values that are closest to our rule of thumb. However, turns out that a K value of \\(3\\) is significantly more profitable than the other K values. We can take a look at the average score for \\(K=3\\) for each metric below:\n\nprint(\"F1 Average:\", f1_df[3].mean())\nprint(\"Accuracy Average:\", accuracy_df[3].mean())\n\nF1 Average: 0.9438503403648584\nAccuracy Average: 0.9856481310028903\n\n\n\n\n\n\n[1] A. Vaswani et al., “Attention is all you need,” arXiv.org, https://arxiv.org/abs/1706.03762. [2] Z. Jiang et al., “‘low-resource’ text classification: A parameter-free classification method with compressors,” ACL Anthology, https://aclanthology.org/2023.findings-acl.426/."
  },
  {
    "objectID": "posts/KNN-text-classification/index.html#introduction",
    "href": "posts/KNN-text-classification/index.html#introduction",
    "title": "KNN for Text Classification",
    "section": "",
    "text": "Neural networks (NN), deep learning (DL), and natural language processing (NLP) are all the rage today, and not without reason. In 2017, Google published “Attention Is All You Need” [1]. This paper presented the transformer architecture, which would later be used in a large variety of NLP models such as Google’s BERT and OpenAI’s GPT, which use transformers within their architecture.\nTransformer architectures show great promise as they tend to be more parallelizable, require less time to train, and tend to be of higher quality [1]. However, neural networks, especially deep neural networks, tend to be computationally expensive and require large datasets to train. Thus, a team from the University of Waterloo proposed an alternative method for classifying text using a GZIP compressor and the K-Nearest Neighbor clustering algorithm.\nThe team demonstrated the viability of the method on a variety of datasets, testing the accuracy compared to non-pretrained datasets. As it turns out, their model is able to compete with the larger models and was even capable of outperforming BERT in some cases.\nIn the following blog post, I plan to try and recreate the said model on a different dataset to see how accurate the model is and to learn to apply it myself. Specifically, I want to apply the model to the most classic example of outlier detection - spam or ham!"
  },
  {
    "objectID": "posts/KNN-text-classification/index.html#implementing-the-algorithm",
    "href": "posts/KNN-text-classification/index.html#implementing-the-algorithm",
    "title": "KNN for Text Classification",
    "section": "",
    "text": "The algorithm is simple and consists of KNN as the classification method and GZIP as the distance metric, but what does it mean?\n\n\nKNN (also known as KNN) is a simple and intuitive machine learning algorithm used for classification and regression tasks. It works by finding the K closest data points (neighbors) to a given input data point in a dataset and then classifying or predicting the target variable based on the majority class or average value of those K neighbors.\n\n\n\nGZIP is a file compression utility that is typically used for compressing files to be transfered over network or for storage. You most likely have used GZIP before in order to do that. An interesting detail of the GZIP compression is that repetative texts will be compressed into smaller sizes than texts that are not repetative. This is due to the fact that the algorithm uses Huffman coding, which replaces the most repeated sequences with shorter sequences, and LZ77, which stores references to repeated sequnces instead of using the sequences themselves (think of it like having a variable assigned to sequence and then using that variable later instead of the sequence).\n\n\n\nKNN can be used in order to classifiy different sentences. However, to classify something when using KNN we need a way to also measure the distance between two datapoints. This is where GZIP comes in. Since GZIP compresses repetative sequences into smaller sizes - similar sequences that are concatonated together will compress to smaller sizes as well. This allows us to formulate a distance metric based on how well a sequence is compressed.\nConsider an example: We have sequences x1, x2, and x3. Assume that x1 and x2 are similar sequences while x3 is less similar. Let C(x, y) be our GZIP compressor. Then it follows that: \\[\nsize(C(x1 + x2)) &lt; size(C(x1 + x3))\n\\]\nThis is the intuition behind the method. We will first begin by implementing the metric.\n\n\n\nThe most basic implementation of the metric would look like this, but the keen eyed may notice that it could be optimized.\n\nimport gzip as gz\n\n\ndef gzip_metric(x1, x2):\n    Cx1 = len(gz.compress(x1.encode()))\n    Cx2 = len(gz.compress(x2.encode()))\n\n    x1x2 = \" \".join([x1, x2])\n    Cx1x2 = len(gz.compress(x1x2.encode()))\n    ncd = (Cx1x2 - min(Cx1, Cx2)) / max(Cx1, Cx2)\n    return ncd\n\n\n\n\nNormally, we would use an already existing library such as Scikit-Learn and simply import the KNN algorithm from there (neighbors.KNeighborsClassifier). However, when trying to do that, I run into an interesting problem. Turns out that Scikit’s KNeighborsClassifier does not operate with strings and expects the user to encode them beforehand. We do not want to encode these strings since we want our GZIP metric to interact with the strings directly. Luckily, KNN is simple to implement by hand and is also implemented in the paper.\n\nimport pandas as pd\nimport numpy as np\n\n\ndef knn_classify(known_df: pd.DataFrame, unknown_sr: pd.Series, k: int = 3):\n    knowns: np.ndarray = known_df.to_numpy(copy=True)\n    unknowns: np.ndarray = unknown_sr.to_numpy(copy=True)\n    predictions = []\n    for x1 in unknowns:\n        distances_from_x1: np.array = np.array(\n            [gzip_metric(x1, x2) for x2, _ in knowns]\n        )\n        sorted_idx = np.argsort(distances_from_x1)\n        top_k_classes = knowns[sorted_idx[:k], 1]\n        values, counts = np.unique(top_k_classes, return_counts=True)\n        predicted_class = values[np.argmax(counts)]\n        predictions.append(predicted_class)\n\n    return predictions\n\n\n\n\nAs you have seen in the KNN section, x2 is computed multiple times for every x1. Therefore, when we use the gzip metric, we unecessarily compress x1 multiple times, which is time-consuming. So, instead, we could precompute that value inside our knn_function and pass the precomputed value into our metric.\n\ndef improved_gzip_metric(Cx1, x1, x2):\n    Cx2 = len(gz.compress(x2.encode()))\n    x1x2 = \" \".join([x1, x2])\n    Cx1x2 = len(gz.compress(x1x2.encode()))\n    ncd = (Cx1x2 - min(Cx1, Cx2)) / max(Cx1, Cx2)\n    return ncd\n\n\ndef improved_knn_classify(known_df: pd.DataFrame, unknown_sr: pd.Series, k: int = 3):\n    knowns: np.ndarray = known_df.to_numpy(copy=True)\n    unknowns: np.ndarray = unknown_sr.to_numpy(copy=True)\n    predictions = []\n    for x1 in unknowns:\n        Cx1 = len(gz.compress(x1.encode()))\n        distances_from_x1: np.array = np.array(\n            [improved_gzip_metric(Cx1, x1, x2) for x2, _ in knowns]\n        )\n        sorted_idx = np.argsort(distances_from_x1)\n        top_k_classes = knowns[sorted_idx[:k], 1]\n        values, counts = np.unique(top_k_classes, return_counts=True)\n        predicted_class = values[np.argmax(counts)]\n        predictions.append(predicted_class)\n\n    return predictions"
  },
  {
    "objectID": "posts/KNN-text-classification/index.html#spam-or-ham",
    "href": "posts/KNN-text-classification/index.html#spam-or-ham",
    "title": "KNN for Text Classification",
    "section": "",
    "text": "The dataset I would like to look at was taken from kaggle and includes a collection of messages labeled as spam or not spam (also called ham).\nLet’s take a quick look at the data using the pandas library.\n\n\n\n# The dataset comes with a single file named train.csv. Interestingly, there is no test.csv.\ndf_spam = pd.read_csv(\"Spam_dataset/train.csv\") \ndf_spam[\"sms\"] = df_spam[\"sms\"].values.astype(\"str\")\ndf_spam.head()\n\n\n\n\n\n\n\n\nsms\nlabel\n\n\n\n\n0\nGo until jurong point, crazy.. Available only ...\n0\n\n\n1\nOk lar... Joking wif u oni...\\n\n0\n\n\n2\nFree entry in 2 a wkly comp to win FA Cup fina...\n1\n\n\n3\nU dun say so early hor... U c already then say...\n0\n\n\n4\nNah I don't think he goes to usf, he lives aro...\n0\n\n\n\n\n\n\n\nYou can see that the dataset is split into two columns. One containing the message, the second containing the label. Conviniently for us, the labels are already hot-encoded, so we do not need to go through the encoding step. Something to note, if the label is ‘0’ then the message is considered ham, if it is labled as ‘1’ the message is spam.\nLet’s quickly look at the distribution of the data.\n\ngrouped_counts = df_spam.groupby(\"label\").size()\ngrouped_counts.plot.bar(x=\"label\")\ngrouped_counts\n\nlabel\n0    4827\n1     747\ndtype: int64\n\n\n\n\n\nWe can see that the overwhelming number of labels belong to ‘ham’. When we split this data into a train and test dataset, we will need to pay special attention that both of the labels are present in both the testing and training datasets. To do that we can use sklearn’s train_test_split.\n\nfrom sklearn.model_selection import train_test_split\nX = df_spam[\"sms\"]\ny = df_spam[\"label\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\ntrain_df = pd.concat([X_train, y_train], axis=1)\ntest_df = pd.concat([X_test, y_test], axis=1)\n\nNow that the data has been split, we can use the algorithm we built to try and classify the text messages.\n\n\n\nFirst let’s try the unoptimized algorithm.\n\ny_bar_slow = knn_classify(train_df, X_test)\n\nThis cell took 3 minutes and 50 seconds to execute on my system. Now, we will run the improved implementation.\n\ny_bar = improved_knn_classify(train_df, X_test)\n\n\ny_bar == y_bar_slow\n\nTrue\n\n\nWe can see that the results are identical, yet the improved version runs faster. So, from now on, we will simply use the improved version. Let’s see how the model has performed.\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nconf_mat = confusion_matrix(y_test, y_bar)\nConfusionMatrixDisplay(conf_mat).plot()\n\nprint(\"f1:\", f1_score(y_test, y_bar))\nprint(\"accuracy:\", accuracy_score(y_test, y_bar))\n\nf1: 0.9547038327526132\naccuracy: 0.9883408071748879\n\n\n\n\n\nThese results are pretty impressive. However, this could be a result of a particularly lucky split. We will now try to perform cross validation while also experimenting with different sizes of K.\n\n\n\nFirst, need to consider the possible size of K. A good rule of thumb for KNN is to select a K that is equal to \\(\\sqrt(n)\\), where \\(n\\) is the number of datapoints in the training set. We have 4459 data points in our training set. That means that our K should be approximately \\(66\\). We could try multiple K values as well. We can try a list of \\(64, 65, 66, 67, 68\\) since these values are in the vicinity of our rule of thumb and also \\(3\\), since we have recieved good results from it.\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom multiprocess import Pool\n\nf1_scores = []\naccuracy_scores = []\nKs = [3, 64, 65, 66, 67, 68]\nskf = StratifiedKFold(n_splits=5)\nwith Pool() as pool:\n    for train_idx, test_idx in skf.split(X, y):\n        train_df = df_spam.iloc[train_idx]\n        test_df = df_spam.iloc[test_idx]\n        args = [(train_df, test_df[\"sms\"], K) for K in Ks]\n        y_bars = pool.starmap(improved_knn_classify, args)\n        accuracies = []\n        f1s = []\n        for y_bar in y_bars:\n            accuracies.append(accuracy_score(test_df[\"label\"], y_bar))\n            f1s.append(f1_score(test_df[\"label\"], y_bar))\n        \n        accuracy_scores.append(accuracies)\n        f1_scores.append(f1s)\n\nYou may notice that the above code uses a new 3rd party library names “multiprocess” (not to be confused with Python’s “multiprocessing” library). It is a fork of the “multiprocessing” built-in Python library that performs better in the Jupyter environment. We are using this library in order to be able to process multiple K values in parallel. Otherwise, this process would have taken significantly longer.\nNow, lets save our metric results as dataframes for further processing.\n\nf1_df = pd.DataFrame(f1_scores, columns=Ks)\naccuracy_df = pd.DataFrame(accuracy_scores, columns=Ks)\nprint(f1_df)\nprint(accuracy_df)\n\n         3         64        65        66        67        68\n0  0.944444  0.892989  0.897059  0.897059  0.897059  0.897059\n1  0.951389  0.897059  0.901099  0.897059  0.897059  0.897059\n2  0.936170  0.888889  0.888889  0.884758  0.884758  0.880597\n3  0.932862  0.872180  0.872180  0.867925  0.872180  0.867925\n4  0.954386  0.900369  0.900369  0.900369  0.900369  0.900369\n         3         64        65        66        67        68\n0  0.985650  0.973991  0.974888  0.974888  0.974888  0.974888\n1  0.987444  0.974888  0.975785  0.974888  0.974888  0.974888\n2  0.983857  0.973094  0.973094  0.972197  0.972197  0.971300\n3  0.982960  0.969507  0.969507  0.968610  0.969507  0.968610\n4  0.988330  0.975763  0.975763  0.975763  0.975763  0.975763\n\n\nWe can create a bar chart in order to understand the results a bit better. We will first look at the \\(F1\\) score.\n\nax = f1_df.plot.bar(rot=0, title=\"F1 Scores of Different Splits and Different K Values\")\nax.legend(loc='center left',bbox_to_anchor=(1.0, 0.5))\nax.set_xlabel(\"Split Index\")\nax.set_ylabel(\"F1 Score\")\n\nText(0, 0.5, 'F1 Score')\n\n\n\n\n\n\nax = accuracy_df.plot.bar(rot=0, title=\"Accuracy Scores of Different Splits and Different K Values\")\nax.legend(loc='center left',bbox_to_anchor=(1.0, 0.5))\nax.set_xlabel(\"Split Index\")\nax.set_ylabel(\"Accuracy Score\")\n\nText(0, 0.5, 'Accuracy Score')\n\n\n\n\n\nFrom both of these plots we can see that there is no significant difference between different splits of the dataset. Additionally, there does not appear to be a significant difference in performance of the K values that are closest to our rule of thumb. However, turns out that a K value of \\(3\\) is significantly more profitable than the other K values. We can take a look at the average score for \\(K=3\\) for each metric below:\n\nprint(\"F1 Average:\", f1_df[3].mean())\nprint(\"Accuracy Average:\", accuracy_df[3].mean())\n\nF1 Average: 0.9438503403648584\nAccuracy Average: 0.9856481310028903"
  },
  {
    "objectID": "posts/KNN-text-classification/index.html#references",
    "href": "posts/KNN-text-classification/index.html#references",
    "title": "KNN for Text Classification",
    "section": "",
    "text": "[1] A. Vaswani et al., “Attention is all you need,” arXiv.org, https://arxiv.org/abs/1706.03762. [2] Z. Jiang et al., “‘low-resource’ text classification: A parameter-free classification method with compressors,” ACL Anthology, https://aclanthology.org/2023.findings-acl.426/."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rock with Lightning",
    "section": "",
    "text": "Welcome to Rock With Lightning\nHello there! Odds are you are here because you are either my professor, or a TA for CS5805, in which case hello and welcome. You will be able to find the blog posts right below the welcome section.\nOn the off chance that you are neither, how did you get here? Not that you are unwelcome. Just questioning the series of events that brought you on a Github page lead by a random college student. You should know, this is a blog that I lead for my machine learning class as part of a class assignment. The posts that you will see here will primary deal with machine learning related topics that I find interesting either inside or outside of class. If this is the sort of content you are interested in, welcome!\n\n\nPosts\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nKNN for Text Classification\n\n\n\n\n\n\n\nKNN\n\n\nNLP\n\n\nOutlier Prediction\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2023\n\n\nDaniel Sabanov\n\n\n\n\n\n\n  \n\n\n\n\nComputer Learns the Beautiful Intricacies of Wine\n\n\n\n\n\n\n\nRegression\n\n\nRandom Forest\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2023\n\n\nDaniel Sabanov\n\n\n\n\n\n\nNo matching items"
  }
]