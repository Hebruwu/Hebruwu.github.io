{
  "hash": "9a2b0161c25f5b0c9949871c7a0a0892",
  "result": {
    "markdown": "---\ntitle: \"Naive Bayes with Diabetes Dataset\"\nauthor: \"Daniel Sabanov\"\ndate: \"2023-12-09\"\ncategories: [Naive Bayes]\n---\n\n# Learning About Probabilities and Related ML Methods\n\nIn this blog we will be discussing Naive Bayes and Logistic Regression.\nThe reason for such a selection of topics is the comonality that both of these algorithms share.\nBoth of these algorithms are probabilistic classifiers.\nThis means that both of these methods can output not only their prediction for an outcome, but also the probability of the outcome.\nWe will see the math behind how these models go about doing it as we progress through the blog.\nFirst, we should discuss the data.\n\n## Diabetes Dataset\nThe current version of the diabetes dataset was taken from [kaggle](https://www.kaggle.com/datasets/mathchi/diabetes-data-set/data).\nHowever, it is only a subset of the full dataset owned by the National Institute of Diabetes and Digestive and Kidney Diseases.\nThis particular subset deals with specifically with women over the age of 21 of the Pima Indian heritage.\nThe dataset contains the following columns:\n\n* `Pregnancies` - the number of pregnancies the woman has gone through.\n* `Glucose` - Glucose level.\n* `BloodPressure` - Diastolic blood pressure in mm Hg.\n* `SkinThickness` - Triceps skin fold thickness in mm\n* `Insulin` - 2-Hour serum insulin test result in mu U/ml \n* `BMI` - Body mass index\n* `DiabetesPedigreeFunction` - Diabetes pedigree function \n* `Age` - The age\n* `Outcome` - Outcome where 1 indicates a positive test result.\n\n\nAs usual, we will be loading the dataset through pandas.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\ndata = pd.read_csv('diabetes.csv')\ndata.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=31}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pregnancies</th>\n      <th>Glucose</th>\n      <th>BloodPressure</th>\n      <th>SkinThickness</th>\n      <th>Insulin</th>\n      <th>BMI</th>\n      <th>DiabetesPedigreeFunction</th>\n      <th>Age</th>\n      <th>Outcome</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>148</td>\n      <td>72</td>\n      <td>35</td>\n      <td>0</td>\n      <td>33.6</td>\n      <td>0.627</td>\n      <td>50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>85</td>\n      <td>66</td>\n      <td>29</td>\n      <td>0</td>\n      <td>26.6</td>\n      <td>0.351</td>\n      <td>31</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8</td>\n      <td>183</td>\n      <td>64</td>\n      <td>0</td>\n      <td>0</td>\n      <td>23.3</td>\n      <td>0.672</td>\n      <td>32</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>89</td>\n      <td>66</td>\n      <td>23</td>\n      <td>94</td>\n      <td>28.1</td>\n      <td>0.167</td>\n      <td>21</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>137</td>\n      <td>40</td>\n      <td>35</td>\n      <td>168</td>\n      <td>43.1</td>\n      <td>2.288</td>\n      <td>33</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nLet's take a quick look at the dataset information to undestand the column compostion.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndata.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 768 entries, 0 to 767\nData columns (total 9 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   Pregnancies               768 non-null    int64  \n 1   Glucose                   768 non-null    int64  \n 2   BloodPressure             768 non-null    int64  \n 3   SkinThickness             768 non-null    int64  \n 4   Insulin                   768 non-null    int64  \n 5   BMI                       768 non-null    float64\n 6   DiabetesPedigreeFunction  768 non-null    float64\n 7   Age                       768 non-null    int64  \n 8   Outcome                   768 non-null    int64  \ndtypes: float64(2), int64(7)\nmemory usage: 54.1 KB\n```\n:::\n:::\n\n\nEvery single column in the dataset is numeric.\nLet us take a look at the distribution of the data in each column.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.color_palette(\"rocket\", as_cmap=True)\nfor col in data.columns:\n    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15,7))\n    ax1.boxplot(data[col])\n    sns.kdeplot(data[col], ax=ax2)\n    plt.title(col)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=1174 height=597}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-2.png){width=1170 height=597}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-3.png){width=1170 height=597}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-4.png){width=1170 height=597}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-5.png){width=1170 height=597}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-6.png){width=1161 height=597}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-7.png){width=1166 height=597}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-8.png){width=1161 height=597}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-9.png){width=1166 height=597}\n:::\n:::\n\n\nWe can see that certain values here were imputed as some of the values can never be equal to 0 in the real world.\nFor example, neither BMI or blood preassure can be equal to 0, yet we do see some values in the columns that are equal to 0.\nWe could impute these values using a mean, median, or perhaps even KNN-imputer in order to try and improve the performance of the model.\nYet surprisingly, when I tried to do so, the models performed worse when I tried to impute these value.\nSo, they will stay as they are.\nNext, we will look at the distributions of the data per outcome.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfor col in data.columns:\n    if col == \"Outcome\":\n        continue\n    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15,7))\n    positives = data[\"Outcome\"] == 1\n    negatives = data[\"Outcome\"] == 0\n    ax1.boxplot(data[col].loc[positives], positions=[1])\n    ax1.boxplot(data[col].loc[negatives], positions=[2])\n    ax1.set_xticklabels([\"Positive\", \"Negative\"])\n    ax1.set_title(f\"{col} Boxplot\")\n    sns.kdeplot(data[col].loc[positives], ax=ax2, label=\"Positive\")\n    sns.kdeplot(data[col].loc[negatives], ax=ax2, label=\"Negative\")\n    plt.legend(loc=\"best\")\n    plt.title(f\"{col} KDE\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=1174 height=597}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-2.png){width=1170 height=597}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-3.png){width=1170 height=597}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-4.png){width=1172 height=597}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-5.png){width=1171 height=597}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-6.png){width=1161 height=597}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-7.png){width=1166 height=597}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-8.png){width=1161 height=597}\n:::\n:::\n\n\nInrestingly, the distributions for both the blood pressure and skin thickness appear to be very similar to each other regardless of the outcome.\nWe could experiment and try to remove these variables from the model down the line and see if we are able to achieve better performance.\nFor now, let's check if we can spot anything interesting happening when we project the data into 2 dimensions.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\n\ny = data[\"Outcome\"]\nX = data.drop(\"Outcome\", axis=1)\n\npipeline = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"pca\", PCA(n_components=2)),\n])\n\npca_data = pd.DataFrame(\n    pipeline.fit_transform(X),\n    columns=[\"PC1\", \"PC2\"],\n    index=data.index,\n)\n\nplt.scatter(pca_data[\"PC1\"], pca_data[\"PC2\"], c=y, cmap=\"viridis\")\nplt.title(\"Projection of the Diabetes Data\")\n```\n\n::: {.cell-output .cell-output-display execution_count=35}\n```\nText(0.5, 1.0, 'Projection of the Diabetes Data')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-2.png){width=569 height=431}\n:::\n:::\n\n\nUnfortunatley, the difference between the two distributions is a bit difficult to spot in these two dimension.\n\n## Training Naive Bayes\nWe will now proceed to train and test the naive bayes model.\nAs usual, we should split the data into train and test sets.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n```\n:::\n\n\nAlright, now let us train the model, but first - some theory.\n\nAs mentioned above, naive bayes is a probabilistic classifier, so it should be able to output the probabilities of each outcome along with its prediction.\n\nIt does so by computing $P(Y | X)$ where $Y$ represents the classified variable and X represents the feature variables.\nAccording to Bayes theorem (that's where the method get's its name):\n$$\nP(Y | X) = \\frac{P(X | Y) P(Y)}{P(X)}\n$$\nAlthough this equation looks simple, it becomes very difficult to compute as $X$ grows larger.\nThis is because we have to calculate $P(X | Y)$.\nFor example, assume X consists of two features $X_1$ and $X_2$.\nThen, we have to calculate \n$$\nP(Y | X_1, X_2) = \\frac{P(X_1, X_2 | Y) P(Y)}{P(X_1, X_2)}\n$$\nSo, now we need data where all the three variables ($X_1$, $X_2$, $Y$) interact.\nHowever, naive bayes makes a very crucial assumption.\nIt assumes that the variables are independent.\nThus, our equation is simplified to \n$$\nP(Y | X_1, X_2) = P(X_1 | Y) P(X_2 | Y)\n$$\nNow, we no longer need the data on how $X_1$ and $X_2$ interact with each other and only need to know how they interact with $Y$.\nThis means that we need significantly less data and the computation becomes much cheaper.\n\nYou may be concerned that the assumption of independnce is flawed since variables in most data are going to have some degree of dependence on each other.\nThat is a good concern to have.\nHowever, naive bayes generally tends to perform well even when that assumption is violated.\n\nSo, when given observations $X_1, X_2, ..., X_n$, naive baise calculates the probabilities for all the possible outcomes $Y$ and then selects the outcome with the highest probability. \nThis is how we both get a probability estimate and a prediction. \n\nNow, we are ready to train the model.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.metrics import ConfusionMatrixDisplay, f1_score, accuracy_score\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(X_train, y_train)\nprediction = nb.predict(X_test)\ncmd = ConfusionMatrixDisplay.from_estimator(nb, X_test, y_test)\nprint(\"Accuracy: \", accuracy_score(prediction, y_test))\nprint(\"F1 score: \", f1_score(prediction, y_test))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy:  0.7662337662337663\nF1 score:  0.6842105263157895\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-2.png){width=496 height=429}\n:::\n:::\n\n\nWe can see that the model struggles a little most likely due to the inbalance in the data.\nHowever, this is a fairly good performance for such a simple model.\nNow, let's look how we can attein the prediction probabilities.\nWe will work with a single sample, but the approach can also be applied to multiple samples.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nx1 = X_test.iloc[0]\nx1\n```\n\n::: {.cell-output .cell-output-display execution_count=38}\n```\nPregnancies                   6.00\nGlucose                      98.00\nBloodPressure                58.00\nSkinThickness                33.00\nInsulin                     190.00\nBMI                          34.00\nDiabetesPedigreeFunction      0.43\nAge                          43.00\nName: 668, dtype: float64\n```\n:::\n:::\n\n\nThis is our input.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nx1 = x1.to_numpy().reshape(1, -1)\nprobs = nb.predict_proba(x1)[0]\nprobs\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/danielsabanov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but GaussianNB was fitted with feature names\n  warnings.warn(\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=39}\n```\narray([0.73024358, 0.26975642])\n```\n:::\n:::\n\n\nSince the largest probability is 0.73 and corresponds to the first class, we know that our prediction is \"0\". \nThus, this specific patient is predicted to not have diabetes.\nLet's see what was the true outcome.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\ny_test.iloc[0]\n```\n\n::: {.cell-output .cell-output-display execution_count=40}\n```\n0\n```\n:::\n:::\n\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(max_iter=1000)\nlr.fit(X_train, y_train)\ncmd = ConfusionMatrixDisplay.from_estimator(lr, X_test, y_test)\nprediction = lr.predict(X_test)\nprint(\"Accuracy: \", accuracy_score(prediction, y_test))\nprint(\"F1 score: \", f1_score(prediction, y_test))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy:  0.7467532467532467\nF1 score:  0.6548672566371682\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-2.png){width=496 height=429}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}