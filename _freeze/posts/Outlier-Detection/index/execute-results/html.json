{
  "hash": "362cd016f1e2b5b14d8368e4b121693d",
  "result": {
    "markdown": "---\ntitle: \"Machine Learning and Fraud\"\nauthor: \"Daniel Sabanov\"\ndate: \"2023-12-11\"\ncategories: [Anomally Detection, Outlier Detection]\n---\n\n# Detecting Fraudulant Transavtions\n\n![Taken from https://blog.hubspot.com/sales/ach-vs-credit-card](image.webp)\n\nCredit card fraud is a serious issue that many credit card companies and banks are required to deal with in order to guarantee their customers' financial safety.\nHowever, detecting credit card fraud is difficult. \nWhen training an ML model one wants to have a good sample size that represents all the possible outcomes, but it is difficult to collect the required ammount of data in order to have a good representative sample of both fraudulant and non-fraudulant transactions as fraudulant transactions are much less common.\nIn this cases, Data Scientists and ML Engineers need to consider a different appraoch to building models to detect bad transactions.\nLuckily, there are various ML techniques that can help with the task.\nThese thechniques sit under the umbrella of anomally detection.\n\nIn the following blog, we will be showing an application of two annomally detection techniques on the credit card fraud dataset.\n\n## Dataset\nThe [dataset](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud) contains data from over a quarter of a million transactions made by credit cards in September of 2013 made by European cardholders.\nThe dataset was normalized and transformed via PCA with its fields also being encrypted in order to conceal the personal information about the cardholders.\nOut of a quarter of a million transactions, only 492 transactions were fraudulant.\nThis is a highly imbalanced dataset, and more common machine learning techniques will struggle with reliably being able to detect the bad transactions.\n\nLet us load the dataset.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\ndata = pd.read_csv(\"creditcard.csv\")\ndata.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>...</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>...</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>...</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>...</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>...</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 31 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom matplotlib import pyplot as plt\ny = data[\"Class\"]\nX = data.drop(columns = [\"Class\"])\nfor column in X.columns:\n    fig1, ax1 = plt.subplots()\n    data[column].plot(kind=\"kde\", title=column, ax=ax1)\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=576 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-2.png){width=597 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-3.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-4.png){width=597 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-5.png){width=602 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-6.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-7.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-8.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-9.png){width=595 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-10.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-11.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-12.png){width=597 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-13.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-14.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-15.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-16.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-17.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-18.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-19.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-20.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-21.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-22.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-23.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-24.png){width=597 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-25.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-26.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-27.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-28.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-29.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-30.png){width=617 height=431}\n:::\n:::\n\n\nWe can see that the dat ain most columns appears to be normally distributed, with a few columns appearing to be multimodal.\n\nWe can also look at the distributions of the fraudulant transactions and the non-fraudulant transactions together.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom matplotlib import pyplot as plt\nfor column in X.columns:\n    fig1, ax1 = plt.subplots()\n    data[column].loc[data[\"Class\"] == 0].plot(kind=\"kde\", title=column, ax=ax1)\n    data[column].loc[data[\"Class\"] == 1].plot(kind=\"kde\", title=column, ax=ax1)\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=576 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-2.png){width=597 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-3.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-4.png){width=597 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-5.png){width=602 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-6.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-7.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-8.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-9.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-10.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-11.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-12.png){width=599 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-13.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-14.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-15.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-16.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-17.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-18.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-19.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-20.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-21.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-22.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-23.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-24.png){width=597 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-25.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-26.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-27.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-28.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-29.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-30.png){width=617 height=431}\n:::\n:::\n\n\nWe see that the distributions of the fraudulant transactions and the non-fraudlant transactions differ from one another.\n\nWe can also look at the two dimentional projection of the data.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\npipeline = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"pca\", PCA(n_components=2)),\n])\n\npca_data = pd.DataFrame(\n    pipeline.fit_transform(X),\n    columns=[\"PC1\", \"PC2\"],\n    index=data.index,\n)\n\nplt.scatter(pca_data[\"PC1\"], pca_data[\"PC2\"], c=y, cmap=\"viridis\", alpha=0.1)\nplt.title(\"Projection of the Transaction Data\")\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nText(0.5, 1.0, 'Projection of the Transaction Data')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-2.png){width=569 height=431}\n:::\n:::\n\n\nHere, it is a bit difficult to tell appart the fraudlant and non-fraudulant transactions as the fraudulant transactions are blended in with the non-fraudulant ones. Let us try to build a model in order to differentiate between the two.\n\n## Training the Models\n\n### First Appraoch\nFor the first approach we will be using local outlier factor in order to detect transactions that would be considered outliers. \nThis method works by examining the the surrounding neighbors of the datapoint and seeing if the current datapoints deviates significantly from its neighbors.\nWe can ran through the model fairly easily using the Scikit-Learn library.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.neighbors import LocalOutlierFactor\nloc = LocalOutlierFactor()\nprediction = loc.fit_predict(X)\n```\n:::\n\n\nNote that we did not need to train the model.\nThe model is unsupervised and tries to learn from the data by itself. \n\nIt is also important to note that the model outputs are integers $1$ and $-1$.\n$1$ indicates that a specific datapoint is not an outlier, while $-1$ indicates that the datapoint is in fact an outlier.\n\nIn order to be able to diagnose the model we want to convert our dataset to the same scheme that our model uses. So, we convert $0$'s into $1$'s and $1$'s into $-1$'s.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ny.replace(1, -1, inplace=True)\ny.replace(0, 1, inplace=True)\n```\n:::\n\n\nNow, we can examine the model performance:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, f1_score\n\nconfusion = confusion_matrix(y, prediction)\nsns.heatmap(\n    confusion,\n    annot=True,\n    fmt=\"d\",\n    cmap=\"Blues\",\n    xticklabels=[\"Predicted -1\", \"Predicted 1\"],\n    yticklabels=[\"Actual -1\", \"Actual 1\"],\n)\nprint(\"f1_score: \", f1_score(y, prediction))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nf1_score:  0.9918557547944818\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-2.png){width=563 height=411}\n:::\n:::\n\n\nDespite having a high f1 score, the model actually performed very poorly since our recall (or sensitivity), the ability to predict true positives, is very low. In our case it is $\\frac{99}{99 + 393}$. Meaning, we were not able to reliably detect the fraudulant transactions.\n\n### Second Approach\nThe second approach we will try are isolation forests.\nIsolation forests are a very common technique that is used in anomally detection and outlier detection due their robustness in the presence of irrelevant attributes and their high efficiency.\nThe way that isolation forests work is by constructing multiple isolation trees that then average their scores.\nEach isolation tree is constructed by partitioning the data accross all of its features until each datapoint is located in its own leaf node.\nThen, the distance to each datapoint is measured. \nThe shorter the distance, the more likely is the datapoint to be an outlier. \nThis distance is also the score that is being averaged out.\n\nOnce again, creating an isolation forest is made easy with scikit-learn.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfrom sklearn.ensemble import IsolationForest\nforest = IsolationForest(random_state=42)\nprediction = forest.fit_predict(X)\n```\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nconfusion = confusion_matrix(y, prediction)\nsns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\",\n            xticklabels=['Predicted -1', 'Predicted 1'],\n            yticklabels=['Actual -1', 'Actual 1'])\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n<Axes: >\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-2.png){width=563 height=411}\n:::\n:::\n\n\nThis model has perfomed significantly better.\nWe were able to find much higher percentage of fraudulant transactions.\nIt is true that there were also much more false possitives outputed by this model.\nHowever, in cases such as this, it is much better to have a high false positive rate (also called low precision) since it is better to be overly cautios in these scenarios.\nThis is why credit companies would sometimes block your credit card transactions due to suspected fraud.\nThier model most likely has had a false positive, but it was much better to be safe than sorry.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}