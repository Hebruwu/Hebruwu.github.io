{
  "hash": "308a06e2e94c46626c6a436b2d04f062",
  "result": {
    "markdown": "---\ntitle: \"Clustered Penguins\"\nauthor: \"Daniel Sabanov\"\ndate: \"2023-12-04\"\ncategories: [Clustering]\n---\n\n# Using Clustering to Learn More About Penguins\n\n![Courtesy: S Richter et al/Journal of Physics D: Applied Physics](huddle.png)\n\nClustering is a machine learning technique used to group similar data togehter into subsets based on inherent similarities and patterns. It is a techineque that is wildly used in explaratory data analysis in order to reveal insights about data. It has a significant overlap with classification as many algorithms that are used in clustering are also used for classification. Such algorithms include K-Means-Clustering, K-Nearest-Neighbor, and many others.\n\nIn this blog, we will be exploring clustering as applied to the [penguins dataset](https://www.kaggle.com/datasets/parulpandey/palmer-archipelago-antarctica-penguin-data). \n\n# About the Data\nThe penguins dataset is a very popular dataset used for both clustering and classification algorithm testing and learning. It was collected as part of a study on [penguins' sexual dimorphism](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0090081). The data contains the following columns:\n\n- species: (Chinstrap, Ad√©lie, or Gentoo).\n- culmen_length_mm: the length of the dorsal ridge of the penguin's bill in millimeters.\n- culmen_depth_mm: the depth of the dorsal ridge of the penguin's bill in millimeters.\n- flipper_length_mm: the length of the penguin's flippers in millimeters.\n- body_mass_g: the mass of the penguin in grams.\n- island: the island on which the penguin was located (Dream, Torgersen, or Biscoe).\n- sex: the sex of the penguin.\n\nLet us load the data:\n\n## Loading and Preprocessing\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\npenguins_data = pd.read_csv(\"penguins_size.csv\")\npenguins_data.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>species</th>\n      <th>island</th>\n      <th>culmen_length_mm</th>\n      <th>culmen_depth_mm</th>\n      <th>flipper_length_mm</th>\n      <th>body_mass_g</th>\n      <th>sex</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>39.1</td>\n      <td>18.7</td>\n      <td>181.0</td>\n      <td>3750.0</td>\n      <td>MALE</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>39.5</td>\n      <td>17.4</td>\n      <td>186.0</td>\n      <td>3800.0</td>\n      <td>FEMALE</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>40.3</td>\n      <td>18.0</td>\n      <td>195.0</td>\n      <td>3250.0</td>\n      <td>FEMALE</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>36.7</td>\n      <td>19.3</td>\n      <td>193.0</td>\n      <td>3450.0</td>\n      <td>FEMALE</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nRight of the bat, we can see that that there are rows in the dataset that contain `NaN`s. One way we could deal with it is by imputing the values. However, it would be much simpler to just drop them as there are only a few such rows.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\npenguins_data = penguins_data.dropna()\n```\n:::\n\n\nNext, we want to encode the said labels for simplicity. \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\npenguins_data[\"species\"], species_scheme = pd.factorize(penguins_data[\"species\"])\n```\n:::\n\n\nIn this case `species_scheme` is an array that contains the previous labels with index that corresponds to their new label. If it sounded like a bunch of jibberish, here is contents:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nspecies_scheme\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nIndex(['Adelie', 'Chinstrap', 'Gentoo'], dtype='object')\n```\n:::\n:::\n\n\n## Learning more about the data\n\nLet's take a look at the distribution of all the numeric values in the dataset:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nnumeric_cols = [\n    \"culmen_length_mm\",\n    \"culmen_depth_mm\",\n    \"flipper_length_mm\",\n    \"body_mass_g\",\n]\ngraph = pd.plotting.scatter_matrix(\n    penguins_data[numeric_cols],\n    figsize=(10, 10),\n    c=penguins_data[\"species\"],\n    label=species_scheme,\n    diagonal=\"kde\",\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=818 height=815}\n:::\n:::\n\n\nWe can see that there exists a pretty clear separation between the distributions of all the subsets. This matrix is a lot of data to look at - at once. Instead we can condense most of the data into two dimentions using principal component analysis (PCA). This will reduce the dimensionality of the data into two, dummy, dimentions. In essence, it will be a projection of the data from the four dimentional space into two dimentions.\n\nFirst, thing we want to do before that is to standardize the data. This is done fairly easily by using `StandardScaler` from Scikit-Learn.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import StandardScaler\nscaled_numeric_penguins_data = StandardScaler().fit_transform(penguins_data[numeric_cols])\n```\n:::\n\n\nPerforming PCA is pretty straight forward as well.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2, random_state=42)\ntwo_component_penguins_data = pca.fit_transform(scaled_numeric_penguins_data)\n```\n:::\n\n\nNow that we have the projection of the data, let's graph it.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfor i, target_class in enumerate(species_scheme):\n    indices = penguins_data[\"species\"] == i\n    plt.scatter(two_component_penguins_data[indices, 0], two_component_penguins_data[indices, 1], label=target_class)\n\nplt.legend()\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n<matplotlib.legend.Legend at 0x2925394d0>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-2.png){width=569 height=411}\n:::\n:::\n\n\nWe can see that Gentoo are pretty distinct from the other two types of penguins. There is also some seperation between Adelies and Chinstraps's, but it is smaller in comparison. This will make it more difficult for our clustering algorithm to distinguish between them.\n\n## Clustering\n\nWe will try to distinguish between the penguins using K-Means-Clustering (K-Means). K-Means works by randomly initiating centers of each cluster (in our case it will initiate 3). It will then assign each point to a cluster based on which center the point is closest to. The centers of each cluster are then recalculated, and the steps are repeated until convergence or a step limit is reached. This is a good clustering algorithm for this dataset as the datapoints resemble unordered blobs with no specific shape that are somewhat distant from one another.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=3, random_state=42)\npredictions = kmeans.fit_predict(scaled_numeric_penguins_data)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/danielsabanov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n```\n:::\n:::\n\n\nLet's just verify that the labels that we got were in the range from 0 to 2.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nset(predictions)\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n{0, 1, 2}\n```\n:::\n:::\n\n\nNow, we can plot the projection of the data again, this time using the predicted labels.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nfor i, target_class in enumerate(set(predictions)):\n    indices = predictions == i\n    plt.scatter(two_component_penguins_data[indices, 0], two_component_penguins_data[indices, 1], label=target_class)\n\nplt.legend()\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n<matplotlib.legend.Legend at 0x296cd94d0>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-2.png){width=569 height=411}\n:::\n:::\n\n\nIt looks fairly similar. It also gives us an understanding which label corresponds to which species based on their distribution in the graph. We can see that 0 corresponds to chinstrap, 1 corresponds to gentoo, and 2 corresponds to adelie.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nclass_to_prediction = {\"adelie\": 2, \"chinstrap\": 0, \"gentoo\": 1}\n```\n:::\n\n\nWe can store the data back into the dataframe for easier access.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\npenguins_data[\"predictions\"] = predictions\n```\n:::\n\n\nLet's take a closer look at the distribtions in the data.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nfig1, ax1 = plt.subplots(ncols=2, figsize=(15, 7))\nfig2, ax2 = plt.subplots(ncols=2, figsize=(15, 7))\nfig3, ax3 = plt.subplots(ncols=2, figsize=(15, 7))\nfig4, ax4 = plt.subplots(ncols=2, figsize=(15, 7))\n\npenguins_data.groupby(\"species\")[\"culmen_length_mm\"].plot.kde(ax=ax1[0], title=\"Culmen Length in mm - True\")\npenguins_data.groupby(\"species\")[\"culmen_depth_mm\"].plot.kde(ax=ax2[0], title=\"Culmen Depth in mm - True\")\npenguins_data.groupby(\"species\")[\"flipper_length_mm\"].plot.kde(ax=ax3[0], title=\"Flipper Length in mm - True\")\npenguins_data.groupby(\"species\")[\"body_mass_g\"].plot.kde(ax=ax4[0], title=\"Body Mass in g - True\")\n\npenguins_data.groupby(\"predictions\")[\"culmen_length_mm\"].plot.kde(ax=ax1[1], title=\"Culmen Length in mm - Predictions\")\npenguins_data.groupby(\"predictions\")[\"culmen_depth_mm\"].plot.kde(ax=ax2[1], title=\"Culmen Depth in mm - Predictions\")\npenguins_data.groupby(\"predictions\")[\"flipper_length_mm\"].plot.kde(ax=ax3[1], title=\"Flipper Length in mm - Predictions\")\npenguins_data.groupby(\"predictions\")[\"body_mass_g\"].plot.kde(ax=ax4[1], title=\"Body Mass in g - Predictions\")\n\n\nplt.plot()\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n[]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-2.png){width=1192 height=579}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-3.png){width=1193 height=579}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-4.png){width=1192 height=579}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-5.png){width=1209 height=579}\n:::\n:::\n\n\nAlternatively, we can also overlay the distributions for easier comparison.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nfor col in numeric_cols:\n    fig1, axes = plt.subplots(ncols=3, figsize=(15,7))\n    ax1, ax2, ax3 = axes\n    \n    penguins_data.loc[penguins_data[\"species\"] == 0][col].plot.kde(ax=ax1, linestyle=\"-.\", title=f\"{col}- Adelie vs. Prediction\")\n    penguins_data.loc[penguins_data[\"predictions\"] == class_to_prediction[\"adelie\"]][col].plot.kde(ax=ax1, linestyle=\"--\")\n    \n    penguins_data.loc[penguins_data[\"species\"] == 1][col].plot.kde(ax=ax2, linestyle=\"-.\", title=f\"{col} - Chinstrap vs. Prediction\")\n    penguins_data.loc[penguins_data[\"predictions\"] == class_to_prediction[\"chinstrap\"]][col].plot.kde(ax=ax2, linestyle=\"--\")\n    \n    penguins_data.loc[penguins_data[\"species\"] == 2][col].plot.kde(ax=ax3, linestyle=\"-.\", title=f\"{col} - Gentoo vs. Prediction\")\n    penguins_data.loc[penguins_data[\"predictions\"] == class_to_prediction[\"gentoo\"]][col].plot.kde(ax=ax3, linestyle=\"--\")\n\n    plt.plot()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-16-output-1.png){width=1200 height=579}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-16-output-2.png){width=1198 height=579}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-16-output-3.png){width=1196 height=579}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-16-output-4.png){width=1209 height=579}\n:::\n:::\n\n\nWe can see pretty clearly in the data that one group of penguins is predicted perfectly, the genttos, as the distributions are identical, while the two others were not predicted as perfectly as the distributions do not match as well.\n\n## Trying to improve the resutls\n\nWe could try to improve the previous results by also introducing the categorical data into the dataset, as previously we were primarily working with only the continous data. This will hopefully give the algorithm more data to distinguish between the groups. \n\nOnce again, we will encode the data.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nencoded = pd.get_dummies(penguins_data, columns=[\"island\", \"sex\"])\nencoded.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>species</th>\n      <th>culmen_length_mm</th>\n      <th>culmen_depth_mm</th>\n      <th>flipper_length_mm</th>\n      <th>body_mass_g</th>\n      <th>predictions</th>\n      <th>island_Biscoe</th>\n      <th>island_Dream</th>\n      <th>island_Torgersen</th>\n      <th>sex_.</th>\n      <th>sex_FEMALE</th>\n      <th>sex_MALE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>39.1</td>\n      <td>18.7</td>\n      <td>181.0</td>\n      <td>3750.0</td>\n      <td>2</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>39.5</td>\n      <td>17.4</td>\n      <td>186.0</td>\n      <td>3800.0</td>\n      <td>2</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>40.3</td>\n      <td>18.0</td>\n      <td>195.0</td>\n      <td>3250.0</td>\n      <td>2</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>36.7</td>\n      <td>19.3</td>\n      <td>193.0</td>\n      <td>3450.0</td>\n      <td>2</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>39.3</td>\n      <td>20.6</td>\n      <td>190.0</td>\n      <td>3650.0</td>\n      <td>2</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWe actually do not need as many columns for the categorical data. We can remove a few columns since the data for these columns can be extrapolated from the other columns.\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nencoded = encoded.drop(columns=[\"sex_.\", \"sex_MALE\", \"island_Torgersen\", \"predictions\"])\nencoded.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>species</th>\n      <th>culmen_length_mm</th>\n      <th>culmen_depth_mm</th>\n      <th>flipper_length_mm</th>\n      <th>body_mass_g</th>\n      <th>island_Biscoe</th>\n      <th>island_Dream</th>\n      <th>sex_FEMALE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>39.1</td>\n      <td>18.7</td>\n      <td>181.0</td>\n      <td>3750.0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>39.5</td>\n      <td>17.4</td>\n      <td>186.0</td>\n      <td>3800.0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>40.3</td>\n      <td>18.0</td>\n      <td>195.0</td>\n      <td>3250.0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>36.7</td>\n      <td>19.3</td>\n      <td>193.0</td>\n      <td>3450.0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>39.3</td>\n      <td>20.6</td>\n      <td>190.0</td>\n      <td>3650.0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nOnce again, we want to scale the data.\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nscaled_penguins_data = encoded\nscaled_penguins_data[numeric_cols] = StandardScaler().fit_transform(scaled_penguins_data[numeric_cols])\nscaled_penguins_data[\"island_Biscoe\"] = scaled_penguins_data[\"island_Biscoe\"].astype(int)\nscaled_penguins_data[\"island_Dream\"] = scaled_penguins_data[\"island_Dream\"].astype(int)\nscaled_penguins_data.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>species</th>\n      <th>culmen_length_mm</th>\n      <th>culmen_depth_mm</th>\n      <th>flipper_length_mm</th>\n      <th>body_mass_g</th>\n      <th>island_Biscoe</th>\n      <th>island_Dream</th>\n      <th>sex_FEMALE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>-0.897653</td>\n      <td>0.783487</td>\n      <td>-1.429521</td>\n      <td>-0.571229</td>\n      <td>0</td>\n      <td>0</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>-0.824290</td>\n      <td>0.121896</td>\n      <td>-1.072408</td>\n      <td>-0.509011</td>\n      <td>0</td>\n      <td>0</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>-0.677564</td>\n      <td>0.427246</td>\n      <td>-0.429605</td>\n      <td>-1.193405</td>\n      <td>0</td>\n      <td>0</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>-1.337831</td>\n      <td>1.088836</td>\n      <td>-0.572450</td>\n      <td>-0.944535</td>\n      <td>0</td>\n      <td>0</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>-0.860972</td>\n      <td>1.750427</td>\n      <td>-0.786718</td>\n      <td>-0.695664</td>\n      <td>0</td>\n      <td>0</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nOnce again, let's perform PCA to understand how the overall data was affected by the introduction of the new variables.\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\npca = PCA(n_components=2, random_state=42)\ntwo_component_penguins_data = pca.fit_transform(scaled_penguins_data)\n\nfor i, target_class in enumerate(species_scheme):\n    indices = penguins_data[\"species\"] == i\n    plt.scatter(two_component_penguins_data[indices, 0], two_component_penguins_data[indices, 1], label=target_class)\n\nplt.legend()\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\n<matplotlib.legend.Legend at 0x29fc214d0>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-20-output-2.png){width=569 height=411}\n:::\n:::\n\n\nThis time, the difference between Adelies and Chinstraps is slightly more distinct as we can see that the Chistrap distribution is located slightly to the left of the Adelie distribution. This was not as apparent in the previous PCA run. Hopefully, we can achieve better clustering results due to this.\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nkmeans = KMeans(n_clusters=3, random_state=42)\npredictions = kmeans.fit_predict(scaled_penguins_data.drop(columns=[\"species\"]))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/danielsabanov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n```\n:::\n:::\n\n\nLet us graph the resutls.\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nfor i, target_class in enumerate(set(predictions)):\n    indices = predictions == i\n    plt.scatter(two_component_penguins_data[indices, 0], two_component_penguins_data[indices, 1], label=target_class)\n\nplt.legend()\n\nclass_to_prediction = {\"adelie\": 0, \"chinstrap\": 2, \"gentoo\": 1}\nencoded[\"predictions\"] = predictions\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-22-output-1.png){width=569 height=411}\n:::\n:::\n\n\nThis time the label 0 correspnds to adelie, 1 corresponds to gentoo, and 2 corresponds to chinstrap. It appears that the classification this time went sligtly better as there is a more significant overlap between the adelie and chinstrap distributions. Be sure let's examine the distributions in more details.\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\nfig1, ax1 = plt.subplots(ncols=2, figsize=(20, 10))\nfig2, ax2 = plt.subplots(ncols=2, figsize=(20, 10))\nfig3, ax3 = plt.subplots(ncols=2, figsize=(20, 10))\nfig4, ax4 = plt.subplots(ncols=2, figsize=(20, 10))\n\nencoded.groupby(\"species\")[\"culmen_length_mm\"].plot.kde(ax=ax1[0], title=\"Culmen Length in mm - True\")\nencoded.groupby(\"species\")[\"culmen_depth_mm\"].plot.kde(ax=ax2[0], title=\"Culmen Depth in mm - True\")\nencoded.groupby(\"species\")[\"flipper_length_mm\"].plot.kde(ax=ax3[0], title=\"Flipper Length in mm - True\")\nencoded.groupby(\"species\")[\"body_mass_g\"].plot.kde(ax=ax4[0], title=\"Body Mass in g - True\")\n\nencoded.groupby(\"predictions\")[\"culmen_length_mm\"].plot.kde(ax=ax1[1], title=\"Culmen Length in mm - Predictions\")\nencoded.groupby(\"predictions\")[\"culmen_depth_mm\"].plot.kde(ax=ax2[1], title=\"Culmen Depth in mm - Predictions\")\nencoded.groupby(\"predictions\")[\"flipper_length_mm\"].plot.kde(ax=ax3[1], title=\"Flipper Length in mm - Predictions\")\nencoded.groupby(\"predictions\")[\"body_mass_g\"].plot.kde(ax=ax4[1], title=\"Body Mass in g - Predictions\")\n\n\nplt.plot()\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\n[]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-23-output-2.png){width=1556 height=801}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-23-output-3.png){width=1556 height=801}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-23-output-4.png){width=1556 height=801}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-23-output-5.png){width=1556 height=801}\n:::\n:::\n\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nfor col in numeric_cols:\n    fig1, axes = plt.subplots(ncols=3, figsize=(15,7))\n    ax1, ax2, ax3 = axes\n    \n    encoded.loc[encoded[\"species\"] == 0][col].plot.kde(ax=ax1, linestyle=\"-.\", title=f\"{col}- Adelie vs. Prediction\")\n    encoded.loc[encoded[\"predictions\"] == class_to_prediction[\"adelie\"]][col].plot.kde(ax=ax1, linestyle=\"--\")\n    \n    encoded.loc[encoded[\"species\"] == 1][col].plot.kde(ax=ax2, linestyle=\"-.\", title=f\"{col} - Chinstrap vs. Prediction\")\n    encoded.loc[encoded[\"predictions\"] == class_to_prediction[\"chinstrap\"]][col].plot.kde(ax=ax2, linestyle=\"--\")\n    \n    encoded.loc[encoded[\"species\"] == 2][col].plot.kde(ax=ax3, linestyle=\"-.\", title=f\"{col} - Gentoo vs. Prediction\")\n    encoded.loc[encoded[\"predictions\"] == class_to_prediction[\"gentoo\"]][col].plot.kde(ax=ax3, linestyle=\"--\")\n\n    plt.plot()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-24-output-1.png){width=1192 height=579}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-24-output-2.png){width=1190 height=579}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-24-output-3.png){width=1188 height=579}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-24-output-4.png){width=1184 height=579}\n:::\n:::\n\n\nThis time it appears that there is a much better overlap between the real and predicted distributions of the chinstrap and adelie respenctive populations. This is actually a sensible results since sex makes a difference in the sizes of the penguins, so given that data, we are better able to tell the difference if a penguinqu is of a certain species.\n\n# Refernces\n\nGorman KB, Williams TD, Fraser WR (2014) Ecological Sexual Dimorphism and Environmental Variability within a Community of Antarctic Penguins (Genus Pygoscelis). PLoS ONE 9(3): e90081. doi:10.1371/journal.pone.0090081\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}