{"title":"Computer Learns the Beautiful Intricacies of Wine","markdown":{"yaml":{"title":"Computer Learns the Beautiful Intricacies of Wine","author":"Daniel Sabanov","date":"2023-12-03","categories":["Regression","Random Forest"]},"headingText":"Computer as a Sommelier","containsRefs":false,"markdown":"\n\n![Alex Tihonovs / EyeEm//Getty Images](img.jpg)\n\nWine occupies an exceptional place in the fabric of human culture, serving as an enduring symbol of tradition, refinement, and celebration spanning countless millennia. Its rich history spans across continents and epochs, with deep-seated connections in both the Old World and the New. Wine's production and consumption have long been integral to diverse societies, providing a window into the evolution of craftsmanship, agricultural practices, and culinary artistry. In its intricate spectrum of flavors and aromas, wine encapsulates the harmonious fusion of nature, culture, and human innovation, standing as a cherished and enduring emblem of our shared heritage. The cultural significance of wine has even given rise to a specific class of experts known as sommeliers, responsible for curating wine lists at restaurants and evaluating the quality of wine produced by vineyards.\n\nA sommelier, often regarded as a professional wine connoisseur, plays a pivotal role in selecting and presenting wines in restaurant settings, as well as assessing the quality of wines produced by various wineries. The quality of wine is dependent on measurable attributes, and if these qualities can be quantified, it stands to reason that we can develop models capable of predicting wine quality, thus making the task of currating wine somewhat simpler. However, the first step on this journey is acquiring the necessary data.\n\n## Data\n\nThe data we will analyze is sourced from [UC Irvine's website](https://archive.ics.uci.edu/dataset/186/wine+quality) and originates from the study \"Modeling wine preferences by data mining from physicochemical properties\" [1]. This dataset showcases wines of the Vinho Verde variety, produced in the breathtaking Minho region of Portugal. Data collection for this dataset took place between May 2004 and February 2007.\n\nThis dataset comprises two separate files: one exclusively dedicated to white wines and the other specifically detailing red wines. Both files contain 11 fields that provide comprehensive information about the physical properties of the wines, complemented by an additional field that characterizes the wine's quality. The quality of the wine was evaluated through a series of blind tests, yielding ratings ranging from 0 to 10. \n\nLet's take a quick look at the dataset:\n\n```{python}\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nwhite_wine_df = pd.read_csv(\"winequality-white.csv\", sep=\";\")\nred_wine_df = pd.read_csv(\"winequality-red.csv\", sep=\";\")\nwhite_wine_df.head()\n```\n\n```{python}\nred_wine_df.head()\n```\n\n```{python}\nimport numpy as np\nplt.figure(figsize=(10,5))\nbin_edges = np.arange(11) - 0.5\nwhite_wine_df[\"quality\"].plot(kind=\"hist\", alpha=0.5, bins=bin_edges, edgecolor=\"k\", density=True)\nred_wine_df[\"quality\"].plot(kind=\"hist\", alpha=0.5, bins=bin_edges, edgecolor=\"k\", density=True)\nplt.legend([\"White\", \"Red\"])\nplt.xticks(range(10))\nplt.xlim([-1, 10])\n```\n\nThe quality distribution of the wines seems to approximate a normal distribution. Additionally, there is a noticeable rightward shift in the distribution of white wine quality compared to red wine, suggesting that, on average, white wine may receive higher rankings. To examine the distributions of other columns in the data—given their continuous nature—it would be more appropriate to utilize Kernel Density Estimation (KDE) instead.\n\n```{python}\nfor column in red_wine_df.columns:\n    if column != \"quality\":\n        plt.figure()    \n        white_wine_df[column].plot(kind=\"kde\", label=\"White\")\n        red_wine_df[column].plot(kind=\"kde\", label=\"Red\")\n        plt.legend()\n        plt.title(column)\n```\n\nThe distributions of the independent variables related to wine quality show noticeable variations. Hence, it would be more sensible to develop separate models for each type of wine rather than trying to merge the data into a single dataframe, at least when we are dealing with simpler models.\n\n## Predicting Wine Quality\n\n### Linear Model\nHaving reviewed the data, our next step is to construct a model that provides a reliable estimate of wine quality. Beginning with the simplest technique, let's explore simple linear regression. To accomplish this, we'll identify the variables that show the strongest correlation with wine quality. For visualization purposes, we'll leverage Seaborn, a high-level API for the Matplotlib library, streamlining the process of plotting specific graphs.\n\n```{python}\nimport seaborn as sns\nplt.figure(figsize=(10,10))\nsns.heatmap(white_wine_df.corr(), annot=True).set(title=\"White Wine Correlation\")\nplt.show()\nplt.figure(figsize=(10,10))\nsns.heatmap(red_wine_df.corr(), annot=True).set(title=\"Red Wine Correlation\")\nplt.show()\n```\n\nInterestingly, it seems that alcohol concentration is the variable most correlated with wine quality. However, given the relatively weak correlation, I'm not overly optimistic about the prospects of this model. Before moving forward, it's crucial to confirm the presence of a linear relationship between the variables.\n\n```{python}\nplt.scatter(\"alcohol\", \"quality\", data=white_wine_df, alpha=0.01)\nplt.title(\"White Wine Alchohol Content vs. Quality\")\n\nplt.figure()\nplt.scatter(\"alcohol\", \"quality\", data=red_wine_df, alpha=0.01)\nplt.title(\"Red Wine Alchohol Content vs. Quality\")\n```\n\nThe relationship between the variables doesn't seem to follow a linear pattern. Nevertheless, for the sake of exploration, we can attempt to fit a linear model to it. To start, we'll divide the data into a training set and a test set.\n\n```{python}\nfrom sklearn.model_selection import train_test_split\nX_white = white_wine_df[\"alcohol\"].to_numpy().reshape(-1, 1) # Reshaped for a single feature\ny_white = white_wine_df[\"quality\"].to_numpy().reshape(-1, 1)\nX_train_white, X_test_white, y_train_white, y_test_white = train_test_split(X_white, y_white, test_size=0.2, random_state=42)\n\nX_red = red_wine_df[\"alcohol\"].to_numpy().reshape(-1, 1)\ny_red = red_wine_df[\"quality\"].to_numpy().reshape(-1, 1)\n\nX_train_red, X_test_red, y_train_red, y_test_red = train_test_split(X_red, y_red, test_size=0.2, random_state=42)\n```\n\n```{python}\nfrom sklearn.linear_model import LinearRegression\nwhite_linear_regression_model = LinearRegression()\nwhite_linear_regression_model.fit(X_train_white, y_train_white)\n\nred_linear_regression_model = LinearRegression()\nred_linear_regression_model.fit(X_train_red, y_train_red)\n```\n\nHaving fitted the model, let's now assess its predictive performance. In this case, we'll evaluate its accuracy using metrics such as Mean Squared Error (MSE) and Root Mean Squared Error (RMSE).\n\n```{python}\nfrom sklearn.metrics import mean_squared_error\nwhite_predictions = white_linear_regression_model.predict(X_test_white)\nred_predictions = red_linear_regression_model.predict(X_test_red)\n\nwhite_error = mean_squared_error(y_test_white, white_predictions)\nred_error = mean_squared_error(y_test_red, red_predictions)\n\nprint(f\"The MSE for white wine is {white_error} and RMSE of {white_error ** 0.5}\")\nprint(f\"The MSE for red wine is {red_error} and RMSE of {red_error ** 0.5}\")\n```\n\nAt first glance, the RMSE of 0.78 may seem promising. However, it's important to consider that the data is heavily concentrated on a few data points. Let's explore a more robust technique—multiple linear regression.\n\nMultiple linear regression, akin to linear regression, involves using multiple variables to model the data. The next significant variables correlating with red wine quality are volatile acidity and density in white wine. We can fit a model in a similar fashion.\n\nFirstly, we divide the data into a test and train set, mirroring the previous procedure. However, this time, we include \"volatile acidity\" as part of our predictor variables (X) for red wine and \"density\" for white wine.\n\n```{python}\nX_white = white_wine_df[[\"alcohol\", \"density\"]].to_numpy()\ny_white = white_wine_df[\"quality\"].to_numpy()\nX_train_white, X_test_white, y_train_white, y_test_white = train_test_split(X_white, y_white, test_size=0.2, random_state=42)\n\nX_red = red_wine_df[[\"alcohol\", \"volatile acidity\"]].to_numpy()\ny_red = red_wine_df[\"quality\"].to_numpy()\n\nX_train_red, X_test_red, y_train_red, y_test_red = train_test_split(X_red, y_red, test_size=0.2, random_state=42)\n```\n\nThe sklearn linear regression model supports multiple linear regression right out of the box. Therefore, the next code block doesn't introduce any new, ground-breaking, concepts. We simply supply it with a slightly modified X, incorporating the additional predictor variables.\n\n```{python}\nwhite_linear_regression_model = LinearRegression()\nwhite_linear_regression_model.fit(X_train_white, y_train_white)\n\nred_linear_regression_model = LinearRegression()\nred_linear_regression_model.fit(X_train_red, y_train_red)\n\nwhite_predictions = white_linear_regression_model.predict(X_test_white)\nred_predictions = red_linear_regression_model.predict(X_test_red)\n\nwhite_error = mean_squared_error(y_test_white, white_predictions)\nred_error = mean_squared_error(y_test_red, red_predictions)\n\nprint(f\"The MSE for white wine is {white_error} and RMSE of {white_error ** 0.5}\")\nprint(f\"The MSE for red wine is {red_error} and RMSE of {red_error ** 0.5}\")\n```\n\nWhen provided with more than one variable, the linear regression model demonstrated improved fitting capabilities on the red wine dataset. However, the RMSE for the white wine dataset remained unaltered, and overall, the RMSE remains relatively high. Perhaps it's worth considering a more potent approach to address the issue.\n\n### Random Forrest\nThe random forest model is significantly more potent than linear regression, albeit at the expense of managing more hyperparameters and being more computationally expensive. To achieve the optimal model, we'll need to fine-tune some of these hyperparameters through a grid search. Although computationally more intensive, this approach ensures the selection of the best hyperparameters for the specific dataset.\n\n```{python}\nX_white = white_wine_df.loc[:, white_wine_df.columns != \"quality\"].to_numpy()\ny_white = white_wine_df[\"quality\"].to_numpy()\nX_train_white, X_test_white, y_train_white, y_test_white = train_test_split(X_white, y_white, test_size=0.2, random_state=42)\n\nX_red = red_wine_df.loc[:, white_wine_df.columns != \"quality\"].to_numpy()\ny_red = red_wine_df[\"quality\"].to_numpy()\nX_train_red, X_test_red, y_train_red, y_test_red = train_test_split(X_red, y_red, test_size=0.2, random_state=42)\n```\n\nHaving re-split our data, we can now perform the grid search. The parameters slated for tuning are n_estimators and max_depth. n_estimators governs the number of trees in the forest, while max_depth regulates the maximum depth of each tree.\n\n```{python}\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nparameters = {\"n_estimators\": list(range(1,14)), \n              \"max_depth\": list(range(5, 20)) + [None]} # None corresponds to no limit on the depth.\nclf_white = GridSearchCV(RandomForestRegressor(), param_grid=parameters, n_jobs=8)\nwhite_tuned_random_forrest = clf_white.fit(X_train_white, y_train_white)\n\nclf_red = GridSearchCV(RandomForestRegressor(), param_grid=parameters, n_jobs=8)\nred_tuned_random_forrest = clf_red.fit(X_train_red, y_train_red)\n```\n\nNow that we have a model for each dataset, let's assess their performance.\n\n```{python}\nwhite_predictions = white_tuned_random_forrest.predict(X_test_white)\nred_predictions = red_tuned_random_forrest.predict(X_test_red)\n\nwhite_error = mean_squared_error(y_test_white, white_predictions)\nred_error = mean_squared_error(y_test_red, red_predictions)\n\nprint(f\"The MSE for white wine is {white_error} and RMSE of {white_error ** 0.5}\")\nprint(f\"The MSE for red wine is {red_error} and RMSE of {red_error ** 0.5}\")\n```\n\nAt the cost of higher training time, the model we've developed is substantially more accurate than the linear regression model. To the extent that I'm quite confident we could produce a robust model without separating the white and red wine datasets.\n\nLet's combine our datasets and then split them for training.\n\n```{python}\ncombined_df = pd.concat([white_wine_df, red_wine_df])\nX_combined = combined_df.loc[:, combined_df.columns != \"quality\"].to_numpy()\ny_combined = combined_df[\"quality\"].to_numpy()\n\nX_train_combined, X_test_combined, y_train_combined, y_test_combined = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)\n```\n\n```{python}\nparameters = {\"n_estimators\": list(range(1,14)), \n              \"max_depth\": list(range(5, 20)) + [None]} # None corresponds to no limit on the depth.\nclf_combined = GridSearchCV(RandomForestRegressor(), param_grid=parameters, n_jobs=8)\ncombined_tuned_random_forrest = clf_combined.fit(X_train_combined, y_train_combined)\n```\n\n```{python}\ncombined_prediction = combined_tuned_random_forrest.predict(X_test_combined)\ncombined_error = mean_squared_error(y_test_combined, combined_prediction)\nprint(f\"The MSE for white wine is {combined_error} and RMSE of {combined_error ** 0.5}\")\n```\n\nEven with the combined dataset, it's apparent that our model can effectively distinguish between white and red wine, assigning a quality score that outperforms the simple linear regression model.\n\n## References\n[1] P. Cortez, et al. \"Modeling wine preferences by data mining from physicochemical properties,\" in Decis. Support Syst., vol. 47, pp. 547-553, 2009.","srcMarkdownNoYaml":"\n\n![Alex Tihonovs / EyeEm//Getty Images](img.jpg)\n\n# Computer as a Sommelier\nWine occupies an exceptional place in the fabric of human culture, serving as an enduring symbol of tradition, refinement, and celebration spanning countless millennia. Its rich history spans across continents and epochs, with deep-seated connections in both the Old World and the New. Wine's production and consumption have long been integral to diverse societies, providing a window into the evolution of craftsmanship, agricultural practices, and culinary artistry. In its intricate spectrum of flavors and aromas, wine encapsulates the harmonious fusion of nature, culture, and human innovation, standing as a cherished and enduring emblem of our shared heritage. The cultural significance of wine has even given rise to a specific class of experts known as sommeliers, responsible for curating wine lists at restaurants and evaluating the quality of wine produced by vineyards.\n\nA sommelier, often regarded as a professional wine connoisseur, plays a pivotal role in selecting and presenting wines in restaurant settings, as well as assessing the quality of wines produced by various wineries. The quality of wine is dependent on measurable attributes, and if these qualities can be quantified, it stands to reason that we can develop models capable of predicting wine quality, thus making the task of currating wine somewhat simpler. However, the first step on this journey is acquiring the necessary data.\n\n## Data\n\nThe data we will analyze is sourced from [UC Irvine's website](https://archive.ics.uci.edu/dataset/186/wine+quality) and originates from the study \"Modeling wine preferences by data mining from physicochemical properties\" [1]. This dataset showcases wines of the Vinho Verde variety, produced in the breathtaking Minho region of Portugal. Data collection for this dataset took place between May 2004 and February 2007.\n\nThis dataset comprises two separate files: one exclusively dedicated to white wines and the other specifically detailing red wines. Both files contain 11 fields that provide comprehensive information about the physical properties of the wines, complemented by an additional field that characterizes the wine's quality. The quality of the wine was evaluated through a series of blind tests, yielding ratings ranging from 0 to 10. \n\nLet's take a quick look at the dataset:\n\n```{python}\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nwhite_wine_df = pd.read_csv(\"winequality-white.csv\", sep=\";\")\nred_wine_df = pd.read_csv(\"winequality-red.csv\", sep=\";\")\nwhite_wine_df.head()\n```\n\n```{python}\nred_wine_df.head()\n```\n\n```{python}\nimport numpy as np\nplt.figure(figsize=(10,5))\nbin_edges = np.arange(11) - 0.5\nwhite_wine_df[\"quality\"].plot(kind=\"hist\", alpha=0.5, bins=bin_edges, edgecolor=\"k\", density=True)\nred_wine_df[\"quality\"].plot(kind=\"hist\", alpha=0.5, bins=bin_edges, edgecolor=\"k\", density=True)\nplt.legend([\"White\", \"Red\"])\nplt.xticks(range(10))\nplt.xlim([-1, 10])\n```\n\nThe quality distribution of the wines seems to approximate a normal distribution. Additionally, there is a noticeable rightward shift in the distribution of white wine quality compared to red wine, suggesting that, on average, white wine may receive higher rankings. To examine the distributions of other columns in the data—given their continuous nature—it would be more appropriate to utilize Kernel Density Estimation (KDE) instead.\n\n```{python}\nfor column in red_wine_df.columns:\n    if column != \"quality\":\n        plt.figure()    \n        white_wine_df[column].plot(kind=\"kde\", label=\"White\")\n        red_wine_df[column].plot(kind=\"kde\", label=\"Red\")\n        plt.legend()\n        plt.title(column)\n```\n\nThe distributions of the independent variables related to wine quality show noticeable variations. Hence, it would be more sensible to develop separate models for each type of wine rather than trying to merge the data into a single dataframe, at least when we are dealing with simpler models.\n\n## Predicting Wine Quality\n\n### Linear Model\nHaving reviewed the data, our next step is to construct a model that provides a reliable estimate of wine quality. Beginning with the simplest technique, let's explore simple linear regression. To accomplish this, we'll identify the variables that show the strongest correlation with wine quality. For visualization purposes, we'll leverage Seaborn, a high-level API for the Matplotlib library, streamlining the process of plotting specific graphs.\n\n```{python}\nimport seaborn as sns\nplt.figure(figsize=(10,10))\nsns.heatmap(white_wine_df.corr(), annot=True).set(title=\"White Wine Correlation\")\nplt.show()\nplt.figure(figsize=(10,10))\nsns.heatmap(red_wine_df.corr(), annot=True).set(title=\"Red Wine Correlation\")\nplt.show()\n```\n\nInterestingly, it seems that alcohol concentration is the variable most correlated with wine quality. However, given the relatively weak correlation, I'm not overly optimistic about the prospects of this model. Before moving forward, it's crucial to confirm the presence of a linear relationship between the variables.\n\n```{python}\nplt.scatter(\"alcohol\", \"quality\", data=white_wine_df, alpha=0.01)\nplt.title(\"White Wine Alchohol Content vs. Quality\")\n\nplt.figure()\nplt.scatter(\"alcohol\", \"quality\", data=red_wine_df, alpha=0.01)\nplt.title(\"Red Wine Alchohol Content vs. Quality\")\n```\n\nThe relationship between the variables doesn't seem to follow a linear pattern. Nevertheless, for the sake of exploration, we can attempt to fit a linear model to it. To start, we'll divide the data into a training set and a test set.\n\n```{python}\nfrom sklearn.model_selection import train_test_split\nX_white = white_wine_df[\"alcohol\"].to_numpy().reshape(-1, 1) # Reshaped for a single feature\ny_white = white_wine_df[\"quality\"].to_numpy().reshape(-1, 1)\nX_train_white, X_test_white, y_train_white, y_test_white = train_test_split(X_white, y_white, test_size=0.2, random_state=42)\n\nX_red = red_wine_df[\"alcohol\"].to_numpy().reshape(-1, 1)\ny_red = red_wine_df[\"quality\"].to_numpy().reshape(-1, 1)\n\nX_train_red, X_test_red, y_train_red, y_test_red = train_test_split(X_red, y_red, test_size=0.2, random_state=42)\n```\n\n```{python}\nfrom sklearn.linear_model import LinearRegression\nwhite_linear_regression_model = LinearRegression()\nwhite_linear_regression_model.fit(X_train_white, y_train_white)\n\nred_linear_regression_model = LinearRegression()\nred_linear_regression_model.fit(X_train_red, y_train_red)\n```\n\nHaving fitted the model, let's now assess its predictive performance. In this case, we'll evaluate its accuracy using metrics such as Mean Squared Error (MSE) and Root Mean Squared Error (RMSE).\n\n```{python}\nfrom sklearn.metrics import mean_squared_error\nwhite_predictions = white_linear_regression_model.predict(X_test_white)\nred_predictions = red_linear_regression_model.predict(X_test_red)\n\nwhite_error = mean_squared_error(y_test_white, white_predictions)\nred_error = mean_squared_error(y_test_red, red_predictions)\n\nprint(f\"The MSE for white wine is {white_error} and RMSE of {white_error ** 0.5}\")\nprint(f\"The MSE for red wine is {red_error} and RMSE of {red_error ** 0.5}\")\n```\n\nAt first glance, the RMSE of 0.78 may seem promising. However, it's important to consider that the data is heavily concentrated on a few data points. Let's explore a more robust technique—multiple linear regression.\n\nMultiple linear regression, akin to linear regression, involves using multiple variables to model the data. The next significant variables correlating with red wine quality are volatile acidity and density in white wine. We can fit a model in a similar fashion.\n\nFirstly, we divide the data into a test and train set, mirroring the previous procedure. However, this time, we include \"volatile acidity\" as part of our predictor variables (X) for red wine and \"density\" for white wine.\n\n```{python}\nX_white = white_wine_df[[\"alcohol\", \"density\"]].to_numpy()\ny_white = white_wine_df[\"quality\"].to_numpy()\nX_train_white, X_test_white, y_train_white, y_test_white = train_test_split(X_white, y_white, test_size=0.2, random_state=42)\n\nX_red = red_wine_df[[\"alcohol\", \"volatile acidity\"]].to_numpy()\ny_red = red_wine_df[\"quality\"].to_numpy()\n\nX_train_red, X_test_red, y_train_red, y_test_red = train_test_split(X_red, y_red, test_size=0.2, random_state=42)\n```\n\nThe sklearn linear regression model supports multiple linear regression right out of the box. Therefore, the next code block doesn't introduce any new, ground-breaking, concepts. We simply supply it with a slightly modified X, incorporating the additional predictor variables.\n\n```{python}\nwhite_linear_regression_model = LinearRegression()\nwhite_linear_regression_model.fit(X_train_white, y_train_white)\n\nred_linear_regression_model = LinearRegression()\nred_linear_regression_model.fit(X_train_red, y_train_red)\n\nwhite_predictions = white_linear_regression_model.predict(X_test_white)\nred_predictions = red_linear_regression_model.predict(X_test_red)\n\nwhite_error = mean_squared_error(y_test_white, white_predictions)\nred_error = mean_squared_error(y_test_red, red_predictions)\n\nprint(f\"The MSE for white wine is {white_error} and RMSE of {white_error ** 0.5}\")\nprint(f\"The MSE for red wine is {red_error} and RMSE of {red_error ** 0.5}\")\n```\n\nWhen provided with more than one variable, the linear regression model demonstrated improved fitting capabilities on the red wine dataset. However, the RMSE for the white wine dataset remained unaltered, and overall, the RMSE remains relatively high. Perhaps it's worth considering a more potent approach to address the issue.\n\n### Random Forrest\nThe random forest model is significantly more potent than linear regression, albeit at the expense of managing more hyperparameters and being more computationally expensive. To achieve the optimal model, we'll need to fine-tune some of these hyperparameters through a grid search. Although computationally more intensive, this approach ensures the selection of the best hyperparameters for the specific dataset.\n\n```{python}\nX_white = white_wine_df.loc[:, white_wine_df.columns != \"quality\"].to_numpy()\ny_white = white_wine_df[\"quality\"].to_numpy()\nX_train_white, X_test_white, y_train_white, y_test_white = train_test_split(X_white, y_white, test_size=0.2, random_state=42)\n\nX_red = red_wine_df.loc[:, white_wine_df.columns != \"quality\"].to_numpy()\ny_red = red_wine_df[\"quality\"].to_numpy()\nX_train_red, X_test_red, y_train_red, y_test_red = train_test_split(X_red, y_red, test_size=0.2, random_state=42)\n```\n\nHaving re-split our data, we can now perform the grid search. The parameters slated for tuning are n_estimators and max_depth. n_estimators governs the number of trees in the forest, while max_depth regulates the maximum depth of each tree.\n\n```{python}\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nparameters = {\"n_estimators\": list(range(1,14)), \n              \"max_depth\": list(range(5, 20)) + [None]} # None corresponds to no limit on the depth.\nclf_white = GridSearchCV(RandomForestRegressor(), param_grid=parameters, n_jobs=8)\nwhite_tuned_random_forrest = clf_white.fit(X_train_white, y_train_white)\n\nclf_red = GridSearchCV(RandomForestRegressor(), param_grid=parameters, n_jobs=8)\nred_tuned_random_forrest = clf_red.fit(X_train_red, y_train_red)\n```\n\nNow that we have a model for each dataset, let's assess their performance.\n\n```{python}\nwhite_predictions = white_tuned_random_forrest.predict(X_test_white)\nred_predictions = red_tuned_random_forrest.predict(X_test_red)\n\nwhite_error = mean_squared_error(y_test_white, white_predictions)\nred_error = mean_squared_error(y_test_red, red_predictions)\n\nprint(f\"The MSE for white wine is {white_error} and RMSE of {white_error ** 0.5}\")\nprint(f\"The MSE for red wine is {red_error} and RMSE of {red_error ** 0.5}\")\n```\n\nAt the cost of higher training time, the model we've developed is substantially more accurate than the linear regression model. To the extent that I'm quite confident we could produce a robust model without separating the white and red wine datasets.\n\nLet's combine our datasets and then split them for training.\n\n```{python}\ncombined_df = pd.concat([white_wine_df, red_wine_df])\nX_combined = combined_df.loc[:, combined_df.columns != \"quality\"].to_numpy()\ny_combined = combined_df[\"quality\"].to_numpy()\n\nX_train_combined, X_test_combined, y_train_combined, y_test_combined = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)\n```\n\n```{python}\nparameters = {\"n_estimators\": list(range(1,14)), \n              \"max_depth\": list(range(5, 20)) + [None]} # None corresponds to no limit on the depth.\nclf_combined = GridSearchCV(RandomForestRegressor(), param_grid=parameters, n_jobs=8)\ncombined_tuned_random_forrest = clf_combined.fit(X_train_combined, y_train_combined)\n```\n\n```{python}\ncombined_prediction = combined_tuned_random_forrest.predict(X_test_combined)\ncombined_error = mean_squared_error(y_test_combined, combined_prediction)\nprint(f\"The MSE for white wine is {combined_error} and RMSE of {combined_error ** 0.5}\")\n```\n\nEven with the combined dataset, it's apparent that our model can effectively distinguish between white and red wine, assigning a quality score that outperforms the simple linear regression model.\n\n## References\n[1] P. Cortez, et al. \"Modeling wine preferences by data mining from physicochemical properties,\" in Decis. Support Syst., vol. 47, pp. 547-553, 2009."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"cosmo","title-block-banner":true,"title":"Computer Learns the Beautiful Intricacies of Wine","author":"Daniel Sabanov","date":"2023-12-03","categories":["Regression","Random Forest"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}