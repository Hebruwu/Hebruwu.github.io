{
  "hash": "fa9de6b79a749ed0588e43ae8dadc373",
  "result": {
    "markdown": "---\ntitle: \"Computer Learns the Beautiful Intricacies of Wine\"\nauthor: \"Daniel Sabanov\"\ndate: \"2023-12-03\"\ncategories: [Regression, Random Forest]\n---\n\n![Alex Tihonovs / EyeEm//Getty Images](img.jpg)\n# Computer as a Sommelier\nWine occupies an exceptional place in the fabric of human culture, serving as an enduring symbol of tradition, refinement, and celebration spanning countless millennia. Its rich history spans across continents and epochs, with deep-seated connections in both the Old World and the New. Wine's production and consumption have long been integral to diverse societies, providing a window into the evolution of craftsmanship, agricultural practices, and culinary artistry. In its intricate spectrum of flavors and aromas, wine encapsulates the harmonious fusion of nature, culture, and human innovation, standing as a cherished and enduring emblem of our shared heritage. The cultural significance of wine has even given rise to a specific class of experts known as sommeliers, responsible for curating wine lists at restaurants and evaluating the quality of wine produced by vineyards.\n\nA sommelier, often regarded as a professional wine connoisseur, plays a pivotal role in selecting and presenting wines in restaurant settings, as well as assessing the quality of wines produced by various wineries. The quality of wine is dependent on measurable attributes, and if these qualities can be quantified, it stands to reason that we can develop models capable of predicting wine quality, thus making the task of currating wine somewhat simpler. However, the first step on this journey is acquiring the necessary data.\n\n## Data\n\nThe data we will analyze is sourced from [UC Irvine's website](https://archive.ics.uci.edu/dataset/186/wine+quality) and originates from the study \"Modeling wine preferences by data mining from physicochemical properties\" [1]. This dataset showcases wines of the Vinho Verde variety, produced in the breathtaking Minho region of Portugal. Data collection for this dataset took place between May 2004 and February 2007.\n\nThis dataset comprises two separate files: one exclusively dedicated to white wines and the other specifically detailing red wines. Both files contain 11 fields that provide comprehensive information about the physical properties of the wines, complemented by an additional field that characterizes the wine's quality. The quality of the wine was evaluated through a series of blind tests, yielding ratings ranging from 0 to 10. \n\nLet's take a quick look at the dataset:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nwhite_wine_df = pd.read_csv(\"winequality-white.csv\", sep=\";\")\nred_wine_df = pd.read_csv(\"winequality-red.csv\", sep=\";\")\nwhite_wine_df.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fixed acidity</th>\n      <th>volatile acidity</th>\n      <th>citric acid</th>\n      <th>residual sugar</th>\n      <th>chlorides</th>\n      <th>free sulfur dioxide</th>\n      <th>total sulfur dioxide</th>\n      <th>density</th>\n      <th>pH</th>\n      <th>sulphates</th>\n      <th>alcohol</th>\n      <th>quality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7.0</td>\n      <td>0.27</td>\n      <td>0.36</td>\n      <td>20.7</td>\n      <td>0.045</td>\n      <td>45.0</td>\n      <td>170.0</td>\n      <td>1.0010</td>\n      <td>3.00</td>\n      <td>0.45</td>\n      <td>8.8</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6.3</td>\n      <td>0.30</td>\n      <td>0.34</td>\n      <td>1.6</td>\n      <td>0.049</td>\n      <td>14.0</td>\n      <td>132.0</td>\n      <td>0.9940</td>\n      <td>3.30</td>\n      <td>0.49</td>\n      <td>9.5</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8.1</td>\n      <td>0.28</td>\n      <td>0.40</td>\n      <td>6.9</td>\n      <td>0.050</td>\n      <td>30.0</td>\n      <td>97.0</td>\n      <td>0.9951</td>\n      <td>3.26</td>\n      <td>0.44</td>\n      <td>10.1</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7.2</td>\n      <td>0.23</td>\n      <td>0.32</td>\n      <td>8.5</td>\n      <td>0.058</td>\n      <td>47.0</td>\n      <td>186.0</td>\n      <td>0.9956</td>\n      <td>3.19</td>\n      <td>0.40</td>\n      <td>9.9</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.2</td>\n      <td>0.23</td>\n      <td>0.32</td>\n      <td>8.5</td>\n      <td>0.058</td>\n      <td>47.0</td>\n      <td>186.0</td>\n      <td>0.9956</td>\n      <td>3.19</td>\n      <td>0.40</td>\n      <td>9.9</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nred_wine_df.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fixed acidity</th>\n      <th>volatile acidity</th>\n      <th>citric acid</th>\n      <th>residual sugar</th>\n      <th>chlorides</th>\n      <th>free sulfur dioxide</th>\n      <th>total sulfur dioxide</th>\n      <th>density</th>\n      <th>pH</th>\n      <th>sulphates</th>\n      <th>alcohol</th>\n      <th>quality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7.4</td>\n      <td>0.70</td>\n      <td>0.00</td>\n      <td>1.9</td>\n      <td>0.076</td>\n      <td>11.0</td>\n      <td>34.0</td>\n      <td>0.9978</td>\n      <td>3.51</td>\n      <td>0.56</td>\n      <td>9.4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7.8</td>\n      <td>0.88</td>\n      <td>0.00</td>\n      <td>2.6</td>\n      <td>0.098</td>\n      <td>25.0</td>\n      <td>67.0</td>\n      <td>0.9968</td>\n      <td>3.20</td>\n      <td>0.68</td>\n      <td>9.8</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7.8</td>\n      <td>0.76</td>\n      <td>0.04</td>\n      <td>2.3</td>\n      <td>0.092</td>\n      <td>15.0</td>\n      <td>54.0</td>\n      <td>0.9970</td>\n      <td>3.26</td>\n      <td>0.65</td>\n      <td>9.8</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.2</td>\n      <td>0.28</td>\n      <td>0.56</td>\n      <td>1.9</td>\n      <td>0.075</td>\n      <td>17.0</td>\n      <td>60.0</td>\n      <td>0.9980</td>\n      <td>3.16</td>\n      <td>0.58</td>\n      <td>9.8</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.4</td>\n      <td>0.70</td>\n      <td>0.00</td>\n      <td>1.9</td>\n      <td>0.076</td>\n      <td>11.0</td>\n      <td>34.0</td>\n      <td>0.9978</td>\n      <td>3.51</td>\n      <td>0.56</td>\n      <td>9.4</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nplt.figure(figsize=(10,5))\nbin_edges = np.arange(11) - 0.5\nwhite_wine_df[\"quality\"].plot(kind=\"hist\", alpha=0.5, bins=bin_edges, edgecolor=\"k\", density=True)\nred_wine_df[\"quality\"].plot(kind=\"hist\", alpha=0.5, bins=bin_edges, edgecolor=\"k\", density=True)\nplt.legend([\"White\", \"Red\"])\nplt.xticks(range(10))\nplt.xlim([-1, 10])\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n(-1.0, 10.0)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-2.png){width=812 height=411}\n:::\n:::\n\n\nThe quality distribution of the wines seems to approximate a normal distribution. Additionally, there is a noticeable rightward shift in the distribution of white wine quality compared to red wine, suggesting that, on average, white wine may receive higher rankings. To examine the distributions of other columns in the data—given their continuous nature—it would be more appropriate to utilize Kernel Density Estimation (KDE) instead.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfor column in red_wine_df.columns:\n    if column != \"quality\":\n        plt.figure()    \n        white_wine_df[column].plot(kind=\"kde\", label=\"White\")\n        red_wine_df[column].plot(kind=\"kde\", label=\"Red\")\n        plt.legend()\n        plt.title(column)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-2.png){width=576 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-3.png){width=576 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-4.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-5.png){width=585 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-6.png){width=597 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-7.png){width=614 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-8.png){width=593 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-9.png){width=589 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-10.png){width=599 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-11.png){width=589 height=431}\n:::\n:::\n\n\nThe distributions of the independent variables related to wine quality show noticeable variations. Hence, it would be more sensible to develop separate models for each type of wine rather than trying to merge the data into a single dataframe, at least when we are dealing with simpler models.\n\n## Predicting Wine Quality\n\n### Linear Model\nHaving reviewed the data, our next step is to construct a model that provides a reliable estimate of wine quality. Beginning with the simplest technique, let's explore simple linear regression. To accomplish this, we'll identify the variables that show the strongest correlation with wine quality. For visualization purposes, we'll leverage Seaborn, a high-level API for the Matplotlib library, streamlining the process of plotting specific graphs.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nimport seaborn as sns\nplt.figure(figsize=(10,10))\nsns.heatmap(white_wine_df.corr(), annot=True).set(title=\"White Wine Correlation\")\nplt.show()\nplt.figure(figsize=(10,10))\nsns.heatmap(red_wine_df.corr(), annot=True).set(title=\"Red Wine Correlation\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=864 height=912}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-2.png){width=864 height=912}\n:::\n:::\n\n\nInterestingly, it seems that alcohol concentration is the variable most correlated with wine quality. However, given the relatively weak correlation, I'm not overly optimistic about the prospects of this model. Before moving forward, it's crucial to confirm the presence of a linear relationship between the variables.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nplt.scatter(\"alcohol\", \"quality\", data=white_wine_df, alpha=0.01)\nplt.title(\"White Wine Alchohol Content vs. Quality\")\n\nplt.figure()\nplt.scatter(\"alcohol\", \"quality\", data=red_wine_df, alpha=0.01)\nplt.title(\"Red Wine Alchohol Content vs. Quality\")\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\nText(0.5, 1.0, 'Red Wine Alchohol Content vs. Quality')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-2.png){width=558 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-3.png){width=558 height=431}\n:::\n:::\n\n\nThe relationship between the variables doesn't seem to follow a linear pattern. Nevertheless, for the sake of exploration, we can attempt to fit a linear model to it. To start, we'll divide the data into a training set and a test set.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nX_white = white_wine_df[\"alcohol\"].to_numpy().reshape(-1, 1) # Reshaped for a single feature\ny_white = white_wine_df[\"quality\"].to_numpy().reshape(-1, 1)\nX_train_white, X_test_white, y_train_white, y_test_white = train_test_split(X_white, y_white, test_size=0.2, random_state=42)\n\nX_red = red_wine_df[\"alcohol\"].to_numpy().reshape(-1, 1)\ny_red = red_wine_df[\"quality\"].to_numpy().reshape(-1, 1)\n\nX_train_red, X_test_red, y_train_red, y_test_red = train_test_split(X_red, y_red, test_size=0.2, random_state=42)\n```\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LinearRegression\nwhite_linear_regression_model = LinearRegression()\nwhite_linear_regression_model.fit(X_train_white, y_train_white)\n\nred_linear_regression_model = LinearRegression()\nred_linear_regression_model.fit(X_train_red, y_train_red)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\nHaving fitted the model, let's now assess its predictive performance. In this case, we'll evaluate its accuracy using metrics such as Mean Squared Error (MSE) and Root Mean Squared Error (RMSE).\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nfrom sklearn.metrics import mean_squared_error\nwhite_predictions = white_linear_regression_model.predict(X_test_white)\nred_predictions = red_linear_regression_model.predict(X_test_red)\n\nwhite_error = mean_squared_error(y_test_white, white_predictions)\nred_error = mean_squared_error(y_test_red, red_predictions)\n\nprint(f\"The MSE for white wine is {white_error} and RMSE of {white_error ** 0.5}\")\nprint(f\"The MSE for red wine is {red_error} and RMSE of {red_error ** 0.5}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe MSE for white wine is 0.6177518759003805 and RMSE of 0.7859719307331404\nThe MSE for red wine is 0.4995281340730445 and RMSE of 0.7067730428313211\n```\n:::\n:::\n\n\nAt first glance, the RMSE of 0.78 may seem promising. However, it's important to consider that the data is heavily concentrated on a few data points. Let's explore a more robust technique—multiple linear regression.\n\nMultiple linear regression, akin to linear regression, involves using multiple variables to model the data. The next significant variables correlating with red wine quality are volatile acidity and density in white wine. We can fit a model in a similar fashion.\n\nFirstly, we divide the data into a test and train set, mirroring the previous procedure. However, this time, we include \"volatile acidity\" as part of our predictor variables (X) for red wine and \"density\" for white wine.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nX_white = white_wine_df[[\"alcohol\", \"density\"]].to_numpy()\ny_white = white_wine_df[\"quality\"].to_numpy()\nX_train_white, X_test_white, y_train_white, y_test_white = train_test_split(X_white, y_white, test_size=0.2, random_state=42)\n\nX_red = red_wine_df[[\"alcohol\", \"volatile acidity\"]].to_numpy()\ny_red = red_wine_df[\"quality\"].to_numpy()\n\nX_train_red, X_test_red, y_train_red, y_test_red = train_test_split(X_red, y_red, test_size=0.2, random_state=42)\n```\n:::\n\n\nThe sklearn linear regression model supports multiple linear regression right out of the box. Therefore, the next code block doesn't introduce any new, ground-breaking, concepts. We simply supply it with a slightly modified X, incorporating the additional predictor variables.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nwhite_linear_regression_model = LinearRegression()\nwhite_linear_regression_model.fit(X_train_white, y_train_white)\n\nred_linear_regression_model = LinearRegression()\nred_linear_regression_model.fit(X_train_red, y_train_red)\n\nwhite_predictions = white_linear_regression_model.predict(X_test_white)\nred_predictions = red_linear_regression_model.predict(X_test_red)\n\nwhite_error = mean_squared_error(y_test_white, white_predictions)\nred_error = mean_squared_error(y_test_red, red_predictions)\n\nprint(f\"The MSE for white wine is {white_error} and RMSE of {white_error ** 0.5}\")\nprint(f\"The MSE for red wine is {red_error} and RMSE of {red_error ** 0.5}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe MSE for white wine is 0.6160746073023867 and RMSE of 0.7849042026275479\nThe MSE for red wine is 0.41775613260831734 and RMSE of 0.646340570139549\n```\n:::\n:::\n\n\nWhen provided with more than one variable, the linear regression model demonstrated improved fitting capabilities on the red wine dataset. However, the RMSE for the white wine dataset remained unaltered, and overall, the RMSE remains relatively high. Perhaps it's worth considering a more potent approach to address the issue.\n\n### Random Forrest\nThe random forest model is significantly more potent than linear regression, albeit at the expense of managing more hyperparameters and being more computationally expensive. To achieve the optimal model, we'll need to fine-tune some of these hyperparameters through a grid search. Although computationally more intensive, this approach ensures the selection of the best hyperparameters for the specific dataset.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nX_white = white_wine_df.loc[:, white_wine_df.columns != \"quality\"].to_numpy()\ny_white = white_wine_df[\"quality\"].to_numpy()\nX_train_white, X_test_white, y_train_white, y_test_white = train_test_split(X_white, y_white, test_size=0.2, random_state=42)\n\nX_red = red_wine_df.loc[:, white_wine_df.columns != \"quality\"].to_numpy()\ny_red = red_wine_df[\"quality\"].to_numpy()\nX_train_red, X_test_red, y_train_red, y_test_red = train_test_split(X_red, y_red, test_size=0.2, random_state=42)\n```\n:::\n\n\nHaving re-split our data, we can now perform the grid search. The parameters slated for tuning are n_estimators and max_depth. n_estimators governs the number of trees in the forest, while max_depth regulates the maximum depth of each tree.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nparameters = {\"n_estimators\": list(range(1,14)), \n              \"max_depth\": list(range(5, 20)) + [None]} # None corresponds to no limit on the depth.\nclf_white = GridSearchCV(RandomForestRegressor(), param_grid=parameters, n_jobs=8)\nwhite_tuned_random_forrest = clf_white.fit(X_train_white, y_train_white)\n\nclf_red = GridSearchCV(RandomForestRegressor(), param_grid=parameters, n_jobs=8)\nred_tuned_random_forrest = clf_red.fit(X_train_red, y_train_red)\n```\n:::\n\n\nNow that we have a model for each dataset, let's assess their performance.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nwhite_predictions = white_tuned_random_forrest.predict(X_test_white)\nred_predictions = red_tuned_random_forrest.predict(X_test_red)\n\nwhite_error = mean_squared_error(y_test_white, white_predictions)\nred_error = mean_squared_error(y_test_red, red_predictions)\n\nprint(f\"The MSE for white wine is {white_error} and RMSE of {white_error ** 0.5}\")\nprint(f\"The MSE for red wine is {red_error} and RMSE of {red_error ** 0.5}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe MSE for white wine is 0.3725697379543533 and RMSE of 0.610384909671228\nThe MSE for red wine is 0.31858549155336 and RMSE of 0.5644337795998393\n```\n:::\n:::\n\n\nAt the cost of higher training time, the model we've developed is substantially more accurate than the linear regression model. To the extent that I'm quite confident we could produce a robust model without separating the white and red wine datasets.\n\nLet's combine our datasets and then split them for training.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\ncombined_df = pd.concat([white_wine_df, red_wine_df])\nX_combined = combined_df.loc[:, combined_df.columns != \"quality\"].to_numpy()\ny_combined = combined_df[\"quality\"].to_numpy()\n\nX_train_combined, X_test_combined, y_train_combined, y_test_combined = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)\n```\n:::\n\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nparameters = {\"n_estimators\": list(range(1,14)), \n              \"max_depth\": list(range(5, 20)) + [None]} # None corresponds to no limit on the depth.\nclf_combined = GridSearchCV(RandomForestRegressor(), param_grid=parameters, n_jobs=8)\ncombined_tuned_random_forrest = clf_combined.fit(X_train_combined, y_train_combined)\n```\n:::\n\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\ncombined_prediction = combined_tuned_random_forrest.predict(X_test_combined)\ncombined_error = mean_squared_error(y_test_combined, combined_prediction)\nprint(f\"The MSE for white wine is {combined_error} and RMSE of {combined_error ** 0.5}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe MSE for white wine is 0.3524850101553075 and RMSE of 0.5937044804911846\n```\n:::\n:::\n\n\nEven with the combined dataset, it's apparent that our model can effectively distinguish between white and red wine, assigning a quality score that outperforms the simple linear regression model.\n\n## References\n[1] P. Cortez, et al. \"Modeling wine preferences by data mining from physicochemical properties,\" in Decis. Support Syst., vol. 47, pp. 547-553, 2009.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}