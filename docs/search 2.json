[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog is lead as part of a machine learning class I am currently taking at Virginia Tech. In this blog I will be exploring machine learning related topics. There will be 5 posts on this website. Each post will be discussing a different machine learning related topic. Each post will include some code and visualizations. The code is executed on my laptop, so you should be able to replecate the results fairly easily without requiring an extremly powerful machine."
  },
  {
    "objectID": "posts/Distributions/index.html",
    "href": "posts/Distributions/index.html",
    "title": "Naive Bayes with Diabetes Dataset",
    "section": "",
    "text": "Kristina Armitage/Quanta Magazine\n\n\nIn this blog we will be discussing Naive Bayes and Logistic Regression. The reason for such a selection of topics is the comonality that both of these algorithms share. Both of these algorithms are probabilistic classifiers. This means that both of these methods can output not only their prediction for an outcome, but also the probability of the outcome. We will see the math behind how these models go about doing it as we progress through the blog. First, we should discuss the data.\n\n\nThe current version of the diabetes dataset was taken from kaggle. However, it is only a subset of the full dataset owned by the National Institute of Diabetes and Digestive and Kidney Diseases. This particular subset deals with specifically with women over the age of 21 of the Pima Indian heritage. The dataset contains the following columns:\n\nPregnancies - the number of pregnancies the woman has gone through.\nGlucose - Glucose level.\nBloodPressure - Diastolic blood pressure in mm Hg.\nSkinThickness - Triceps skin fold thickness in mm\nInsulin - 2-Hour serum insulin test result in mu U/ml\nBMI - Body mass index\nDiabetesPedigreeFunction - Diabetes pedigree function\nAge - The age\nOutcome - Outcome where 1 indicates a positive test result.\n\nAs usual, we will be loading the dataset through pandas.\n\nimport pandas as pd\ndata = pd.read_csv('diabetes.csv')\ndata.head()\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\n0\n6\n148\n72\n35\n0\n33.6\n0.627\n50\n1\n\n\n1\n1\n85\n66\n29\n0\n26.6\n0.351\n31\n0\n\n\n2\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n3\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\n\n\n4\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1\n\n\n\n\n\n\n\nLet’s take a quick look at the dataset information to undestand the column compostion.\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 768 entries, 0 to 767\nData columns (total 9 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   Pregnancies               768 non-null    int64  \n 1   Glucose                   768 non-null    int64  \n 2   BloodPressure             768 non-null    int64  \n 3   SkinThickness             768 non-null    int64  \n 4   Insulin                   768 non-null    int64  \n 5   BMI                       768 non-null    float64\n 6   DiabetesPedigreeFunction  768 non-null    float64\n 7   Age                       768 non-null    int64  \n 8   Outcome                   768 non-null    int64  \ndtypes: float64(2), int64(7)\nmemory usage: 54.1 KB\n\n\nEvery single column in the dataset is numeric. Let us take a look at the distribution of the data in each column.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.color_palette(\"rocket\", as_cmap=True)\nfor col in data.columns:\n    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15,7))\n    ax1.boxplot(data[col])\n    sns.kdeplot(data[col], ax=ax2)\n    plt.title(col)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that certain values here were imputed as some of the values can never be equal to 0 in the real world. For example, neither BMI or blood preassure can be equal to 0, yet we do see some values in the columns that are equal to 0. We could impute these values using a mean, median, or perhaps even KNN-imputer in order to try and improve the performance of the model. Yet surprisingly, when I tried to do so, the models performed worse when I tried to impute these value. So, they will stay as they are. Next, we will look at the distributions of the data per outcome.\n\nfor col in data.columns:\n    if col == \"Outcome\":\n        continue\n    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15,7))\n    positives = data[\"Outcome\"] == 1\n    negatives = data[\"Outcome\"] == 0\n    ax1.boxplot(data[col].loc[positives], positions=[1])\n    ax1.boxplot(data[col].loc[negatives], positions=[2])\n    ax1.set_xticklabels([\"Positive\", \"Negative\"])\n    ax1.set_title(f\"{col} Boxplot\")\n    sns.kdeplot(data[col].loc[positives], ax=ax2, label=\"Positive\")\n    sns.kdeplot(data[col].loc[negatives], ax=ax2, label=\"Negative\")\n    plt.legend(loc=\"best\")\n    plt.title(f\"{col} KDE\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInrestingly, the distributions for both the blood pressure and skin thickness appear to be very similar to each other regardless of the outcome. We could experiment and try to remove these variables from the model down the line and see if we are able to achieve better performance. For now, let’s check if we can spot anything interesting happening when we project the data into 2 dimensions.\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\n\ny = data[\"Outcome\"]\nX = data.drop(\"Outcome\", axis=1)\n\npipeline = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"pca\", PCA(n_components=2)),\n])\n\npca_data = pd.DataFrame(\n    pipeline.fit_transform(X),\n    columns=[\"PC1\", \"PC2\"],\n    index=data.index,\n)\n\nplt.scatter(pca_data[\"PC1\"], pca_data[\"PC2\"], c=y, cmap=\"viridis\")\nplt.title(\"Projection of the Diabetes Data\")\n\nText(0.5, 1.0, 'Projection of the Diabetes Data')\n\n\n\n\n\nUnfortunatley, the difference between the two distributions is a bit difficult to spot in these two dimension.\n\n\n\nWe will now proceed to train and test the naive bayes model. As usual, we should split the data into train and test sets.\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nAlright, now let us train the model, but first - some theory.\nAs mentioned above, naive bayes is a probabilistic classifier, so it should be able to output the probabilities of each outcome along with its prediction.\nIt does so by computing \\(P(Y | X)\\) where \\(Y\\) represents the classified variable and X represents the feature variables. According to Bayes theorem (that’s where the method get’s its name): \\[\nP(Y | X) = \\frac{P(X | Y) P(Y)}{P(X)}\n\\] Although this equation looks simple, it becomes very difficult to compute as \\(X\\) grows larger. This is because we have to calculate \\(P(X | Y)\\). For example, assume X consists of two features \\(X_1\\) and \\(X_2\\). Then, we have to calculate \\[\nP(Y | X_1, X_2) = \\frac{P(X_1, X_2 | Y) P(Y)}{P(X_1, X_2)}\n\\] So, now we need data where all the three variables (\\(X_1\\), \\(X_2\\), \\(Y\\)) interact. However, naive bayes makes a very crucial assumption. It assumes that the variables are independent. Thus, our equation is simplified to \\[\nP(Y | X_1, X_2) = P(X_1 | Y) P(X_2 | Y)\n\\] Now, we no longer need the data on how \\(X_1\\) and \\(X_2\\) interact with each other and only need to know how they interact with \\(Y\\). This means that we need significantly less data and the computation becomes much cheaper.\nYou may be concerned that the assumption of independnce is flawed since variables in most data are going to have some degree of dependence on each other. That is a good concern to have. However, naive bayes generally tends to perform well even when that assumption is violated.\nSo, when given observations \\(X_1, X_2, ..., X_n\\), naive baise calculates the probabilities for all the possible outcomes \\(Y\\) and then selects the outcome with the highest probability. This is how we both get a probability estimate and a prediction.\nNow, we are ready to train the model.\n\nfrom sklearn.metrics import ConfusionMatrixDisplay, f1_score, accuracy_score\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(X_train, y_train)\nprediction = nb.predict(X_test)\ncmd = ConfusionMatrixDisplay.from_estimator(nb, X_test, y_test)\nprint(\"Accuracy: \", accuracy_score(prediction, y_test))\nprint(\"F1 score: \", f1_score(prediction, y_test))\n\nAccuracy:  0.7662337662337663\nF1 score:  0.6842105263157895\n\n\n\n\n\nWe can see that the model struggles a little most likely due to the inbalance in the data. However, this is a fairly good performance for such a simple model. Now, let’s look how we can attein the prediction probabilities. We will work with a single sample, but the approach can also be applied to multiple samples.\n\nx1 = X_test.iloc[0]\nx1\n\nPregnancies                   6.00\nGlucose                      98.00\nBloodPressure                58.00\nSkinThickness                33.00\nInsulin                     190.00\nBMI                          34.00\nDiabetesPedigreeFunction      0.43\nAge                          43.00\nName: 668, dtype: float64\n\n\nThis is our input.\n\nx1 = x1.to_numpy().reshape(1, -1)\nprobs = nb.predict_proba(x1)[0]\nprobs\n\n/Users/danielsabanov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but GaussianNB was fitted with feature names\n  warnings.warn(\n\n\narray([0.73024358, 0.26975642])\n\n\nSince the largest probability is 0.73 and corresponds to the first class, we know that our prediction is “0”. Thus, this specific patient is predicted to not have diabetes. Let’s see what was the true outcome.\n\ny_test.iloc[0]\n\n0\n\n\nAnd that checks out!\n\n\n\nRemember the assumption that naive bayes makes? Well, so far we do not know if the assumption was violated. Even though naive bayes tends to perform well even when its assumptions are violated we may be able to improve performance if we reduce the number of correlated variables. We should take a look at the correlation matrix.\n\nsns.heatmap(X_train.corr(), annot=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\nIt looks like the number of pregnancies is correlated with age, which makes sense. Additionally we can see that there is a correlation between skin thickness and BMI. We could try to remove these variables from the equation and see how the model performs.\n\nreduced_X_train = X_train.drop(columns=[\"Pregnancies\", \"SkinThickness\"])\nreduced_X_test = X_test.drop(columns=[\"Pregnancies\", \"SkinThickness\"])\n\nnb = GaussianNB()\nnb.fit(reduced_X_train, y_train)\nprediction = nb.predict(reduced_X_test)\ncmd = ConfusionMatrixDisplay.from_estimator(nb, reduced_X_test, y_test)\nprint(\"Accuracy: \", accuracy_score(prediction, y_test))\nprint(\"F1 score: \", f1_score(prediction, y_test))\n\nAccuracy:  0.7792207792207793\nF1 score:  0.679245283018868\n\n\n\n\n\nIt looks like we were not able to improve the predictive power of the model since even though our accuracy went up our f1 score dropped. This is because we now have more false negatives. Never the less, we were able to reduce the number of variables that we are working with, while maintaining most of the predictive power of the model. However, if we have access to these variables, they should probably still kept within the model.\nNext, we will be examining a different type of model, that also relies on probability.\n\n\n\nSimilarly to naive bayes, logistic regression is also capable to predict the probabilities of its predictions. Let’s examine the math behind it.\nWe know that the linear regression equaiton is given by: \\[\ny = \\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n\n\\]\nHowever, instead of trying to predict \\(y\\) directly. We will be trying to predict the log odds of a class. So, we have: \\[\n\\log(odds) = \\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n\n\\]\nWe know that probability can be predicted using odds with the following equation: \\[\np = \\frac{odds}{1 + odds}\n\\]\nWe can substitute that into our equaiton, but before we do we first need to find the odds equation. So we exponentiate the function. \\[\nodds = e^{\\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n}\n\\]\nNow we substitute that into our probability function.\n\\[\np = \\frac{e^{\\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n}}{1 + e^{\\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n}}\n\\] We can simplify the equation by dividing by the odds function. We end up with. \\[\np = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n)}}\n\\]\nNow, we have an equation that can predict a probability of a specific class given \\(X\\).\nTraining the model is very easy with scikit-learn.\n\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(max_iter=1000)\nlr.fit(X_train, y_train)\ncmd = ConfusionMatrixDisplay.from_estimator(lr, X_test, y_test)\nprediction = lr.predict(X_test)\nprint(\"Accuracy: \", accuracy_score(prediction, y_test))\nprint(\"F1 score: \", f1_score(prediction, y_test))\n\nAccuracy:  0.7467532467532467\nF1 score:  0.6548672566371682\n\n\n\n\n\nWe can see that the model was able to perform very similarly to the naive bayes model.\n\n\n\nLogistic regression assumest that there is no multicollinearity in the data. That means no variable can be linearly related to other variables. We already know we have some variables that are somewhat related to each other. Let’s see how the model performs when we drop the related columns.\n\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(max_iter=1000)\nlr.fit(reduced_X_train, y_train)\ncmd = ConfusionMatrixDisplay.from_estimator(lr, reduced_X_test, y_test)\nprediction = lr.predict(reduced_X_test)\nprint(\"Accuracy: \", accuracy_score(prediction, y_test))\nprint(\"F1 score: \", f1_score(prediction, y_test))\n\nAccuracy:  0.7597402597402597\nF1 score:  0.6542056074766355\n\n\n\n\n\nIn this case there was no significant impact on the performance of the model. Yet, we were still able to reduce the amount of data that the model requires for training. However, if this model was to be used in the real world, I would recommend keeping the variables as our model sensitivite has decreased. It is much better to have a false positive in a medical setting than a false negative since we rist patients going without treatment."
  },
  {
    "objectID": "posts/Distributions/index.html#diabetes-dataset",
    "href": "posts/Distributions/index.html#diabetes-dataset",
    "title": "Naive Bayes with Diabetes Dataset",
    "section": "",
    "text": "The current version of the diabetes dataset was taken from kaggle. However, it is only a subset of the full dataset owned by the National Institute of Diabetes and Digestive and Kidney Diseases. This particular subset deals with specifically with women over the age of 21 of the Pima Indian heritage. The dataset contains the following columns:\n\nPregnancies - the number of pregnancies the woman has gone through.\nGlucose - Glucose level.\nBloodPressure - Diastolic blood pressure in mm Hg.\nSkinThickness - Triceps skin fold thickness in mm\nInsulin - 2-Hour serum insulin test result in mu U/ml\nBMI - Body mass index\nDiabetesPedigreeFunction - Diabetes pedigree function\nAge - The age\nOutcome - Outcome where 1 indicates a positive test result.\n\nAs usual, we will be loading the dataset through pandas.\n\nimport pandas as pd\ndata = pd.read_csv('diabetes.csv')\ndata.head()\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\n0\n6\n148\n72\n35\n0\n33.6\n0.627\n50\n1\n\n\n1\n1\n85\n66\n29\n0\n26.6\n0.351\n31\n0\n\n\n2\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n3\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\n\n\n4\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1\n\n\n\n\n\n\n\nLet’s take a quick look at the dataset information to undestand the column compostion.\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 768 entries, 0 to 767\nData columns (total 9 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   Pregnancies               768 non-null    int64  \n 1   Glucose                   768 non-null    int64  \n 2   BloodPressure             768 non-null    int64  \n 3   SkinThickness             768 non-null    int64  \n 4   Insulin                   768 non-null    int64  \n 5   BMI                       768 non-null    float64\n 6   DiabetesPedigreeFunction  768 non-null    float64\n 7   Age                       768 non-null    int64  \n 8   Outcome                   768 non-null    int64  \ndtypes: float64(2), int64(7)\nmemory usage: 54.1 KB\n\n\nEvery single column in the dataset is numeric. Let us take a look at the distribution of the data in each column.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.color_palette(\"rocket\", as_cmap=True)\nfor col in data.columns:\n    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15,7))\n    ax1.boxplot(data[col])\n    sns.kdeplot(data[col], ax=ax2)\n    plt.title(col)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that certain values here were imputed as some of the values can never be equal to 0 in the real world. For example, neither BMI or blood preassure can be equal to 0, yet we do see some values in the columns that are equal to 0. We could impute these values using a mean, median, or perhaps even KNN-imputer in order to try and improve the performance of the model. Yet surprisingly, when I tried to do so, the models performed worse when I tried to impute these value. So, they will stay as they are. Next, we will look at the distributions of the data per outcome.\n\nfor col in data.columns:\n    if col == \"Outcome\":\n        continue\n    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15,7))\n    positives = data[\"Outcome\"] == 1\n    negatives = data[\"Outcome\"] == 0\n    ax1.boxplot(data[col].loc[positives], positions=[1])\n    ax1.boxplot(data[col].loc[negatives], positions=[2])\n    ax1.set_xticklabels([\"Positive\", \"Negative\"])\n    ax1.set_title(f\"{col} Boxplot\")\n    sns.kdeplot(data[col].loc[positives], ax=ax2, label=\"Positive\")\n    sns.kdeplot(data[col].loc[negatives], ax=ax2, label=\"Negative\")\n    plt.legend(loc=\"best\")\n    plt.title(f\"{col} KDE\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInrestingly, the distributions for both the blood pressure and skin thickness appear to be very similar to each other regardless of the outcome. We could experiment and try to remove these variables from the model down the line and see if we are able to achieve better performance. For now, let’s check if we can spot anything interesting happening when we project the data into 2 dimensions.\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\n\ny = data[\"Outcome\"]\nX = data.drop(\"Outcome\", axis=1)\n\npipeline = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"pca\", PCA(n_components=2)),\n])\n\npca_data = pd.DataFrame(\n    pipeline.fit_transform(X),\n    columns=[\"PC1\", \"PC2\"],\n    index=data.index,\n)\n\nplt.scatter(pca_data[\"PC1\"], pca_data[\"PC2\"], c=y, cmap=\"viridis\")\nplt.title(\"Projection of the Diabetes Data\")\n\nText(0.5, 1.0, 'Projection of the Diabetes Data')\n\n\n\n\n\nUnfortunatley, the difference between the two distributions is a bit difficult to spot in these two dimension."
  },
  {
    "objectID": "posts/Distributions/index.html#training-naive-bayes",
    "href": "posts/Distributions/index.html#training-naive-bayes",
    "title": "Naive Bayes with Diabetes Dataset",
    "section": "",
    "text": "We will now proceed to train and test the naive bayes model. As usual, we should split the data into train and test sets.\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nAlright, now let us train the model, but first - some theory.\nAs mentioned above, naive bayes is a probabilistic classifier, so it should be able to output the probabilities of each outcome along with its prediction.\nIt does so by computing \\(P(Y | X)\\) where \\(Y\\) represents the classified variable and X represents the feature variables. According to Bayes theorem (that’s where the method get’s its name): \\[\nP(Y | X) = \\frac{P(X | Y) P(Y)}{P(X)}\n\\] Although this equation looks simple, it becomes very difficult to compute as \\(X\\) grows larger. This is because we have to calculate \\(P(X | Y)\\). For example, assume X consists of two features \\(X_1\\) and \\(X_2\\). Then, we have to calculate \\[\nP(Y | X_1, X_2) = \\frac{P(X_1, X_2 | Y) P(Y)}{P(X_1, X_2)}\n\\] So, now we need data where all the three variables (\\(X_1\\), \\(X_2\\), \\(Y\\)) interact. However, naive bayes makes a very crucial assumption. It assumes that the variables are independent. Thus, our equation is simplified to \\[\nP(Y | X_1, X_2) = P(X_1 | Y) P(X_2 | Y)\n\\] Now, we no longer need the data on how \\(X_1\\) and \\(X_2\\) interact with each other and only need to know how they interact with \\(Y\\). This means that we need significantly less data and the computation becomes much cheaper.\nYou may be concerned that the assumption of independnce is flawed since variables in most data are going to have some degree of dependence on each other. That is a good concern to have. However, naive bayes generally tends to perform well even when that assumption is violated.\nSo, when given observations \\(X_1, X_2, ..., X_n\\), naive baise calculates the probabilities for all the possible outcomes \\(Y\\) and then selects the outcome with the highest probability. This is how we both get a probability estimate and a prediction.\nNow, we are ready to train the model.\n\nfrom sklearn.metrics import ConfusionMatrixDisplay, f1_score, accuracy_score\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(X_train, y_train)\nprediction = nb.predict(X_test)\ncmd = ConfusionMatrixDisplay.from_estimator(nb, X_test, y_test)\nprint(\"Accuracy: \", accuracy_score(prediction, y_test))\nprint(\"F1 score: \", f1_score(prediction, y_test))\n\nAccuracy:  0.7662337662337663\nF1 score:  0.6842105263157895\n\n\n\n\n\nWe can see that the model struggles a little most likely due to the inbalance in the data. However, this is a fairly good performance for such a simple model. Now, let’s look how we can attein the prediction probabilities. We will work with a single sample, but the approach can also be applied to multiple samples.\n\nx1 = X_test.iloc[0]\nx1\n\nPregnancies                   6.00\nGlucose                      98.00\nBloodPressure                58.00\nSkinThickness                33.00\nInsulin                     190.00\nBMI                          34.00\nDiabetesPedigreeFunction      0.43\nAge                          43.00\nName: 668, dtype: float64\n\n\nThis is our input.\n\nx1 = x1.to_numpy().reshape(1, -1)\nprobs = nb.predict_proba(x1)[0]\nprobs\n\n/Users/danielsabanov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but GaussianNB was fitted with feature names\n  warnings.warn(\n\n\narray([0.73024358, 0.26975642])\n\n\nSince the largest probability is 0.73 and corresponds to the first class, we know that our prediction is “0”. Thus, this specific patient is predicted to not have diabetes. Let’s see what was the true outcome.\n\ny_test.iloc[0]\n\n0\n\n\nAnd that checks out!"
  },
  {
    "objectID": "posts/Distributions/index.html#examining-possible-improvements",
    "href": "posts/Distributions/index.html#examining-possible-improvements",
    "title": "Naive Bayes with Diabetes Dataset",
    "section": "",
    "text": "Remember the assumption that naive bayes makes? Well, so far we do not know if the assumption was violated. Even though naive bayes tends to perform well even when its assumptions are violated we may be able to improve performance if we reduce the number of correlated variables. We should take a look at the correlation matrix.\n\nsns.heatmap(X_train.corr(), annot=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\nIt looks like the number of pregnancies is correlated with age, which makes sense. Additionally we can see that there is a correlation between skin thickness and BMI. We could try to remove these variables from the equation and see how the model performs.\n\nreduced_X_train = X_train.drop(columns=[\"Pregnancies\", \"SkinThickness\"])\nreduced_X_test = X_test.drop(columns=[\"Pregnancies\", \"SkinThickness\"])\n\nnb = GaussianNB()\nnb.fit(reduced_X_train, y_train)\nprediction = nb.predict(reduced_X_test)\ncmd = ConfusionMatrixDisplay.from_estimator(nb, reduced_X_test, y_test)\nprint(\"Accuracy: \", accuracy_score(prediction, y_test))\nprint(\"F1 score: \", f1_score(prediction, y_test))\n\nAccuracy:  0.7792207792207793\nF1 score:  0.679245283018868\n\n\n\n\n\nIt looks like we were not able to improve the predictive power of the model since even though our accuracy went up our f1 score dropped. This is because we now have more false negatives. Never the less, we were able to reduce the number of variables that we are working with, while maintaining most of the predictive power of the model. However, if we have access to these variables, they should probably still kept within the model.\nNext, we will be examining a different type of model, that also relies on probability."
  },
  {
    "objectID": "posts/Distributions/index.html#training-logistic-regression",
    "href": "posts/Distributions/index.html#training-logistic-regression",
    "title": "Naive Bayes with Diabetes Dataset",
    "section": "",
    "text": "Similarly to naive bayes, logistic regression is also capable to predict the probabilities of its predictions. Let’s examine the math behind it.\nWe know that the linear regression equaiton is given by: \\[\ny = \\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n\n\\]\nHowever, instead of trying to predict \\(y\\) directly. We will be trying to predict the log odds of a class. So, we have: \\[\n\\log(odds) = \\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n\n\\]\nWe know that probability can be predicted using odds with the following equation: \\[\np = \\frac{odds}{1 + odds}\n\\]\nWe can substitute that into our equaiton, but before we do we first need to find the odds equation. So we exponentiate the function. \\[\nodds = e^{\\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n}\n\\]\nNow we substitute that into our probability function.\n\\[\np = \\frac{e^{\\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n}}{1 + e^{\\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n}}\n\\] We can simplify the equation by dividing by the odds function. We end up with. \\[\np = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n)}}\n\\]\nNow, we have an equation that can predict a probability of a specific class given \\(X\\).\nTraining the model is very easy with scikit-learn.\n\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(max_iter=1000)\nlr.fit(X_train, y_train)\ncmd = ConfusionMatrixDisplay.from_estimator(lr, X_test, y_test)\nprediction = lr.predict(X_test)\nprint(\"Accuracy: \", accuracy_score(prediction, y_test))\nprint(\"F1 score: \", f1_score(prediction, y_test))\n\nAccuracy:  0.7467532467532467\nF1 score:  0.6548672566371682\n\n\n\n\n\nWe can see that the model was able to perform very similarly to the naive bayes model."
  },
  {
    "objectID": "posts/Distributions/index.html#trying-to-improve-performance",
    "href": "posts/Distributions/index.html#trying-to-improve-performance",
    "title": "Naive Bayes with Diabetes Dataset",
    "section": "",
    "text": "Logistic regression assumest that there is no multicollinearity in the data. That means no variable can be linearly related to other variables. We already know we have some variables that are somewhat related to each other. Let’s see how the model performs when we drop the related columns.\n\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(max_iter=1000)\nlr.fit(reduced_X_train, y_train)\ncmd = ConfusionMatrixDisplay.from_estimator(lr, reduced_X_test, y_test)\nprediction = lr.predict(reduced_X_test)\nprint(\"Accuracy: \", accuracy_score(prediction, y_test))\nprint(\"F1 score: \", f1_score(prediction, y_test))\n\nAccuracy:  0.7597402597402597\nF1 score:  0.6542056074766355\n\n\n\n\n\nIn this case there was no significant impact on the performance of the model. Yet, we were still able to reduce the amount of data that the model requires for training. However, if this model was to be used in the real world, I would recommend keeping the variables as our model sensitivite has decreased. It is much better to have a false positive in a medical setting than a false negative since we rist patients going without treatment."
  },
  {
    "objectID": "posts/KNN-text-classification/index.html",
    "href": "posts/KNN-text-classification/index.html",
    "title": "KNN for Text Classification",
    "section": "",
    "text": "Taken From Mac Business Solutions\n\n\n\n\nNeural networks (NN), deep learning (DL), and natural language processing (NLP) are all the rage today, and not without reason. In 2017, Google published “Attention Is All You Need” [1]. This paper presented the transformer architecture, which would later be used in a large variety of NLP models such as Google’s BERT and OpenAI’s GPT, which use transformers within their architecture.\nTransformer architectures show great promise as they tend to be more parallelizable, require less time to train, and tend to be of higher quality [1]. However, neural networks, especially deep neural networks, tend to be computationally expensive and require large datasets to train. Thus, a team from the University of Waterloo proposed an alternative method for classifying text using a GZIP compressor and the K-Nearest Neighbor clustering algorithm.\nThe team demonstrated the viability of the method on a variety of datasets, testing the accuracy compared to non-pretrained datasets. As it turns out, their model is able to compete with the larger models and was even capable of outperforming BERT in some cases.\nIn the following blog post, I plan to try and recreate the said model on a different dataset to see how accurate the model is and to learn to apply it myself. Specifically, I want to apply the model to the most classic example of outlier detection - spam or ham!\n\n\n\nThe algorithm is simple and consists of KNN as the classification method and GZIP as the distance metric, but what does it mean?\n\n\nKNN (also known as KNN) is a simple and intuitive machine learning algorithm used for classification and regression tasks. It works by finding the K closest data points (neighbors) to a given input data point in a dataset and then classifying or predicting the target variable based on the majority class or average value of those K neighbors.\n\n\n\nGZIP is a file compression utility that is typically used for compressing files to be transfered over network or for storage. You most likely have used GZIP before in order to do that. An interesting detail of the GZIP compression is that repetative texts will be compressed into smaller sizes than texts that are not repetative. This is due to the fact that the algorithm uses Huffman coding, which replaces the most repeated sequences with shorter sequences, and LZ77, which stores references to repeated sequnces instead of using the sequences themselves (think of it like having a variable assigned to sequence and then using that variable later instead of the sequence).\n\n\n\nKNN can be used in order to classifiy different sentences. However, to classify something when using KNN we need a way to also measure the distance between two datapoints. This is where GZIP comes in. Since GZIP compresses repetative sequences into smaller sizes - similar sequences that are concatonated together will compress to smaller sizes as well. This allows us to formulate a distance metric based on how well a sequence is compressed.\nConsider an example: We have sequences x1, x2, and x3. Assume that x1 and x2 are similar sequences while x3 is less similar. Let C(x, y) be our GZIP compressor. Then it follows that: \\[\nsize(C(x1 + x2)) &lt; size(C(x1 + x3))\n\\]\nThis is the intuition behind the method. We will first begin by implementing the metric.\n\n\n\nThe most basic implementation of the metric would look like this, but the keen eyed may notice that it could be optimized.\n\nimport gzip as gz\n\n\ndef gzip_metric(x1, x2):\n    Cx1 = len(gz.compress(x1.encode()))\n    Cx2 = len(gz.compress(x2.encode()))\n\n    x1x2 = \" \".join([x1, x2])\n    Cx1x2 = len(gz.compress(x1x2.encode()))\n    ncd = (Cx1x2 - min(Cx1, Cx2)) / max(Cx1, Cx2)\n    return ncd\n\n\n\n\nNormally, we would use an already existing library such as Scikit-Learn and simply import the KNN algorithm from there (neighbors.KNeighborsClassifier). However, when trying to do that, I run into an interesting problem. Turns out that Scikit’s KNeighborsClassifier does not operate with strings and expects the user to encode them beforehand. We do not want to encode these strings since we want our GZIP metric to interact with the strings directly. Luckily, KNN is simple to implement by hand and is also implemented in the paper.\n\nimport pandas as pd\nimport numpy as np\n\n\ndef knn_classify(known_df: pd.DataFrame, unknown_sr: pd.Series, k: int = 3):\n    knowns: np.ndarray = known_df.to_numpy(copy=True)\n    unknowns: np.ndarray = unknown_sr.to_numpy(copy=True)\n    predictions = []\n    for x1 in unknowns:\n        distances_from_x1: np.array = np.array(\n            [gzip_metric(x1, x2) for x2, _ in knowns]\n        )\n        sorted_idx = np.argsort(distances_from_x1)\n        top_k_classes = knowns[sorted_idx[:k], 1]\n        values, counts = np.unique(top_k_classes, return_counts=True)\n        predicted_class = values[np.argmax(counts)]\n        predictions.append(predicted_class)\n\n    return predictions\n\n\n\n\nAs you have seen in the KNN section, x2 is computed multiple times for every x1. Therefore, when we use the gzip metric, we unecessarily compress x1 multiple times, which is time-consuming. So, instead, we could precompute that value inside our knn_function and pass the precomputed value into our metric.\n\ndef improved_gzip_metric(Cx1, x1, x2):\n    Cx2 = len(gz.compress(x2.encode()))\n    x1x2 = \" \".join([x1, x2])\n    Cx1x2 = len(gz.compress(x1x2.encode()))\n    ncd = (Cx1x2 - min(Cx1, Cx2)) / max(Cx1, Cx2)\n    return ncd\n\n\ndef improved_knn_classify(known_df: pd.DataFrame, unknown_sr: pd.Series, k: int = 3):\n    knowns: np.ndarray = known_df.to_numpy(copy=True)\n    unknowns: np.ndarray = unknown_sr.to_numpy(copy=True)\n    predictions = []\n    for x1 in unknowns:\n        Cx1 = len(gz.compress(x1.encode()))\n        distances_from_x1: np.array = np.array(\n            [improved_gzip_metric(Cx1, x1, x2) for x2, _ in knowns]\n        )\n        sorted_idx = np.argsort(distances_from_x1)\n        top_k_classes = knowns[sorted_idx[:k], 1]\n        values, counts = np.unique(top_k_classes, return_counts=True)\n        predicted_class = values[np.argmax(counts)]\n        predictions.append(predicted_class)\n\n    return predictions\n\n\n\n\n\nThe dataset I would like to look at was taken from kaggle and includes a collection of messages labeled as spam or not spam (also called ham).\nLet’s take a quick look at the data using the pandas library.\n\n\n\n# The dataset comes with a single file named train.csv. Interestingly, there is no test.csv.\ndf_spam = pd.read_csv(\"Spam_dataset/train.csv\") \ndf_spam[\"sms\"] = df_spam[\"sms\"].values.astype(\"str\")\ndf_spam.head()\n\n\n\n\n\n\n\n\nsms\nlabel\n\n\n\n\n0\nGo until jurong point, crazy.. Available only ...\n0\n\n\n1\nOk lar... Joking wif u oni...\\n\n0\n\n\n2\nFree entry in 2 a wkly comp to win FA Cup fina...\n1\n\n\n3\nU dun say so early hor... U c already then say...\n0\n\n\n4\nNah I don't think he goes to usf, he lives aro...\n0\n\n\n\n\n\n\n\nYou can see that the dataset is split into two columns. One containing the message, the second containing the label. Conviniently for us, the labels are already hot-encoded, so we do not need to go through the encoding step. Something to note, if the label is ‘0’ then the message is considered ham, if it is labled as ‘1’ the message is spam.\nLet’s quickly look at the distribution of the data.\n\ngrouped_counts = df_spam.groupby(\"label\").size()\ngrouped_counts.plot.bar(x=\"label\")\ngrouped_counts\n\nlabel\n0    4827\n1     747\ndtype: int64\n\n\n\n\n\nWe can see that the overwhelming number of labels belong to ‘ham’. When we split this data into a train and test dataset, we will need to pay special attention that both of the labels are present in both the testing and training datasets. To do that we can use sklearn’s train_test_split.\n\nfrom sklearn.model_selection import train_test_split\nX = df_spam[\"sms\"]\ny = df_spam[\"label\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\ntrain_df = pd.concat([X_train, y_train], axis=1)\ntest_df = pd.concat([X_test, y_test], axis=1)\n\nNow that the data has been split, we can use the algorithm we built to try and classify the text messages.\n\n\n\nFirst let’s try the unoptimized algorithm.\n\ny_bar_slow = knn_classify(train_df, X_test)\n\nThis cell took 3 minutes and 50 seconds to execute on my system. Now, we will run the improved implementation.\n\ny_bar = improved_knn_classify(train_df, X_test)\n\n\ny_bar == y_bar_slow\n\nTrue\n\n\nWe can see that the results are identical, yet the improved version runs faster. So, from now on, we will simply use the improved version. Let’s see how the model has performed.\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nconf_mat = confusion_matrix(y_test, y_bar)\nConfusionMatrixDisplay(conf_mat).plot()\n\nprint(\"f1:\", f1_score(y_test, y_bar))\nprint(\"accuracy:\", accuracy_score(y_test, y_bar))\n\nf1: 0.9547038327526132\naccuracy: 0.9883408071748879\n\n\n\n\n\nThese results are pretty impressive. However, this could be a result of a particularly lucky split. We will now try to perform cross validation while also experimenting with different sizes of K.\n\n\n\nFirst, need to consider the possible size of K. A good rule of thumb for KNN is to select a K that is equal to \\(\\sqrt(n)\\), where \\(n\\) is the number of datapoints in the training set. We have 4459 data points in our training set. That means that our K should be approximately \\(66\\). We could try multiple K values as well. We can try a list of \\(64, 65, 66, 67, 68\\) since these values are in the vicinity of our rule of thumb and also \\(3\\), since we have recieved good results from it.\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom multiprocess import Pool\n\nf1_scores = []\naccuracy_scores = []\nKs = [3, 64, 65, 66, 67, 68]\nskf = StratifiedKFold(n_splits=5)\nwith Pool() as pool:\n    for train_idx, test_idx in skf.split(X, y):\n        train_df = df_spam.iloc[train_idx]\n        test_df = df_spam.iloc[test_idx]\n        args = [(train_df, test_df[\"sms\"], K) for K in Ks]\n        y_bars = pool.starmap(improved_knn_classify, args)\n        accuracies = []\n        f1s = []\n        for y_bar in y_bars:\n            accuracies.append(accuracy_score(test_df[\"label\"], y_bar))\n            f1s.append(f1_score(test_df[\"label\"], y_bar))\n        \n        accuracy_scores.append(accuracies)\n        f1_scores.append(f1s)\n\nYou may notice that the above code uses a new 3rd party library names “multiprocess” (not to be confused with Python’s “multiprocessing” library). It is a fork of the “multiprocessing” built-in Python library that performs better in the Jupyter environment. We are using this library in order to be able to process multiple K values in parallel. Otherwise, this process would have taken significantly longer.\nNow, lets save our metric results as dataframes for further processing.\n\nf1_df = pd.DataFrame(f1_scores, columns=Ks)\naccuracy_df = pd.DataFrame(accuracy_scores, columns=Ks)\nprint(f1_df)\nprint(accuracy_df)\n\n         3         64        65        66        67        68\n0  0.944444  0.892989  0.897059  0.897059  0.897059  0.897059\n1  0.951389  0.897059  0.901099  0.897059  0.897059  0.897059\n2  0.936170  0.888889  0.888889  0.884758  0.884758  0.880597\n3  0.932862  0.872180  0.872180  0.867925  0.872180  0.867925\n4  0.954386  0.900369  0.900369  0.900369  0.900369  0.900369\n         3         64        65        66        67        68\n0  0.985650  0.973991  0.974888  0.974888  0.974888  0.974888\n1  0.987444  0.974888  0.975785  0.974888  0.974888  0.974888\n2  0.983857  0.973094  0.973094  0.972197  0.972197  0.971300\n3  0.982960  0.969507  0.969507  0.968610  0.969507  0.968610\n4  0.988330  0.975763  0.975763  0.975763  0.975763  0.975763\n\n\nWe can create a bar chart in order to understand the results a bit better. We will first look at the \\(F1\\) score.\n\nax = f1_df.plot.bar(rot=0, title=\"F1 Scores of Different Splits and Different K Values\")\nax.legend(loc='center left',bbox_to_anchor=(1.0, 0.5))\nax.set_xlabel(\"Split Index\")\nax.set_ylabel(\"F1 Score\")\n\nText(0, 0.5, 'F1 Score')\n\n\n\n\n\n\nax = accuracy_df.plot.bar(rot=0, title=\"Accuracy Scores of Different Splits and Different K Values\")\nax.legend(loc='center left',bbox_to_anchor=(1.0, 0.5))\nax.set_xlabel(\"Split Index\")\nax.set_ylabel(\"Accuracy Score\")\n\nText(0, 0.5, 'Accuracy Score')\n\n\n\n\n\nFrom both of these plots we can see that there is no significant difference between different splits of the dataset. Additionally, there does not appear to be a significant difference in performance of the K values that are closest to our rule of thumb. However, turns out that a K value of \\(3\\) is significantly more profitable than the other K values. We can take a look at the average score for \\(K=3\\) for each metric below:\n\nprint(\"F1 Average:\", f1_df[3].mean())\nprint(\"Accuracy Average:\", accuracy_df[3].mean())\n\nF1 Average: 0.9438503403648584\nAccuracy Average: 0.9856481310028903\n\n\n\n\n\n\n[1] A. Vaswani et al., “Attention is all you need,” arXiv.org, https://arxiv.org/abs/1706.03762. [2] Z. Jiang et al., “‘low-resource’ text classification: A parameter-free classification method with compressors,” ACL Anthology, https://aclanthology.org/2023.findings-acl.426/."
  },
  {
    "objectID": "posts/KNN-text-classification/index.html#introduction",
    "href": "posts/KNN-text-classification/index.html#introduction",
    "title": "KNN for Text Classification",
    "section": "",
    "text": "Neural networks (NN), deep learning (DL), and natural language processing (NLP) are all the rage today, and not without reason. In 2017, Google published “Attention Is All You Need” [1]. This paper presented the transformer architecture, which would later be used in a large variety of NLP models such as Google’s BERT and OpenAI’s GPT, which use transformers within their architecture.\nTransformer architectures show great promise as they tend to be more parallelizable, require less time to train, and tend to be of higher quality [1]. However, neural networks, especially deep neural networks, tend to be computationally expensive and require large datasets to train. Thus, a team from the University of Waterloo proposed an alternative method for classifying text using a GZIP compressor and the K-Nearest Neighbor clustering algorithm.\nThe team demonstrated the viability of the method on a variety of datasets, testing the accuracy compared to non-pretrained datasets. As it turns out, their model is able to compete with the larger models and was even capable of outperforming BERT in some cases.\nIn the following blog post, I plan to try and recreate the said model on a different dataset to see how accurate the model is and to learn to apply it myself. Specifically, I want to apply the model to the most classic example of outlier detection - spam or ham!"
  },
  {
    "objectID": "posts/KNN-text-classification/index.html#implementing-the-algorithm",
    "href": "posts/KNN-text-classification/index.html#implementing-the-algorithm",
    "title": "KNN for Text Classification",
    "section": "",
    "text": "The algorithm is simple and consists of KNN as the classification method and GZIP as the distance metric, but what does it mean?\n\n\nKNN (also known as KNN) is a simple and intuitive machine learning algorithm used for classification and regression tasks. It works by finding the K closest data points (neighbors) to a given input data point in a dataset and then classifying or predicting the target variable based on the majority class or average value of those K neighbors.\n\n\n\nGZIP is a file compression utility that is typically used for compressing files to be transfered over network or for storage. You most likely have used GZIP before in order to do that. An interesting detail of the GZIP compression is that repetative texts will be compressed into smaller sizes than texts that are not repetative. This is due to the fact that the algorithm uses Huffman coding, which replaces the most repeated sequences with shorter sequences, and LZ77, which stores references to repeated sequnces instead of using the sequences themselves (think of it like having a variable assigned to sequence and then using that variable later instead of the sequence).\n\n\n\nKNN can be used in order to classifiy different sentences. However, to classify something when using KNN we need a way to also measure the distance between two datapoints. This is where GZIP comes in. Since GZIP compresses repetative sequences into smaller sizes - similar sequences that are concatonated together will compress to smaller sizes as well. This allows us to formulate a distance metric based on how well a sequence is compressed.\nConsider an example: We have sequences x1, x2, and x3. Assume that x1 and x2 are similar sequences while x3 is less similar. Let C(x, y) be our GZIP compressor. Then it follows that: \\[\nsize(C(x1 + x2)) &lt; size(C(x1 + x3))\n\\]\nThis is the intuition behind the method. We will first begin by implementing the metric.\n\n\n\nThe most basic implementation of the metric would look like this, but the keen eyed may notice that it could be optimized.\n\nimport gzip as gz\n\n\ndef gzip_metric(x1, x2):\n    Cx1 = len(gz.compress(x1.encode()))\n    Cx2 = len(gz.compress(x2.encode()))\n\n    x1x2 = \" \".join([x1, x2])\n    Cx1x2 = len(gz.compress(x1x2.encode()))\n    ncd = (Cx1x2 - min(Cx1, Cx2)) / max(Cx1, Cx2)\n    return ncd\n\n\n\n\nNormally, we would use an already existing library such as Scikit-Learn and simply import the KNN algorithm from there (neighbors.KNeighborsClassifier). However, when trying to do that, I run into an interesting problem. Turns out that Scikit’s KNeighborsClassifier does not operate with strings and expects the user to encode them beforehand. We do not want to encode these strings since we want our GZIP metric to interact with the strings directly. Luckily, KNN is simple to implement by hand and is also implemented in the paper.\n\nimport pandas as pd\nimport numpy as np\n\n\ndef knn_classify(known_df: pd.DataFrame, unknown_sr: pd.Series, k: int = 3):\n    knowns: np.ndarray = known_df.to_numpy(copy=True)\n    unknowns: np.ndarray = unknown_sr.to_numpy(copy=True)\n    predictions = []\n    for x1 in unknowns:\n        distances_from_x1: np.array = np.array(\n            [gzip_metric(x1, x2) for x2, _ in knowns]\n        )\n        sorted_idx = np.argsort(distances_from_x1)\n        top_k_classes = knowns[sorted_idx[:k], 1]\n        values, counts = np.unique(top_k_classes, return_counts=True)\n        predicted_class = values[np.argmax(counts)]\n        predictions.append(predicted_class)\n\n    return predictions\n\n\n\n\nAs you have seen in the KNN section, x2 is computed multiple times for every x1. Therefore, when we use the gzip metric, we unecessarily compress x1 multiple times, which is time-consuming. So, instead, we could precompute that value inside our knn_function and pass the precomputed value into our metric.\n\ndef improved_gzip_metric(Cx1, x1, x2):\n    Cx2 = len(gz.compress(x2.encode()))\n    x1x2 = \" \".join([x1, x2])\n    Cx1x2 = len(gz.compress(x1x2.encode()))\n    ncd = (Cx1x2 - min(Cx1, Cx2)) / max(Cx1, Cx2)\n    return ncd\n\n\ndef improved_knn_classify(known_df: pd.DataFrame, unknown_sr: pd.Series, k: int = 3):\n    knowns: np.ndarray = known_df.to_numpy(copy=True)\n    unknowns: np.ndarray = unknown_sr.to_numpy(copy=True)\n    predictions = []\n    for x1 in unknowns:\n        Cx1 = len(gz.compress(x1.encode()))\n        distances_from_x1: np.array = np.array(\n            [improved_gzip_metric(Cx1, x1, x2) for x2, _ in knowns]\n        )\n        sorted_idx = np.argsort(distances_from_x1)\n        top_k_classes = knowns[sorted_idx[:k], 1]\n        values, counts = np.unique(top_k_classes, return_counts=True)\n        predicted_class = values[np.argmax(counts)]\n        predictions.append(predicted_class)\n\n    return predictions"
  },
  {
    "objectID": "posts/KNN-text-classification/index.html#spam-or-ham",
    "href": "posts/KNN-text-classification/index.html#spam-or-ham",
    "title": "KNN for Text Classification",
    "section": "",
    "text": "The dataset I would like to look at was taken from kaggle and includes a collection of messages labeled as spam or not spam (also called ham).\nLet’s take a quick look at the data using the pandas library.\n\n\n\n# The dataset comes with a single file named train.csv. Interestingly, there is no test.csv.\ndf_spam = pd.read_csv(\"Spam_dataset/train.csv\") \ndf_spam[\"sms\"] = df_spam[\"sms\"].values.astype(\"str\")\ndf_spam.head()\n\n\n\n\n\n\n\n\nsms\nlabel\n\n\n\n\n0\nGo until jurong point, crazy.. Available only ...\n0\n\n\n1\nOk lar... Joking wif u oni...\\n\n0\n\n\n2\nFree entry in 2 a wkly comp to win FA Cup fina...\n1\n\n\n3\nU dun say so early hor... U c already then say...\n0\n\n\n4\nNah I don't think he goes to usf, he lives aro...\n0\n\n\n\n\n\n\n\nYou can see that the dataset is split into two columns. One containing the message, the second containing the label. Conviniently for us, the labels are already hot-encoded, so we do not need to go through the encoding step. Something to note, if the label is ‘0’ then the message is considered ham, if it is labled as ‘1’ the message is spam.\nLet’s quickly look at the distribution of the data.\n\ngrouped_counts = df_spam.groupby(\"label\").size()\ngrouped_counts.plot.bar(x=\"label\")\ngrouped_counts\n\nlabel\n0    4827\n1     747\ndtype: int64\n\n\n\n\n\nWe can see that the overwhelming number of labels belong to ‘ham’. When we split this data into a train and test dataset, we will need to pay special attention that both of the labels are present in both the testing and training datasets. To do that we can use sklearn’s train_test_split.\n\nfrom sklearn.model_selection import train_test_split\nX = df_spam[\"sms\"]\ny = df_spam[\"label\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\ntrain_df = pd.concat([X_train, y_train], axis=1)\ntest_df = pd.concat([X_test, y_test], axis=1)\n\nNow that the data has been split, we can use the algorithm we built to try and classify the text messages.\n\n\n\nFirst let’s try the unoptimized algorithm.\n\ny_bar_slow = knn_classify(train_df, X_test)\n\nThis cell took 3 minutes and 50 seconds to execute on my system. Now, we will run the improved implementation.\n\ny_bar = improved_knn_classify(train_df, X_test)\n\n\ny_bar == y_bar_slow\n\nTrue\n\n\nWe can see that the results are identical, yet the improved version runs faster. So, from now on, we will simply use the improved version. Let’s see how the model has performed.\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nconf_mat = confusion_matrix(y_test, y_bar)\nConfusionMatrixDisplay(conf_mat).plot()\n\nprint(\"f1:\", f1_score(y_test, y_bar))\nprint(\"accuracy:\", accuracy_score(y_test, y_bar))\n\nf1: 0.9547038327526132\naccuracy: 0.9883408071748879\n\n\n\n\n\nThese results are pretty impressive. However, this could be a result of a particularly lucky split. We will now try to perform cross validation while also experimenting with different sizes of K.\n\n\n\nFirst, need to consider the possible size of K. A good rule of thumb for KNN is to select a K that is equal to \\(\\sqrt(n)\\), where \\(n\\) is the number of datapoints in the training set. We have 4459 data points in our training set. That means that our K should be approximately \\(66\\). We could try multiple K values as well. We can try a list of \\(64, 65, 66, 67, 68\\) since these values are in the vicinity of our rule of thumb and also \\(3\\), since we have recieved good results from it.\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom multiprocess import Pool\n\nf1_scores = []\naccuracy_scores = []\nKs = [3, 64, 65, 66, 67, 68]\nskf = StratifiedKFold(n_splits=5)\nwith Pool() as pool:\n    for train_idx, test_idx in skf.split(X, y):\n        train_df = df_spam.iloc[train_idx]\n        test_df = df_spam.iloc[test_idx]\n        args = [(train_df, test_df[\"sms\"], K) for K in Ks]\n        y_bars = pool.starmap(improved_knn_classify, args)\n        accuracies = []\n        f1s = []\n        for y_bar in y_bars:\n            accuracies.append(accuracy_score(test_df[\"label\"], y_bar))\n            f1s.append(f1_score(test_df[\"label\"], y_bar))\n        \n        accuracy_scores.append(accuracies)\n        f1_scores.append(f1s)\n\nYou may notice that the above code uses a new 3rd party library names “multiprocess” (not to be confused with Python’s “multiprocessing” library). It is a fork of the “multiprocessing” built-in Python library that performs better in the Jupyter environment. We are using this library in order to be able to process multiple K values in parallel. Otherwise, this process would have taken significantly longer.\nNow, lets save our metric results as dataframes for further processing.\n\nf1_df = pd.DataFrame(f1_scores, columns=Ks)\naccuracy_df = pd.DataFrame(accuracy_scores, columns=Ks)\nprint(f1_df)\nprint(accuracy_df)\n\n         3         64        65        66        67        68\n0  0.944444  0.892989  0.897059  0.897059  0.897059  0.897059\n1  0.951389  0.897059  0.901099  0.897059  0.897059  0.897059\n2  0.936170  0.888889  0.888889  0.884758  0.884758  0.880597\n3  0.932862  0.872180  0.872180  0.867925  0.872180  0.867925\n4  0.954386  0.900369  0.900369  0.900369  0.900369  0.900369\n         3         64        65        66        67        68\n0  0.985650  0.973991  0.974888  0.974888  0.974888  0.974888\n1  0.987444  0.974888  0.975785  0.974888  0.974888  0.974888\n2  0.983857  0.973094  0.973094  0.972197  0.972197  0.971300\n3  0.982960  0.969507  0.969507  0.968610  0.969507  0.968610\n4  0.988330  0.975763  0.975763  0.975763  0.975763  0.975763\n\n\nWe can create a bar chart in order to understand the results a bit better. We will first look at the \\(F1\\) score.\n\nax = f1_df.plot.bar(rot=0, title=\"F1 Scores of Different Splits and Different K Values\")\nax.legend(loc='center left',bbox_to_anchor=(1.0, 0.5))\nax.set_xlabel(\"Split Index\")\nax.set_ylabel(\"F1 Score\")\n\nText(0, 0.5, 'F1 Score')\n\n\n\n\n\n\nax = accuracy_df.plot.bar(rot=0, title=\"Accuracy Scores of Different Splits and Different K Values\")\nax.legend(loc='center left',bbox_to_anchor=(1.0, 0.5))\nax.set_xlabel(\"Split Index\")\nax.set_ylabel(\"Accuracy Score\")\n\nText(0, 0.5, 'Accuracy Score')\n\n\n\n\n\nFrom both of these plots we can see that there is no significant difference between different splits of the dataset. Additionally, there does not appear to be a significant difference in performance of the K values that are closest to our rule of thumb. However, turns out that a K value of \\(3\\) is significantly more profitable than the other K values. We can take a look at the average score for \\(K=3\\) for each metric below:\n\nprint(\"F1 Average:\", f1_df[3].mean())\nprint(\"Accuracy Average:\", accuracy_df[3].mean())\n\nF1 Average: 0.9438503403648584\nAccuracy Average: 0.9856481310028903"
  },
  {
    "objectID": "posts/KNN-text-classification/index.html#references",
    "href": "posts/KNN-text-classification/index.html#references",
    "title": "KNN for Text Classification",
    "section": "",
    "text": "[1] A. Vaswani et al., “Attention is all you need,” arXiv.org, https://arxiv.org/abs/1706.03762. [2] Z. Jiang et al., “‘low-resource’ text classification: A parameter-free classification method with compressors,” ACL Anthology, https://aclanthology.org/2023.findings-acl.426/."
  },
  {
    "objectID": "posts/Neural-Networks/index.html",
    "href": "posts/Neural-Networks/index.html",
    "title": "Neural Networks and Images",
    "section": "",
    "text": "Gradient-Based Learning Applied to Document Recognition by LeCun et al."
  },
  {
    "objectID": "posts/Neural-Networks/index.html#the-data",
    "href": "posts/Neural-Networks/index.html#the-data",
    "title": "Neural Networks and Images",
    "section": "The Data!",
    "text": "The Data!\n\nSummary of the Data\nFor training and evaluation of our neural network, we will be using the popular MNIST dataset. The MNIST dataset comprises various \\(28 \\times 28\\) images of handwritten numbers from 0 through 9 in a monochrome format with values from 0 to 255. This copy of the MNIST dataset was taken from kaggle. The data is stored in a particularly interesting format, so we will be using the provided example to unpack the dataset.\n\nimport numpy as np\nimport struct\nfrom array import array\nfrom os.path import join\n\n\n# MNIST Data Loader Class\nclass MnistDataloader(object):\n    def __init__(self, training_images_filepath, training_labels_filepath,\n                 test_images_filepath, test_labels_filepath):\n        self.training_images_filepath = training_images_filepath\n        self.training_labels_filepath = training_labels_filepath\n        self.test_images_filepath = test_images_filepath\n        self.test_labels_filepath = test_labels_filepath\n\n    @staticmethod\n    def read_images_labels(images_filepath, labels_filepath):\n        labels = []\n        with open(labels_filepath, 'rb') as file:\n            magic, size = struct.unpack(\"&gt;II\", file.read(8))\n            if magic != 2049:\n                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n            labels = array(\"B\", file.read())\n\n        with open(images_filepath, 'rb') as file:\n            magic, size, rows, cols = struct.unpack(\"&gt;IIII\", file.read(16))\n            if magic != 2051:\n                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n            image_data = array(\"B\", file.read())\n        images = []\n        for i in range(size):\n            images.append([0] * rows * cols)\n        for i in range(size):\n            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n            img = img.reshape(28, 28)\n            images[i][:] = img\n\n        return images, labels\n\n    def load_data(self):\n        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n        return (x_train, y_train), (x_test, y_test)\n\n\n%matplotlib inline\nimport random\nimport matplotlib.pyplot as plt\n\n# Set file paths based on added MNIST Datasets\ninput_path = '/Users/danielsabanov/Documents/VT_Notes/9th_Semester/ML1Blog/Post4/MNIST'  # YOU WILL NEED TO MODIFY THIS\ntraining_images_filepath = join(input_path, 'train-images-idx3-ubyte/train-images-idx3-ubyte')\ntraining_labels_filepath = join(input_path, 'train-labels-idx1-ubyte/train-labels-idx1-ubyte')\ntest_images_filepath = join(input_path, 't10k-images-idx3-ubyte/t10k-images-idx3-ubyte')\ntest_labels_filepath = join(input_path, 't10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte')\n\n\n# Helper function to show a list of images with their relating titles\ndef show_images(images, title_texts):\n    cols = 5\n    rows = int(len(images) / cols) + 1\n    plt.figure(figsize=(30, 20))\n    index = 1\n    for x in zip(images, title_texts):\n        image = x[0]\n        title_text = x[1]\n        plt.subplot(rows, cols, index)\n        plt.imshow(image, cmap=plt.cm.gray)\n        if (title_text != ''):\n            plt.title(title_text, fontsize=15)\n        index += 1\n\n\n# Load MINST dataset\nmnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath,\n                                   test_labels_filepath)\n(x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()\n\n# Show some random training and test images \nimages_2_show = []\ntitles_2_show = []\nfor i in range(0, 10):\n    r = random.randint(1, 60000)\n    images_2_show.append(x_train[r])\n    titles_2_show.append('training image [' + str(r) + '] = ' + str(y_train[r]))\n\nfor i in range(0, 5):\n    r = random.randint(1, 10000)\n    images_2_show.append(x_test[r])\n    titles_2_show.append('test image [' + str(r) + '] = ' + str(y_test[r]))\n\nshow_images(images_2_show, titles_2_show)\n\n\n\n\nIf you want to work with the provided dataset, you will need to modify the input path to the location where you store your data. Alternatively, various libraries, such as Scikit-Learn and PyTorch provide their own versions of the data. In my case I am using the version from Kaggle because it will also provide practice in converting the dataset to a format that works with PyTorch. Now lets examine the distribution of the digits within both the training and testing sets.\n\ndigit_counts_train = {digit: y_train.count(digit) for digit in range(10)}\ndigit_counts_test = {digit: y_test.count(digit) for digit in range(10)}\n\nfig_train, ax_train = plt.subplots()\nax_train.bar(digit_counts_train.keys(), digit_counts_train.values(), color='skyblue')\nax_train.set_xlabel('Digit')\nax_train.set_ylabel('Number of Data Points')\nax_train.set_title('Number of Data Points for Each Digit in MNIST training partition')\n\nplt.show()\n\nfig_test, ax_test = plt.subplots()\nax_test.bar(digit_counts_test.keys(), digit_counts_test.values(), color='skyblue')\nax_test.set_xlabel('Digit')\nax_test.set_ylabel('Number of Data Points')\nax_test.set_title('Number of Data Points for Each Digit in MNIST training partition')\n\nplt.show()\n\n\n\n\n\n\n\nWe can see that the digit labels in both the training and testing partitions are roughly evenly distributed. This means that there should be minimal bias in the results, and there is no need to try to balance the data. Now, let’s convert the data to a format PyTorch is able to work with.\n\n\nConverting the Data\nPyTorch has a very nice API for working with datasets. It requires the user to define a subclass of torch.utils.data.Dataset, this subclass must contain implementations for the following functions: __init__, __len__, and __getitem__. Let us build one such dataset class.\n\nimport torch\n\n\nclass MNISTDataset(torch.utils.data.Dataset):\n    def __init__(self, x, y):\n        self.x_tensor = torch.tensor(x, dtype=torch.float32)\n        self.y_tensor = torch.tensor(y, dtype=torch.long)\n\n    def __len__(self):\n        assert len(self.x_tensor) == len(self.y_tensor)\n        return len(self.y_tensor)\n\n    def __getitem__(self, item):\n        image = torch.unsqueeze(self.x_tensor[item, :, :], 0)\n        label = self.y_tensor[item]\n        return image, label\n\nLet’s address a few nuances in our dataset definition. First of all, observe that we are creating two types of tensors. The first, called x_tensor contains a tensor that stores number images in the float32 format. This is because, as you will see, the first layer of our neural network is a 2-dimensional convolution layer. According to PyTorch documentation, it expects the inputs to be in float32 format. The second tensor, called y_tensorthat stores the labels uses torch.long format. This is done for ease of interaction with our loss function. I will be explaining more about it once we reach it.\nNow, I want you to observe that we are unsqueezing the image when we return it from __getitem__. This is done also due to the restrictions posed by the convolution layer. According to its documentation, the dimensions of the input must be shaped as (batch size, number of channels, height, width). The batch size will be handled by our data loader, which we will shortly implement. However, the number of channels is for us to handle. The number of channels represents the number of values that are used to define the color of a single pixel. For example, an image in RGB format would have three channels. Since MNIST is monochrome, we need the number of channels to be 1. We can do so using the unsqueeze function. Now let’s handle batching…\n\ntrain_data = MNISTDataset(x_train, y_train)\ntest_data = MNISTDataset(x_test, y_test)\n\nBATCH_SIZE = 4\n\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE)\n\n/var/folders/1n/_w4rlkb92pq1mwmj66_byr080000gn/T/ipykernel_18742/2513850566.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:264.)\n  self.x_tensor = torch.tensor(x, dtype=torch.float32)\n\n\nThe torch.utils.data.DataLoader class provides batching functionality. Batching is used to feed multiple inputs into a neural network at once. This is done because it is actually more efficient to do so. Since GPU is a parallel architecture, we gain a training speedup by feeding multiple inputs at once. Let’s verify that our input into the network is of appropriate size.\n\nnext(iter(train_loader))[0].size()\n\ntorch.Size([4, 1, 28, 28])"
  },
  {
    "objectID": "posts/Neural-Networks/index.html#neural-networks-background",
    "href": "posts/Neural-Networks/index.html#neural-networks-background",
    "title": "Neural Networks and Images",
    "section": "Neural Networks Background",
    "text": "Neural Networks Background\nThe architecture is typically used for image recognition is a Convolutional Neural Network (CNN). They are like regular neural networks, but convoluted! More seriously, a CNN typically consists of several convolution layers (do not worry, I will explain what it is soon) followed by linear layers. A basic way to think about CNNs is as if they were a pipeline of matrix and vector operations.\n\nThe Convolution Layer\nA convolution operation is actually trivial in nature. It is the process of passing a larger matrix through a smaller filter matrix. Still sounds like a word salad? It is easier when one visualizes it. Say we have a matrix of size \\(3 \\times 3\\) and a filter of size \\(2 \\times 2\\). \\[\nA =\n\\left[ \\begin{matrix}\na_1 & a_2 & a_3\\\\\na_4 & a_5 & a_6\\\\\na_7 & a_8 & a_9\\\\\n\\end{matrix} \\right]\n\\] \\[\nfilter =\n\\left[ \\begin{matrix}\nf_1 & f_2 \\\\\nf_3 & f_4 \\\\\n\\end{matrix} \\right]\n\\] Let our result be \\[\nR = \\left[ \\begin{matrix}\nr_1 & r_2 \\\\\nr_3 & r_4 \\\\\n\\end{matrix}\\right]\n\\]\nThen, our \\(r_1\\) would be the sum of the products of the numbers in the top left corner of \\(A\\) (\\(a_1\\), \\(a_2\\), \\(a_4\\), \\(a_5\\)) that can be overlapped by the filter. \\[\nr_1 = a_1 \\times f_1 + a2 \\times f_2 + a_4 \\times f_3 + a_5 \\times f_4\n\\] We will do the same for the top right corner of \\(A\\) \\[\nr_2 = a_2 \\times f_1 + a3 \\times f_2 + a_5 \\times f_3 + a_6 \\times f_4\n\\]\nThis pattern will continue until all of \\(A\\) passes through the filter. ### The Linear Layer The linear layer is even simpler. It is called linear because it represents a linear equation. That means that given an input \\(X\\), the output will be \\[\n\\vec{f(x)} = W \\vec{x} + \\vec{b}\n\\] where \\(W\\) represents the weights and \\(b\\) represents bias. Both the weight and the bias will be the values that the network will be learning."
  },
  {
    "objectID": "posts/Neural-Networks/index.html#building-the-neural-network",
    "href": "posts/Neural-Networks/index.html#building-the-neural-network",
    "title": "Neural Networks and Images",
    "section": "Building the Neural Network!",
    "text": "Building the Neural Network!\nNow that we have a general understanding of how CNNs function, we should try and build one. Pytorch represents models as classes containing an initialization method and a “forward” method. Initialization is pretty self-explanatory, it is the initialization function that initializes all the layers. The “forward” may sound a bit cryptic, but it is also very trivial. It is a method that describes how the data is propagated forward through the network. Finally, every Neural Network also has to have a backwards pass to adjust the weights of the model. This is done by a method automatically generated by PyTorch—the “backward” method.\n\nfrom torch.nn import functional as F\nfrom torch import nn\n\n\nclass NumberClassifier(nn.Module):\n    def __init__(self):\n        super(NumberClassifier, self).__init__()\n\n        self.conv_layer1 = nn.Conv2d(1, 4, (3, 3))\n        self.conv_layer2 = nn.Conv2d(4, 8, (2, 2))\n        self.linear_layer1 = nn.Linear(288, 144)\n        self.linear_layer2 = nn.Linear(144, 72)\n        self.linear_layer3 = nn.Linear(72, 10)\n\n    def forward(self, x):\n        # First Layer\n        x = self.conv_layer1(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, (2, 2))\n\n        # Second Layer\n        x = self.conv_layer2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, (2, 2))\n\n        # Third Layer\n        x = x.view(-1, self.num_flat_features(x))\n        x = self.linear_layer1(x)\n        x = F.relu(x)\n\n        # Fourth Layer\n        x = self.linear_layer2(x)\n        x = F.relu(x)\n\n        # Fifth Layer\n        x = self.linear_layer3(x)\n\n        return x\n\n    def num_flat_features(self, x):\n        num_features = 1\n        for s in x.size()[1:]:\n            num_features *= s\n        return num_features\n\nNow that we have assembled our model, let’s take a deeper look into the components, so I could explain what are the values that are being fed into each layer.\n\nFirst Layer\nThe first convolution layer is initialized with the values 1, 4, and (3, 3). The first value, 1, represents the number of channels the number of channels that are being fed into the layer. Since our image is monochrome, we only need a single channel. The second value, 4, represents the number of output channels that we want from the layer. There is no specific reason for selecting this value other than the fact that 4 is greater than 1. It is generally considered better for convolution layers to output a larger number of channels than what they receive. (3, 3) is the size of the kernel, or what we previously referred to as a filter. One again, I picked it somewhat arbitrarily while making it square and of odd height and width to simplify the computations later on.\nYou can see that the output of the convolution is passed into something called a ReLU. ReLU is an acronym for Rectified Linear Unit, a function defined as\n\\[\nf(x) = max(0, x)\n\\]\nWe are using a ReLU function to introduce non-linearity to our neural network. There are many other functions that can be used instead of ReLU. However, ReLU is amongst the simplest activation functions and thus reduces the computation complexity-meaning it increases both inference and training speed. Non-linearity is important since if all the layers would be linear, the entire neural network would become a very convoluted representation of a linear function.\nThe output of ReLU is then passed into a two-dimensional max pool function. What it does is split its input matrix into multiple \\(2 \\times 2\\) squares and select the maximum value out of each square. This allows the neural network to focus on the most prominent features in the image.\n\n\nSecond Layer\nThe second layer is very similar to the first layer. We now specify that the number of input channels is 4 since the output of the previous layer had four channels.\n\n\nThird Layer\nThe third layer is simpler than the previous two. In this layer we accept a specific number of features, in this case 288, and pass it through our linear layer to the ReLU function. However, why do we accept 288 features specifically, and what are we doing before that? Were we not working with matrices previously? My answer to this question is-exactly! The first thing we do before we pass output from the previous layer into our linear layer is to flatten the tensors into vectors.\nNow let’s calculate the dimensions of the tensor that gets out of the second layer. When a matrix goes through a 2d convolution, the output will be of size\n\\[\nD_{out} = D_{in} - (kernel\\_dim - 1)\n\\]\nWhere D represents either the width or the height of the matrix (you are going to run this calculation once for each dimension). So, our new matrix is of size \\(26 \\times 26\\).\nWe pass our matrix through a two-dimensional max-pool of size \\(2 \\times 2\\). This reduces the size of each dimension by half. Our matrix is now of size \\(13 \\times 13\\).\nAfter passing through our next layer, the size becomes \\(12 \\times 12\\) and then \\(6 \\times 6\\). So each matrix has thirty-six values. We also have eight channels. So the total number of values to come out of the second layer is 288. The exact number we are feeding into our third layer.\n\n\nFourth Layer\nThis layer is even simpler that the previous layer since we do not have to flatten anything, we just pass the output from one side to the other.\n\n\nFifth Layer\nSince this is the final layer, there is no need for the ReLU function as there is no need to avoid linearity. Since we are trying to classify our input into 10 classes, the output from this layer should also be split into 10 values. The index of the highest value will represent the class that the network is trying to predict."
  },
  {
    "objectID": "posts/Neural-Networks/index.html#teaching-the-network",
    "href": "posts/Neural-Networks/index.html#teaching-the-network",
    "title": "Neural Networks and Images",
    "section": "Teaching the Network!",
    "text": "Teaching the Network!\nNow that we have defined our model, it is time to handle its training, as it will not be capable of classifying the digits accurately without training. First, we will create an instance of our network.\n\ncnn = NumberClassifier()\n\nNow, I have mentioned GPUs several times in this post; how about we use one to train the network? To do so, we need a reference to our GPU, since in my case I am training the network on an M1 laptop, I will need to use the MPS backend. If you are training on an Nvidia GPU you will need to change the way you are getting the reference to the device. I encourage you to look through the PyTorch CUDA documentation to understand how to do that.\nAfter we have a reference to the device, we will want to send our network to the said device. Do note that from now on you will also need to send all the tensors to the device as well!\n\ndevice = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\ncnn.to(device)\n\nNumberClassifier(\n  (conv_layer1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n  (conv_layer2): Conv2d(4, 8, kernel_size=(2, 2), stride=(1, 1))\n  (linear_layer1): Linear(in_features=288, out_features=144, bias=True)\n  (linear_layer2): Linear(in_features=144, out_features=72, bias=True)\n  (linear_layer3): Linear(in_features=72, out_features=10, bias=True)\n)\n\n\nNext, we will define our loss metric and optimizer. The loss metric is used to help our model understand the measure by which it was wrong. Since we are using multiclass classification, we will want to use CrossEntropyLoss. Remember when we were discussing the reasoning for using long format for the true labels in our model and I said that it has to do with the way our loss function will work? Well, this is the explanation. Cross-entropy loss works with hot-encoded vectors, meaning our predictions vector and labels vector must be hot encoded. We could have hot encoded our labels, but that would have taken a little extra work. Luckily, PyTorch developers recognized that, so instead, CrossEntropyLoss is capable of hot encoding the labels by itself, as long as the labels are provided in the long format. This simplifies the work for us since we do not need to worry about encoding the labels anymore.\nWe should discuss the optimizer a little. The optimizer is used to adjust the weights of the model based on the loss function. It is what performs the gradient descent in the model. PyTorch offers many different optimizers. I have chosen the AdamW optimizer since it is the optimizer I see most often used. However, it may not be the perfect optimizer for this task. I encourage the reader to experiment with their own optimizers to perhaps find one that works better.\n\nloss_metric = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(cnn.parameters())\n\nNow that we have defined everything. It is time to train our network. To validate the performance of the model, we will be using the f1-score and accuracy metrics. We will run the model against the testing dataset in no grad mode (which means gradients will not be computed and the model will not remember the data) every 1000 steps. We will also do one validation run after the model finishes training.\n\nfrom sklearn.metrics import f1_score, accuracy_score\n\ndef evaluate_performance(loader):\n    predictions_list = []\n    labels_list = []\n    for batch in loader:\n        image, label = batch\n        with torch.no_grad():\n            image = image.to(device)\n            outputs = cnn(image)\n            predictions = torch.argmax(outputs, dim=1)\n            predictions_list += predictions.tolist()\n            labels_list += label.tolist()\n            \n    f1 = f1_score(labels_list, predictions_list, average=\"micro\")\n    acc = accuracy_score(labels_list, predictions_list)\n    return f1, acc\n\n\nNUM_EPOCHS = 2\nlosses = []\naccuracies = []\nf1_scores=  []\nfor epoch in range(NUM_EPOCHS):\n    for i, data in enumerate(train_loader):\n        image, label = data\n        optimizer.zero_grad()\n        image = image.to(device)\n        label = label.to(device)\n        outputs = cnn(image)\n        loss = loss_metric(outputs, label)\n        losses.append(loss.item())\n\n        loss.backward()\n        optimizer.step()\n\n        if i % 1000 == 0:\n            print(f\"The current step is {i} in the {epoch} epoch. \"\n                  f\"The loss for the last batch was {loss.item()}. \"\n                  f\"The total loss is {sum(losses)}\")\n            f1, acc = evaluate_performance(test_loader)\n            accuracies.append(acc)\n            f1_scores.append(f1)\n\nThe current step is 0 in the 0 epoch. The loss for the last batch was 5.68056583404541. The total loss is 5.68056583404541\nThe current step is 1000 in the 0 epoch. The loss for the last batch was 0.0033018924295902252. The total loss is 552.0863177699503\nThe current step is 2000 in the 0 epoch. The loss for the last batch was 0.0059255617670714855. The total loss is 814.0782704689045\nThe current step is 3000 in the 0 epoch. The loss for the last batch was 0.06884118169546127. The total loss is 1044.9189079682546\nThe current step is 4000 in the 0 epoch. The loss for the last batch was 0.0769830048084259. The total loss is 1257.0625633281788\nThe current step is 5000 in the 0 epoch. The loss for the last batch was 0.1409284770488739. The total loss is 1413.7068831235347\nThe current step is 6000 in the 0 epoch. The loss for the last batch was 0.021139046177268028. The total loss is 1559.5092783509208\nThe current step is 7000 in the 0 epoch. The loss for the last batch was 0.000831345038022846. The total loss is 1715.2658046736137\nThe current step is 8000 in the 0 epoch. The loss for the last batch was 0.004426009953022003. The total loss is 1864.890646117095\nThe current step is 9000 in the 0 epoch. The loss for the last batch was 0.007887255400419235. The total loss is 2001.0973939303726\nThe current step is 10000 in the 0 epoch. The loss for the last batch was 0.0310039184987545. The total loss is 2142.1981314078153\nThe current step is 11000 in the 0 epoch. The loss for the last batch was 0.004002015106379986. The total loss is 2276.7468204431366\nThe current step is 12000 in the 0 epoch. The loss for the last batch was 0.004485937301069498. The total loss is 2409.59029880478\nThe current step is 13000 in the 0 epoch. The loss for the last batch was 0.0007990289013832808. The total loss is 2538.933369719378\nThe current step is 14000 in the 0 epoch. The loss for the last batch was 0.007755488622933626. The total loss is 2654.6810632726256\nThe current step is 0 in the 1 epoch. The loss for the last batch was 0.0021050558425486088. The total loss is 2757.288812872535\nThe current step is 1000 in the 1 epoch. The loss for the last batch was 0.0006367879104800522. The total loss is 2869.6518699631747\nThe current step is 2000 in the 1 epoch. The loss for the last batch was 0.005833970382809639. The total loss is 2991.5952052153184\nThe current step is 3000 in the 1 epoch. The loss for the last batch was 0.12352235615253448. The total loss is 3102.5677232857415\nThe current step is 4000 in the 1 epoch. The loss for the last batch was 0.00011294198338873684. The total loss is 3202.5640222237203\nThe current step is 5000 in the 1 epoch. The loss for the last batch was 0.005728553980588913. The total loss is 3303.068875451456\nThe current step is 6000 in the 1 epoch. The loss for the last batch was 0.00019064114894717932. The total loss is 3406.229488440803\nThe current step is 7000 in the 1 epoch. The loss for the last batch was 0.00022386967611964792. The total loss is 3511.2122435310966\nThe current step is 8000 in the 1 epoch. The loss for the last batch was 0.00018528304644860327. The total loss is 3615.7973996183114\nThe current step is 9000 in the 1 epoch. The loss for the last batch was 0.0028845490887761116. The total loss is 3711.1961789740894\nThe current step is 10000 in the 1 epoch. The loss for the last batch was 0.0010837310692295432. The total loss is 3808.655136974545\nThe current step is 11000 in the 1 epoch. The loss for the last batch was 0.00020842113008257002. The total loss is 3909.6032301442674\nThe current step is 12000 in the 1 epoch. The loss for the last batch was 0.003431916469708085. The total loss is 4012.272500330948\nThe current step is 13000 in the 1 epoch. The loss for the last batch was 0.0013995182234793901. The total loss is 4107.566439142381\nThe current step is 14000 in the 1 epoch. The loss for the last batch was 0.031621456146240234. The total loss is 4195.64182331661\n\n\nLet’s graph the performance of the model as it was training.\n\nsteps = [i * 1000 for i in range(len(f1_scores))]\nfig, ax = plt.subplots()\nax.plot(steps, f1_scores, label=\"F1 Score\")\nax.plot(steps, accuracies, label=\"Accuracy\")\nplt.xlabel(\"Step\")\nplt.ylabel(\"Score Value\")\nplt.title(\"Training Time Performance\")\nplt.legend()\nplt.show()\n\nrunning_loss = [sum(losses[:i+1])/(i+1) for i in range(len(losses))]\nfig, ax = plt.subplots()\nax.plot(list(range(len(running_loss))), running_loss)\nplt.xlabel(\"Step\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Time Accumulated Loss\")\nplt.show()\n\n\n\n\n\n\n\nWe can see that our model trains very quickly. As a matter of fact, we did not even need the two epochs of training as both accuracy and F1-score reach a high value after less than 5000 steps.\nLet us take a look at the final performance values of the model.\n\nf1, acc = evaluate_performance(test_loader)\nprint(f\"The accuracy was {acc} and the f1 score was {f1}\")\n\nThe accuracy was 0.956 and the f1 score was 0.956"
  },
  {
    "objectID": "posts/Neural-Networks/index.html#afterward",
    "href": "posts/Neural-Networks/index.html#afterward",
    "title": "Neural Networks and Images",
    "section": "Afterward",
    "text": "Afterward\nNeural Networks are a very complex subject and this blog post has been but a tiny glimpse into the world of neural networks. To learn more about neural networks, I encourage the reader to look deeper into the PyTorch documentation, or TensorFlow documentation. Additionally, I reccomend looking at Neural Networks from Scratch in Python. If you would like to learn more about the background for the architecture used, I would recomend reading Gradient Based Learning Applied to Document Recognition."
  },
  {
    "objectID": "posts/Penguin-clustering/index.html",
    "href": "posts/Penguin-clustering/index.html",
    "title": "Clustered Penguins",
    "section": "",
    "text": "Courtesy: S Richter et al/Journal of Physics D: Applied Physics\n\n\nClustering is a machine learning technique used to group similar data togehter into subsets based on inherent similarities and patterns. It is a techineque that is wildly used in explaratory data analysis in order to reveal insights about data. It has a significant overlap with classification as many algorithms that are used in clustering are also used for classification. Such algorithms include K-Means-Clustering, K-Nearest-Neighbor, and many others.\nIn this blog, we will be exploring clustering as applied to the penguins dataset."
  },
  {
    "objectID": "posts/Penguin-clustering/index.html#loading-and-preprocessing",
    "href": "posts/Penguin-clustering/index.html#loading-and-preprocessing",
    "title": "Clustered Penguins",
    "section": "Loading and Preprocessing",
    "text": "Loading and Preprocessing\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\npenguins_data = pd.read_csv(\"penguins_size.csv\")\npenguins_data.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nculmen_length_mm\nculmen_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMALE\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n\n\n\n\n\n\n\nRight of the bat, we can see that that there are rows in the dataset that contain NaNs. One way we could deal with it is by imputing the values. However, it would be much simpler to just drop them as there are only a few such rows.\n\npenguins_data = penguins_data.dropna()\n\nNext, we want to encode the said labels for simplicity.\n\npenguins_data[\"species\"], species_scheme = pd.factorize(penguins_data[\"species\"])\n\nIn this case species_scheme is an array that contains the previous labels with index that corresponds to their new label. If it sounded like a bunch of jibberish, here is contents:\n\nspecies_scheme\n\nIndex(['Adelie', 'Chinstrap', 'Gentoo'], dtype='object')"
  },
  {
    "objectID": "posts/Penguin-clustering/index.html#learning-more-about-the-data",
    "href": "posts/Penguin-clustering/index.html#learning-more-about-the-data",
    "title": "Clustered Penguins",
    "section": "Learning more about the data",
    "text": "Learning more about the data\nLet’s take a look at the distribution of all the numeric values in the dataset:\n\nnumeric_cols = [\n    \"culmen_length_mm\",\n    \"culmen_depth_mm\",\n    \"flipper_length_mm\",\n    \"body_mass_g\",\n]\ngraph = pd.plotting.scatter_matrix(\n    penguins_data[numeric_cols],\n    figsize=(10, 10),\n    c=penguins_data[\"species\"],\n    label=species_scheme,\n    diagonal=\"kde\",\n)\n\n\n\n\nWe can see that there exists a pretty clear separation between the distributions of all the subsets. This matrix is a lot of data to look at - at once. Instead we can condense most of the data into two dimentions using principal component analysis (PCA). This will reduce the dimensionality of the data into two, dummy, dimentions. In essence, it will be a projection of the data from the four dimentional space into two dimentions.\nFirst, thing we want to do before that is to standardize the data. This is done fairly easily by using StandardScaler from Scikit-Learn.\n\nfrom sklearn.preprocessing import StandardScaler\nscaled_numeric_penguins_data = StandardScaler().fit_transform(penguins_data[numeric_cols])\n\nPerforming PCA is pretty straight forward as well.\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2, random_state=42)\ntwo_component_penguins_data = pca.fit_transform(scaled_numeric_penguins_data)\n\nNow that we have the projection of the data, let’s graph it.\n\nfor i, target_class in enumerate(species_scheme):\n    indices = penguins_data[\"species\"] == i\n    plt.scatter(two_component_penguins_data[indices, 0], two_component_penguins_data[indices, 1], label=target_class)\n\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x2925394d0&gt;\n\n\n\n\n\nWe can see that Gentoo are pretty distinct from the other two types of penguins. There is also some seperation between Adelies and Chinstraps’s, but it is smaller in comparison. This will make it more difficult for our clustering algorithm to distinguish between them."
  },
  {
    "objectID": "posts/Penguin-clustering/index.html#clustering",
    "href": "posts/Penguin-clustering/index.html#clustering",
    "title": "Clustered Penguins",
    "section": "Clustering",
    "text": "Clustering\nWe will try to distinguish between the penguins using K-Means-Clustering (K-Means). K-Means works by randomly initiating centers of each cluster (in our case it will initiate 3). It will then assign each point to a cluster based on which center the point is closest to. The centers of each cluster are then recalculated, and the steps are repeated until convergence or a step limit is reached. This is a good clustering algorithm for this dataset as the datapoints resemble unordered blobs with no specific shape that are somewhat distant from one another.\n\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=3, random_state=42)\npredictions = kmeans.fit_predict(scaled_numeric_penguins_data)\n\n/Users/danielsabanov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\nLet’s just verify that the labels that we got were in the range from 0 to 2.\n\nset(predictions)\n\n{0, 1, 2}\n\n\nNow, we can plot the projection of the data again, this time using the predicted labels.\n\nfor i, target_class in enumerate(set(predictions)):\n    indices = predictions == i\n    plt.scatter(two_component_penguins_data[indices, 0], two_component_penguins_data[indices, 1], label=target_class)\n\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x296cd94d0&gt;\n\n\n\n\n\nIt looks fairly similar. It also gives us an understanding which label corresponds to which species based on their distribution in the graph. We can see that 0 corresponds to chinstrap, 1 corresponds to gentoo, and 2 corresponds to adelie.\n\nclass_to_prediction = {\"adelie\": 2, \"chinstrap\": 0, \"gentoo\": 1}\n\nWe can store the data back into the dataframe for easier access.\n\npenguins_data[\"predictions\"] = predictions\n\nLet’s take a closer look at the distribtions in the data.\n\nfig1, ax1 = plt.subplots(ncols=2, figsize=(15, 7))\nfig2, ax2 = plt.subplots(ncols=2, figsize=(15, 7))\nfig3, ax3 = plt.subplots(ncols=2, figsize=(15, 7))\nfig4, ax4 = plt.subplots(ncols=2, figsize=(15, 7))\n\npenguins_data.groupby(\"species\")[\"culmen_length_mm\"].plot.kde(ax=ax1[0], title=\"Culmen Length in mm - True\")\npenguins_data.groupby(\"species\")[\"culmen_depth_mm\"].plot.kde(ax=ax2[0], title=\"Culmen Depth in mm - True\")\npenguins_data.groupby(\"species\")[\"flipper_length_mm\"].plot.kde(ax=ax3[0], title=\"Flipper Length in mm - True\")\npenguins_data.groupby(\"species\")[\"body_mass_g\"].plot.kde(ax=ax4[0], title=\"Body Mass in g - True\")\n\npenguins_data.groupby(\"predictions\")[\"culmen_length_mm\"].plot.kde(ax=ax1[1], title=\"Culmen Length in mm - Predictions\")\npenguins_data.groupby(\"predictions\")[\"culmen_depth_mm\"].plot.kde(ax=ax2[1], title=\"Culmen Depth in mm - Predictions\")\npenguins_data.groupby(\"predictions\")[\"flipper_length_mm\"].plot.kde(ax=ax3[1], title=\"Flipper Length in mm - Predictions\")\npenguins_data.groupby(\"predictions\")[\"body_mass_g\"].plot.kde(ax=ax4[1], title=\"Body Mass in g - Predictions\")\n\n\nplt.plot()\n\n[]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlternatively, we can also overlay the distributions for easier comparison.\n\nfor col in numeric_cols:\n    fig1, axes = plt.subplots(ncols=3, figsize=(15,7))\n    ax1, ax2, ax3 = axes\n    \n    penguins_data.loc[penguins_data[\"species\"] == 0][col].plot.kde(ax=ax1, linestyle=\"-.\", title=f\"{col}- Adelie vs. Prediction\")\n    penguins_data.loc[penguins_data[\"predictions\"] == class_to_prediction[\"adelie\"]][col].plot.kde(ax=ax1, linestyle=\"--\")\n    \n    penguins_data.loc[penguins_data[\"species\"] == 1][col].plot.kde(ax=ax2, linestyle=\"-.\", title=f\"{col} - Chinstrap vs. Prediction\")\n    penguins_data.loc[penguins_data[\"predictions\"] == class_to_prediction[\"chinstrap\"]][col].plot.kde(ax=ax2, linestyle=\"--\")\n    \n    penguins_data.loc[penguins_data[\"species\"] == 2][col].plot.kde(ax=ax3, linestyle=\"-.\", title=f\"{col} - Gentoo vs. Prediction\")\n    penguins_data.loc[penguins_data[\"predictions\"] == class_to_prediction[\"gentoo\"]][col].plot.kde(ax=ax3, linestyle=\"--\")\n\n    plt.plot()\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see pretty clearly in the data that one group of penguins is predicted perfectly, the genttos, as the distributions are identical, while the two others were not predicted as perfectly as the distributions do not match as well."
  },
  {
    "objectID": "posts/Penguin-clustering/index.html#trying-to-improve-the-resutls",
    "href": "posts/Penguin-clustering/index.html#trying-to-improve-the-resutls",
    "title": "Clustered Penguins",
    "section": "Trying to improve the resutls",
    "text": "Trying to improve the resutls\nWe could try to improve the previous results by also introducing the categorical data into the dataset, as previously we were primarily working with only the continous data. This will hopefully give the algorithm more data to distinguish between the groups.\nOnce again, we will encode the data.\n\nencoded = pd.get_dummies(penguins_data, columns=[\"island\", \"sex\"])\nencoded.head()\n\n\n\n\n\n\n\n\nspecies\nculmen_length_mm\nculmen_depth_mm\nflipper_length_mm\nbody_mass_g\npredictions\nisland_Biscoe\nisland_Dream\nisland_Torgersen\nsex_.\nsex_FEMALE\nsex_MALE\n\n\n\n\n0\n0\n39.1\n18.7\n181.0\n3750.0\n2\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\n\n\n1\n0\n39.5\n17.4\n186.0\n3800.0\n2\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n2\n0\n40.3\n18.0\n195.0\n3250.0\n2\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n4\n0\n36.7\n19.3\n193.0\n3450.0\n2\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n5\n0\n39.3\n20.6\n190.0\n3650.0\n2\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\n\n\n\n\n\n\n\nWe actually do not need as many columns for the categorical data. We can remove a few columns since the data for these columns can be extrapolated from the other columns.\n\nencoded = encoded.drop(columns=[\"sex_.\", \"sex_MALE\", \"island_Torgersen\", \"predictions\"])\nencoded.head()\n\n\n\n\n\n\n\n\nspecies\nculmen_length_mm\nculmen_depth_mm\nflipper_length_mm\nbody_mass_g\nisland_Biscoe\nisland_Dream\nsex_FEMALE\n\n\n\n\n0\n0\n39.1\n18.7\n181.0\n3750.0\nFalse\nFalse\nFalse\n\n\n1\n0\n39.5\n17.4\n186.0\n3800.0\nFalse\nFalse\nTrue\n\n\n2\n0\n40.3\n18.0\n195.0\n3250.0\nFalse\nFalse\nTrue\n\n\n4\n0\n36.7\n19.3\n193.0\n3450.0\nFalse\nFalse\nTrue\n\n\n5\n0\n39.3\n20.6\n190.0\n3650.0\nFalse\nFalse\nFalse\n\n\n\n\n\n\n\nOnce again, we want to scale the data.\n\nscaled_penguins_data = encoded\nscaled_penguins_data[numeric_cols] = StandardScaler().fit_transform(scaled_penguins_data[numeric_cols])\nscaled_penguins_data[\"island_Biscoe\"] = scaled_penguins_data[\"island_Biscoe\"].astype(int)\nscaled_penguins_data[\"island_Dream\"] = scaled_penguins_data[\"island_Dream\"].astype(int)\nscaled_penguins_data.head()\n\n\n\n\n\n\n\n\nspecies\nculmen_length_mm\nculmen_depth_mm\nflipper_length_mm\nbody_mass_g\nisland_Biscoe\nisland_Dream\nsex_FEMALE\n\n\n\n\n0\n0\n-0.897653\n0.783487\n-1.429521\n-0.571229\n0\n0\nFalse\n\n\n1\n0\n-0.824290\n0.121896\n-1.072408\n-0.509011\n0\n0\nTrue\n\n\n2\n0\n-0.677564\n0.427246\n-0.429605\n-1.193405\n0\n0\nTrue\n\n\n4\n0\n-1.337831\n1.088836\n-0.572450\n-0.944535\n0\n0\nTrue\n\n\n5\n0\n-0.860972\n1.750427\n-0.786718\n-0.695664\n0\n0\nFalse\n\n\n\n\n\n\n\nOnce again, let’s perform PCA to understand how the overall data was affected by the introduction of the new variables.\n\npca = PCA(n_components=2, random_state=42)\ntwo_component_penguins_data = pca.fit_transform(scaled_penguins_data)\n\nfor i, target_class in enumerate(species_scheme):\n    indices = penguins_data[\"species\"] == i\n    plt.scatter(two_component_penguins_data[indices, 0], two_component_penguins_data[indices, 1], label=target_class)\n\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x29fc214d0&gt;\n\n\n\n\n\nThis time, the difference between Adelies and Chinstraps is slightly more distinct as we can see that the Chistrap distribution is located slightly to the left of the Adelie distribution. This was not as apparent in the previous PCA run. Hopefully, we can achieve better clustering results due to this.\n\nkmeans = KMeans(n_clusters=3, random_state=42)\npredictions = kmeans.fit_predict(scaled_penguins_data.drop(columns=[\"species\"]))\n\n/Users/danielsabanov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\nLet us graph the resutls.\n\nfor i, target_class in enumerate(set(predictions)):\n    indices = predictions == i\n    plt.scatter(two_component_penguins_data[indices, 0], two_component_penguins_data[indices, 1], label=target_class)\n\nplt.legend()\n\nclass_to_prediction = {\"adelie\": 0, \"chinstrap\": 2, \"gentoo\": 1}\nencoded[\"predictions\"] = predictions\n\n\n\n\nThis time the label 0 correspnds to adelie, 1 corresponds to gentoo, and 2 corresponds to chinstrap. It appears that the classification this time went sligtly better as there is a more significant overlap between the adelie and chinstrap distributions. Be sure let’s examine the distributions in more details.\n\nfig1, ax1 = plt.subplots(ncols=2, figsize=(20, 10))\nfig2, ax2 = plt.subplots(ncols=2, figsize=(20, 10))\nfig3, ax3 = plt.subplots(ncols=2, figsize=(20, 10))\nfig4, ax4 = plt.subplots(ncols=2, figsize=(20, 10))\n\nencoded.groupby(\"species\")[\"culmen_length_mm\"].plot.kde(ax=ax1[0], title=\"Culmen Length in mm - True\")\nencoded.groupby(\"species\")[\"culmen_depth_mm\"].plot.kde(ax=ax2[0], title=\"Culmen Depth in mm - True\")\nencoded.groupby(\"species\")[\"flipper_length_mm\"].plot.kde(ax=ax3[0], title=\"Flipper Length in mm - True\")\nencoded.groupby(\"species\")[\"body_mass_g\"].plot.kde(ax=ax4[0], title=\"Body Mass in g - True\")\n\nencoded.groupby(\"predictions\")[\"culmen_length_mm\"].plot.kde(ax=ax1[1], title=\"Culmen Length in mm - Predictions\")\nencoded.groupby(\"predictions\")[\"culmen_depth_mm\"].plot.kde(ax=ax2[1], title=\"Culmen Depth in mm - Predictions\")\nencoded.groupby(\"predictions\")[\"flipper_length_mm\"].plot.kde(ax=ax3[1], title=\"Flipper Length in mm - Predictions\")\nencoded.groupby(\"predictions\")[\"body_mass_g\"].plot.kde(ax=ax4[1], title=\"Body Mass in g - Predictions\")\n\n\nplt.plot()\n\n[]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor col in numeric_cols:\n    fig1, axes = plt.subplots(ncols=3, figsize=(15,7))\n    ax1, ax2, ax3 = axes\n    \n    encoded.loc[encoded[\"species\"] == 0][col].plot.kde(ax=ax1, linestyle=\"-.\", title=f\"{col}- Adelie vs. Prediction\")\n    encoded.loc[encoded[\"predictions\"] == class_to_prediction[\"adelie\"]][col].plot.kde(ax=ax1, linestyle=\"--\")\n    \n    encoded.loc[encoded[\"species\"] == 1][col].plot.kde(ax=ax2, linestyle=\"-.\", title=f\"{col} - Chinstrap vs. Prediction\")\n    encoded.loc[encoded[\"predictions\"] == class_to_prediction[\"chinstrap\"]][col].plot.kde(ax=ax2, linestyle=\"--\")\n    \n    encoded.loc[encoded[\"species\"] == 2][col].plot.kde(ax=ax3, linestyle=\"-.\", title=f\"{col} - Gentoo vs. Prediction\")\n    encoded.loc[encoded[\"predictions\"] == class_to_prediction[\"gentoo\"]][col].plot.kde(ax=ax3, linestyle=\"--\")\n\n    plt.plot()\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis time it appears that there is a much better overlap between the real and predicted distributions of the chinstrap and adelie respenctive populations. This is actually a sensible results since sex makes a difference in the sizes of the penguins, so given that data, we are better able to tell the difference if a penguinqu is of a certain species."
  },
  {
    "objectID": "posts/Regression-wine/index.html",
    "href": "posts/Regression-wine/index.html",
    "title": "Computer Learns the Beautiful Intricacies of Wine",
    "section": "",
    "text": "Alex Tihonovs / EyeEm//Getty Images"
  },
  {
    "objectID": "posts/Regression-wine/index.html#data",
    "href": "posts/Regression-wine/index.html#data",
    "title": "Computer Learns the Beautiful Intricacies of Wine",
    "section": "Data",
    "text": "Data\nThe data we will analyze is sourced from UC Irvine’s website and originates from the study “Modeling wine preferences by data mining from physicochemical properties” [1]. This dataset showcases wines of the Vinho Verde variety, produced in the breathtaking Minho region of Portugal. Data collection for this dataset took place between May 2004 and February 2007.\nThis dataset comprises two separate files: one exclusively dedicated to white wines and the other specifically detailing red wines. Both files contain 11 fields that provide comprehensive information about the physical properties of the wines, complemented by an additional field that characterizes the wine’s quality. The quality of the wine was evaluated through a series of blind tests, yielding ratings ranging from 0 to 10.\nLet’s take a quick look at the dataset:\n\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nwhite_wine_df = pd.read_csv(\"winequality-white.csv\", sep=\";\")\nred_wine_df = pd.read_csv(\"winequality-red.csv\", sep=\";\")\nwhite_wine_df.head()\n\n\n\n\n\n\n\n\nfixed acidity\nvolatile acidity\ncitric acid\nresidual sugar\nchlorides\nfree sulfur dioxide\ntotal sulfur dioxide\ndensity\npH\nsulphates\nalcohol\nquality\n\n\n\n\n0\n7.0\n0.27\n0.36\n20.7\n0.045\n45.0\n170.0\n1.0010\n3.00\n0.45\n8.8\n6\n\n\n1\n6.3\n0.30\n0.34\n1.6\n0.049\n14.0\n132.0\n0.9940\n3.30\n0.49\n9.5\n6\n\n\n2\n8.1\n0.28\n0.40\n6.9\n0.050\n30.0\n97.0\n0.9951\n3.26\n0.44\n10.1\n6\n\n\n3\n7.2\n0.23\n0.32\n8.5\n0.058\n47.0\n186.0\n0.9956\n3.19\n0.40\n9.9\n6\n\n\n4\n7.2\n0.23\n0.32\n8.5\n0.058\n47.0\n186.0\n0.9956\n3.19\n0.40\n9.9\n6\n\n\n\n\n\n\n\n\nred_wine_df.head()\n\n\n\n\n\n\n\n\nfixed acidity\nvolatile acidity\ncitric acid\nresidual sugar\nchlorides\nfree sulfur dioxide\ntotal sulfur dioxide\ndensity\npH\nsulphates\nalcohol\nquality\n\n\n\n\n0\n7.4\n0.70\n0.00\n1.9\n0.076\n11.0\n34.0\n0.9978\n3.51\n0.56\n9.4\n5\n\n\n1\n7.8\n0.88\n0.00\n2.6\n0.098\n25.0\n67.0\n0.9968\n3.20\n0.68\n9.8\n5\n\n\n2\n7.8\n0.76\n0.04\n2.3\n0.092\n15.0\n54.0\n0.9970\n3.26\n0.65\n9.8\n5\n\n\n3\n11.2\n0.28\n0.56\n1.9\n0.075\n17.0\n60.0\n0.9980\n3.16\n0.58\n9.8\n6\n\n\n4\n7.4\n0.70\n0.00\n1.9\n0.076\n11.0\n34.0\n0.9978\n3.51\n0.56\n9.4\n5\n\n\n\n\n\n\n\n\nimport numpy as np\nplt.figure(figsize=(10,5))\nbin_edges = np.arange(11) - 0.5\nwhite_wine_df[\"quality\"].plot(kind=\"hist\", alpha=0.5, bins=bin_edges, edgecolor=\"k\", density=True)\nred_wine_df[\"quality\"].plot(kind=\"hist\", alpha=0.5, bins=bin_edges, edgecolor=\"k\", density=True)\nplt.legend([\"White\", \"Red\"])\nplt.xticks(range(10))\nplt.xlim([-1, 10])\n\n(-1.0, 10.0)\n\n\n\n\n\nThe quality distribution of the wines seems to approximate a normal distribution. Additionally, there is a noticeable rightward shift in the distribution of white wine quality compared to red wine, suggesting that, on average, white wine may receive higher rankings. To examine the distributions of other columns in the data—given their continuous nature—it would be more appropriate to utilize Kernel Density Estimation (KDE) instead.\n\nfor column in red_wine_df.columns:\n    if column != \"quality\":\n        plt.figure()    \n        white_wine_df[column].plot(kind=\"kde\", label=\"White\")\n        red_wine_df[column].plot(kind=\"kde\", label=\"Red\")\n        plt.legend()\n        plt.title(column)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe distributions of the independent variables related to wine quality show noticeable variations. Hence, it would be more sensible to develop separate models for each type of wine rather than trying to merge the data into a single dataframe, at least when we are dealing with simpler models."
  },
  {
    "objectID": "posts/Regression-wine/index.html#predicting-wine-quality",
    "href": "posts/Regression-wine/index.html#predicting-wine-quality",
    "title": "Computer Learns the Beautiful Intricacies of Wine",
    "section": "Predicting Wine Quality",
    "text": "Predicting Wine Quality\n\nLinear Model\nHaving reviewed the data, our next step is to construct a model that provides a reliable estimate of wine quality. Beginning with the simplest technique, let’s explore simple linear regression. To accomplish this, we’ll identify the variables that show the strongest correlation with wine quality. For visualization purposes, we’ll leverage Seaborn, a high-level API for the Matplotlib library, streamlining the process of plotting specific graphs.\n\nimport seaborn as sns\nplt.figure(figsize=(10,10))\nsns.heatmap(white_wine_df.corr(), annot=True).set(title=\"White Wine Correlation\")\nplt.show()\nplt.figure(figsize=(10,10))\nsns.heatmap(red_wine_df.corr(), annot=True).set(title=\"Red Wine Correlation\")\nplt.show()\n\n\n\n\n\n\n\nInterestingly, it seems that alcohol concentration is the variable most correlated with wine quality. However, given the relatively weak correlation, I’m not overly optimistic about the prospects of this model. Before moving forward, it’s crucial to confirm the presence of a linear relationship between the variables.\n\nplt.scatter(\"alcohol\", \"quality\", data=white_wine_df, alpha=0.01)\nplt.title(\"White Wine Alchohol Content vs. Quality\")\n\nplt.figure()\nplt.scatter(\"alcohol\", \"quality\", data=red_wine_df, alpha=0.01)\nplt.title(\"Red Wine Alchohol Content vs. Quality\")\n\nText(0.5, 1.0, 'Red Wine Alchohol Content vs. Quality')\n\n\n\n\n\n\n\n\nThe relationship between the variables doesn’t seem to follow a linear pattern. Nevertheless, for the sake of exploration, we can attempt to fit a linear model to it. To start, we’ll divide the data into a training set and a test set.\n\nfrom sklearn.model_selection import train_test_split\nX_white = white_wine_df[\"alcohol\"].to_numpy().reshape(-1, 1) # Reshaped for a single feature\ny_white = white_wine_df[\"quality\"].to_numpy().reshape(-1, 1)\nX_train_white, X_test_white, y_train_white, y_test_white = train_test_split(X_white, y_white, test_size=0.2, random_state=42)\n\nX_red = red_wine_df[\"alcohol\"].to_numpy().reshape(-1, 1)\ny_red = red_wine_df[\"quality\"].to_numpy().reshape(-1, 1)\n\nX_train_red, X_test_red, y_train_red, y_test_red = train_test_split(X_red, y_red, test_size=0.2, random_state=42)\n\n\nfrom sklearn.linear_model import LinearRegression\nwhite_linear_regression_model = LinearRegression()\nwhite_linear_regression_model.fit(X_train_white, y_train_white)\n\nred_linear_regression_model = LinearRegression()\nred_linear_regression_model.fit(X_train_red, y_train_red)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nHaving fitted the model, let’s now assess its predictive performance. In this case, we’ll evaluate its accuracy using metrics such as Mean Squared Error (MSE) and Root Mean Squared Error (RMSE).\n\nfrom sklearn.metrics import mean_squared_error\nwhite_predictions = white_linear_regression_model.predict(X_test_white)\nred_predictions = red_linear_regression_model.predict(X_test_red)\n\nwhite_error = mean_squared_error(y_test_white, white_predictions)\nred_error = mean_squared_error(y_test_red, red_predictions)\n\nprint(f\"The MSE for white wine is {white_error} and RMSE of {white_error ** 0.5}\")\nprint(f\"The MSE for red wine is {red_error} and RMSE of {red_error ** 0.5}\")\n\nThe MSE for white wine is 0.6177518759003805 and RMSE of 0.7859719307331404\nThe MSE for red wine is 0.4995281340730445 and RMSE of 0.7067730428313211\n\n\nAt first glance, the RMSE of 0.78 may seem promising. However, it’s important to consider that the data is heavily concentrated on a few data points. Let’s explore a more robust technique—multiple linear regression.\nMultiple linear regression, akin to linear regression, involves using multiple variables to model the data. The next significant variables correlating with red wine quality are volatile acidity and density in white wine. We can fit a model in a similar fashion.\nFirstly, we divide the data into a test and train set, mirroring the previous procedure. However, this time, we include “volatile acidity” as part of our predictor variables (X) for red wine and “density” for white wine.\n\nX_white = white_wine_df[[\"alcohol\", \"density\"]].to_numpy()\ny_white = white_wine_df[\"quality\"].to_numpy()\nX_train_white, X_test_white, y_train_white, y_test_white = train_test_split(X_white, y_white, test_size=0.2, random_state=42)\n\nX_red = red_wine_df[[\"alcohol\", \"volatile acidity\"]].to_numpy()\ny_red = red_wine_df[\"quality\"].to_numpy()\n\nX_train_red, X_test_red, y_train_red, y_test_red = train_test_split(X_red, y_red, test_size=0.2, random_state=42)\n\nThe sklearn linear regression model supports multiple linear regression right out of the box. Therefore, the next code block doesn’t introduce any new, ground-breaking, concepts. We simply supply it with a slightly modified X, incorporating the additional predictor variables.\n\nwhite_linear_regression_model = LinearRegression()\nwhite_linear_regression_model.fit(X_train_white, y_train_white)\n\nred_linear_regression_model = LinearRegression()\nred_linear_regression_model.fit(X_train_red, y_train_red)\n\nwhite_predictions = white_linear_regression_model.predict(X_test_white)\nred_predictions = red_linear_regression_model.predict(X_test_red)\n\nwhite_error = mean_squared_error(y_test_white, white_predictions)\nred_error = mean_squared_error(y_test_red, red_predictions)\n\nprint(f\"The MSE for white wine is {white_error} and RMSE of {white_error ** 0.5}\")\nprint(f\"The MSE for red wine is {red_error} and RMSE of {red_error ** 0.5}\")\n\nThe MSE for white wine is 0.6160746073023867 and RMSE of 0.7849042026275479\nThe MSE for red wine is 0.41775613260831734 and RMSE of 0.646340570139549\n\n\nWhen provided with more than one variable, the linear regression model demonstrated improved fitting capabilities on the red wine dataset. However, the RMSE for the white wine dataset remained unaltered, and overall, the RMSE remains relatively high. Perhaps it’s worth considering a more potent approach to address the issue.\n\n\nRandom Forrest\nThe random forest model is significantly more potent than linear regression, albeit at the expense of managing more hyperparameters and being more computationally expensive. To achieve the optimal model, we’ll need to fine-tune some of these hyperparameters through a grid search. Although computationally more intensive, this approach ensures the selection of the best hyperparameters for the specific dataset.\n\nX_white = white_wine_df.loc[:, white_wine_df.columns != \"quality\"].to_numpy()\ny_white = white_wine_df[\"quality\"].to_numpy()\nX_train_white, X_test_white, y_train_white, y_test_white = train_test_split(X_white, y_white, test_size=0.2, random_state=42)\n\nX_red = red_wine_df.loc[:, white_wine_df.columns != \"quality\"].to_numpy()\ny_red = red_wine_df[\"quality\"].to_numpy()\nX_train_red, X_test_red, y_train_red, y_test_red = train_test_split(X_red, y_red, test_size=0.2, random_state=42)\n\nHaving re-split our data, we can now perform the grid search. The parameters slated for tuning are n_estimators and max_depth. n_estimators governs the number of trees in the forest, while max_depth regulates the maximum depth of each tree.\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nparameters = {\"n_estimators\": list(range(1,14)), \n              \"max_depth\": list(range(5, 20)) + [None]} # None corresponds to no limit on the depth.\nclf_white = GridSearchCV(RandomForestRegressor(), param_grid=parameters, n_jobs=8)\nwhite_tuned_random_forrest = clf_white.fit(X_train_white, y_train_white)\n\nclf_red = GridSearchCV(RandomForestRegressor(), param_grid=parameters, n_jobs=8)\nred_tuned_random_forrest = clf_red.fit(X_train_red, y_train_red)\n\nNow that we have a model for each dataset, let’s assess their performance.\n\nwhite_predictions = white_tuned_random_forrest.predict(X_test_white)\nred_predictions = red_tuned_random_forrest.predict(X_test_red)\n\nwhite_error = mean_squared_error(y_test_white, white_predictions)\nred_error = mean_squared_error(y_test_red, red_predictions)\n\nprint(f\"The MSE for white wine is {white_error} and RMSE of {white_error ** 0.5}\")\nprint(f\"The MSE for red wine is {red_error} and RMSE of {red_error ** 0.5}\")\n\nThe MSE for white wine is 0.3733305156382079 and RMSE of 0.611007786888357\nThe MSE for red wine is 0.33291420118343196 and RMSE of 0.5769871759263215\n\n\nAt the cost of higher training time, the model we’ve developed is substantially more accurate than the linear regression model. To the extent that I’m quite confident we could produce a robust model without separating the white and red wine datasets.\nLet’s combine our datasets and then split them for training.\n\ncombined_df = pd.concat([white_wine_df, red_wine_df])\nX_combined = combined_df.loc[:, combined_df.columns != \"quality\"].to_numpy()\ny_combined = combined_df[\"quality\"].to_numpy()\n\nX_train_combined, X_test_combined, y_train_combined, y_test_combined = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)\n\n\nparameters = {\"n_estimators\": list(range(1,14)), \n              \"max_depth\": list(range(5, 20)) + [None]} # None corresponds to no limit on the depth.\nclf_combined = GridSearchCV(RandomForestRegressor(), param_grid=parameters, n_jobs=8)\ncombined_tuned_random_forrest = clf_combined.fit(X_train_combined, y_train_combined)\n\n\ncombined_prediction = combined_tuned_random_forrest.predict(X_test_combined)\ncombined_error = mean_squared_error(y_test_combined, combined_prediction)\nprint(f\"The MSE for white wine is {combined_error} and RMSE of {combined_error ** 0.5}\")\n\nThe MSE for white wine is 0.35226136041273964 and RMSE of 0.5935160995396331\n\n\nEven with the combined dataset, it’s apparent that our model can effectively distinguish between white and red wine, assigning a quality score that outperforms the simple linear regression model."
  },
  {
    "objectID": "posts/Regression-wine/index.html#references",
    "href": "posts/Regression-wine/index.html#references",
    "title": "Computer Learns the Beautiful Intricacies of Wine",
    "section": "References",
    "text": "References\n[1] P. Cortez, et al. “Modeling wine preferences by data mining from physicochemical properties,” in Decis. Support Syst., vol. 47, pp. 547-553, 2009."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rock with Lightning",
    "section": "",
    "text": "Welcome to Rock With Lightning\nHello there! Odds are you are here because you are either my professor, or a TA for CS5805, in which case hello and welcome. You will be able to find the blog posts right below the welcome section.\nOn the off chance that you are neither, how did you get here? Not that you are unwelcome. Just questioning the series of events that brought you on a Github page lead by a random college student. You should know, this is a blog that I lead for my machine learning class as part of a class assignment. The posts that you will see here will primary deal with machine learning related topics that I find interesting either inside or outside of class. If this is the sort of content you are interested in, welcome!\n\n\nPosts\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nNaive Bayes with Diabetes Dataset\n\n\n\n\n\n\n\nNaive Bayes\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2023\n\n\nDaniel Sabanov\n\n\n\n\n\n\n  \n\n\n\n\nClustered Penguins\n\n\n\n\n\n\n\nClustering\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nDaniel Sabanov\n\n\n\n\n\n\n  \n\n\n\n\nNeural Networks and Images\n\n\n\n\n\n\n\nNeural Networks\n\n\nImage Classification\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2023\n\n\nDaniel Sabanov\n\n\n\n\n\n\n  \n\n\n\n\nKNN for Text Classification\n\n\n\n\n\n\n\nKNN\n\n\nNLP\n\n\nOutlier Detection\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2023\n\n\nDaniel Sabanov\n\n\n\n\n\n\n  \n\n\n\n\nComputer Learns the Beautiful Intricacies of Wine\n\n\n\n\n\n\n\nRegression\n\n\nRandom Forest\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2023\n\n\nDaniel Sabanov\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Outlier-Detection/index.html",
    "href": "posts/Outlier-Detection/index.html",
    "title": "Machine Learning and Fraud",
    "section": "",
    "text": "Taken from https://blog.hubspot.com/sales/ach-vs-credit-card\n\n\nCredit card fraud is a serious issue that many credit card companies and banks are required to deal with in order to guarantee their customers’ financial safety. However, detecting credit card fraud is difficult. When training an ML model one wants to have a good sample size that represents all the possible outcomes, but it is difficult to collect the required ammount of data in order to have a good representative sample of both fraudulant and non-fraudulant transactions as fraudulant transactions are much less common. In this cases, Data Scientists and ML Engineers need to consider a different appraoch to building models to detect bad transactions. Luckily, there are various ML techniques that can help with the task. These thechniques sit under the umbrella of anomally detection.\nIn the following blog, we will be showing an application of two annomally detection techniques on the credit card fraud dataset.\n\n\nThe dataset contains data from over a quarter of a million transactions made by credit cards in September of 2013 made by European cardholders. The dataset was normalized and transformed via PCA with its fields also being encrypted in order to conceal the personal information about the cardholders. Out of a quarter of a million transactions, only 492 transactions were fraudulant. This is a highly imbalanced dataset, and more common machine learning techniques will struggle with reliably being able to detect the bad transactions.\nLet us load the dataset.\n\nimport pandas as pd\ndata = pd.read_csv(\"creditcard.csv\")\ndata.head()\n\n\n\n\n\n\n\n\nTime\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\n...\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\nAmount\nClass\n\n\n\n\n0\n0.0\n-1.359807\n-0.072781\n2.536347\n1.378155\n-0.338321\n0.462388\n0.239599\n0.098698\n0.363787\n...\n-0.018307\n0.277838\n-0.110474\n0.066928\n0.128539\n-0.189115\n0.133558\n-0.021053\n149.62\n0\n\n\n1\n0.0\n1.191857\n0.266151\n0.166480\n0.448154\n0.060018\n-0.082361\n-0.078803\n0.085102\n-0.255425\n...\n-0.225775\n-0.638672\n0.101288\n-0.339846\n0.167170\n0.125895\n-0.008983\n0.014724\n2.69\n0\n\n\n2\n1.0\n-1.358354\n-1.340163\n1.773209\n0.379780\n-0.503198\n1.800499\n0.791461\n0.247676\n-1.514654\n...\n0.247998\n0.771679\n0.909412\n-0.689281\n-0.327642\n-0.139097\n-0.055353\n-0.059752\n378.66\n0\n\n\n3\n1.0\n-0.966272\n-0.185226\n1.792993\n-0.863291\n-0.010309\n1.247203\n0.237609\n0.377436\n-1.387024\n...\n-0.108300\n0.005274\n-0.190321\n-1.175575\n0.647376\n-0.221929\n0.062723\n0.061458\n123.50\n0\n\n\n4\n2.0\n-1.158233\n0.877737\n1.548718\n0.403034\n-0.407193\n0.095921\n0.592941\n-0.270533\n0.817739\n...\n-0.009431\n0.798278\n-0.137458\n0.141267\n-0.206010\n0.502292\n0.219422\n0.215153\n69.99\n0\n\n\n\n\n5 rows × 31 columns\n\n\n\n\nfrom matplotlib import pyplot as plt\ny = data[\"Class\"]\nX = data.drop(columns = [\"Class\"])\nfor column in X.columns:\n    fig1, ax1 = plt.subplots()\n    data[column].plot(kind=\"kde\", title=column, ax=ax1)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that the dat ain most columns appears to be normally distributed, with a few columns appearing to be multimodal.\nWe can also look at the distributions of the fraudulant transactions and the non-fraudulant transactions together.\n\nfrom matplotlib import pyplot as plt\nfor column in X.columns:\n    fig1, ax1 = plt.subplots()\n    data[column].loc[data[\"Class\"] == 0].plot(kind=\"kde\", title=column, ax=ax1)\n    data[column].loc[data[\"Class\"] == 1].plot(kind=\"kde\", title=column, ax=ax1)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe see that the distributions of the fraudulant transactions and the non-fraudlant transactions differ from one another.\nWe can also look at the two dimentional projection of the data.\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\npipeline = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"pca\", PCA(n_components=2)),\n])\n\npca_data = pd.DataFrame(\n    pipeline.fit_transform(X),\n    columns=[\"PC1\", \"PC2\"],\n    index=data.index,\n)\n\nplt.scatter(pca_data[\"PC1\"], pca_data[\"PC2\"], c=y, cmap=\"viridis\", alpha=0.1)\nplt.title(\"Projection of the Transaction Data\")\n\nText(0.5, 1.0, 'Projection of the Transaction Data')\n\n\n\n\n\nHere, it is a bit difficult to tell appart the fraudlant and non-fraudulant transactions as the fraudulant transactions are blended in with the non-fraudulant ones. Let us try to build a model in order to differentiate between the two.\n\n\n\n\n\nFor the first approach we will be using local outlier factor in order to detect transactions that would be considered outliers. This method works by examining the the surrounding neighbors of the datapoint and seeing if the current datapoints deviates significantly from its neighbors. We can ran through the model fairly easily using the Scikit-Learn library.\n\nfrom sklearn.neighbors import LocalOutlierFactor\nloc = LocalOutlierFactor()\nprediction = loc.fit_predict(X)\n\nNote that we did not need to train the model. The model is unsupervised and tries to learn from the data by itself.\nIt is also important to note that the model outputs are integers \\(1\\) and \\(-1\\). \\(1\\) indicates that a specific datapoint is not an outlier, while \\(-1\\) indicates that the datapoint is in fact an outlier.\nIn order to be able to diagnose the model we want to convert our dataset to the same scheme that our model uses. So, we convert \\(0\\)’s into \\(1\\)’s and \\(1\\)’s into \\(-1\\)’s.\n\ny.replace(1, -1, inplace=True)\ny.replace(0, 1, inplace=True)\n\nNow, we can examine the model performance:\n\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, f1_score\n\nconfusion = confusion_matrix(y, prediction)\nsns.heatmap(\n    confusion,\n    annot=True,\n    fmt=\"d\",\n    cmap=\"Blues\",\n    xticklabels=[\"Predicted -1\", \"Predicted 1\"],\n    yticklabels=[\"Actual -1\", \"Actual 1\"],\n)\nprint(\"f1_score: \", f1_score(y, prediction))\n\nf1_score:  0.9918557547944818\n\n\n\n\n\nDespite having a high f1 score, the model actually performed very poorly since our recall (or sensitivity), the ability to predict true positives, is very low. In our case it is \\(\\frac{99}{99 + 393}\\). Meaning, we were not able to reliably detect the fraudulant transactions.\n\n\n\nThe second approach we will try are isolation forests. Isolation forests are a very common technique that is used in anomally detection and outlier detection due their robustness in the presence of irrelevant attributes and their high efficiency. The way that isolation forests work is by constructing multiple isolation trees that then average their scores. Each isolation tree is constructed by partitioning the data accross all of its features until each datapoint is located in its own leaf node. Then, the distance to each datapoint is measured. The shorter the distance, the more likely is the datapoint to be an outlier. This distance is also the score that is being averaged out.\nOnce again, creating an isolation forest is made easy with scikit-learn.\n\nfrom sklearn.ensemble import IsolationForest\nforest = IsolationForest(random_state=42)\nprediction = forest.fit_predict(X)\n\n\nconfusion = confusion_matrix(y, prediction)\nsns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\",\n            xticklabels=['Predicted -1', 'Predicted 1'],\n            yticklabels=['Actual -1', 'Actual 1'])\n\n&lt;Axes: &gt;\n\n\n\n\n\nThis model has perfomed significantly better. We were able to find much higher percentage of fraudulant transactions. It is true that there were also much more false possitives outputed by this model. However, in cases such as this, it is much better to have a high false positive rate (also called low precision) since it is better to be overly cautios in these scenarios. This is why credit companies would sometimes block your credit card transactions due to suspected fraud. Thier model most likely has had a false positive, but it was much better to be safe than sorry."
  },
  {
    "objectID": "posts/Outlier-Detection/index.html#dataset",
    "href": "posts/Outlier-Detection/index.html#dataset",
    "title": "Machine Learning and Fraud",
    "section": "",
    "text": "The dataset contains data from over a quarter of a million transactions made by credit cards in September of 2013 made by European cardholders. The dataset was normalized and transformed via PCA with its fields also being encrypted in order to conceal the personal information about the cardholders. Out of a quarter of a million transactions, only 492 transactions were fraudulant. This is a highly imbalanced dataset, and more common machine learning techniques will struggle with reliably being able to detect the bad transactions.\nLet us load the dataset.\n\nimport pandas as pd\ndata = pd.read_csv(\"creditcard.csv\")\ndata.head()\n\n\n\n\n\n\n\n\nTime\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\n...\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\nAmount\nClass\n\n\n\n\n0\n0.0\n-1.359807\n-0.072781\n2.536347\n1.378155\n-0.338321\n0.462388\n0.239599\n0.098698\n0.363787\n...\n-0.018307\n0.277838\n-0.110474\n0.066928\n0.128539\n-0.189115\n0.133558\n-0.021053\n149.62\n0\n\n\n1\n0.0\n1.191857\n0.266151\n0.166480\n0.448154\n0.060018\n-0.082361\n-0.078803\n0.085102\n-0.255425\n...\n-0.225775\n-0.638672\n0.101288\n-0.339846\n0.167170\n0.125895\n-0.008983\n0.014724\n2.69\n0\n\n\n2\n1.0\n-1.358354\n-1.340163\n1.773209\n0.379780\n-0.503198\n1.800499\n0.791461\n0.247676\n-1.514654\n...\n0.247998\n0.771679\n0.909412\n-0.689281\n-0.327642\n-0.139097\n-0.055353\n-0.059752\n378.66\n0\n\n\n3\n1.0\n-0.966272\n-0.185226\n1.792993\n-0.863291\n-0.010309\n1.247203\n0.237609\n0.377436\n-1.387024\n...\n-0.108300\n0.005274\n-0.190321\n-1.175575\n0.647376\n-0.221929\n0.062723\n0.061458\n123.50\n0\n\n\n4\n2.0\n-1.158233\n0.877737\n1.548718\n0.403034\n-0.407193\n0.095921\n0.592941\n-0.270533\n0.817739\n...\n-0.009431\n0.798278\n-0.137458\n0.141267\n-0.206010\n0.502292\n0.219422\n0.215153\n69.99\n0\n\n\n\n\n5 rows × 31 columns\n\n\n\n\nfrom matplotlib import pyplot as plt\ny = data[\"Class\"]\nX = data.drop(columns = [\"Class\"])\nfor column in X.columns:\n    fig1, ax1 = plt.subplots()\n    data[column].plot(kind=\"kde\", title=column, ax=ax1)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that the dat ain most columns appears to be normally distributed, with a few columns appearing to be multimodal.\nWe can also look at the distributions of the fraudulant transactions and the non-fraudulant transactions together.\n\nfrom matplotlib import pyplot as plt\nfor column in X.columns:\n    fig1, ax1 = plt.subplots()\n    data[column].loc[data[\"Class\"] == 0].plot(kind=\"kde\", title=column, ax=ax1)\n    data[column].loc[data[\"Class\"] == 1].plot(kind=\"kde\", title=column, ax=ax1)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe see that the distributions of the fraudulant transactions and the non-fraudlant transactions differ from one another.\nWe can also look at the two dimentional projection of the data.\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\npipeline = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"pca\", PCA(n_components=2)),\n])\n\npca_data = pd.DataFrame(\n    pipeline.fit_transform(X),\n    columns=[\"PC1\", \"PC2\"],\n    index=data.index,\n)\n\nplt.scatter(pca_data[\"PC1\"], pca_data[\"PC2\"], c=y, cmap=\"viridis\", alpha=0.1)\nplt.title(\"Projection of the Transaction Data\")\n\nText(0.5, 1.0, 'Projection of the Transaction Data')\n\n\n\n\n\nHere, it is a bit difficult to tell appart the fraudlant and non-fraudulant transactions as the fraudulant transactions are blended in with the non-fraudulant ones. Let us try to build a model in order to differentiate between the two."
  },
  {
    "objectID": "posts/Outlier-Detection/index.html#training-the-models",
    "href": "posts/Outlier-Detection/index.html#training-the-models",
    "title": "Machine Learning and Fraud",
    "section": "",
    "text": "For the first approach we will be using local outlier factor in order to detect transactions that would be considered outliers. This method works by examining the the surrounding neighbors of the datapoint and seeing if the current datapoints deviates significantly from its neighbors. We can ran through the model fairly easily using the Scikit-Learn library.\n\nfrom sklearn.neighbors import LocalOutlierFactor\nloc = LocalOutlierFactor()\nprediction = loc.fit_predict(X)\n\nNote that we did not need to train the model. The model is unsupervised and tries to learn from the data by itself.\nIt is also important to note that the model outputs are integers \\(1\\) and \\(-1\\). \\(1\\) indicates that a specific datapoint is not an outlier, while \\(-1\\) indicates that the datapoint is in fact an outlier.\nIn order to be able to diagnose the model we want to convert our dataset to the same scheme that our model uses. So, we convert \\(0\\)’s into \\(1\\)’s and \\(1\\)’s into \\(-1\\)’s.\n\ny.replace(1, -1, inplace=True)\ny.replace(0, 1, inplace=True)\n\nNow, we can examine the model performance:\n\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, f1_score\n\nconfusion = confusion_matrix(y, prediction)\nsns.heatmap(\n    confusion,\n    annot=True,\n    fmt=\"d\",\n    cmap=\"Blues\",\n    xticklabels=[\"Predicted -1\", \"Predicted 1\"],\n    yticklabels=[\"Actual -1\", \"Actual 1\"],\n)\nprint(\"f1_score: \", f1_score(y, prediction))\n\nf1_score:  0.9918557547944818\n\n\n\n\n\nDespite having a high f1 score, the model actually performed very poorly since our recall (or sensitivity), the ability to predict true positives, is very low. In our case it is \\(\\frac{99}{99 + 393}\\). Meaning, we were not able to reliably detect the fraudulant transactions.\n\n\n\nThe second approach we will try are isolation forests. Isolation forests are a very common technique that is used in anomally detection and outlier detection due their robustness in the presence of irrelevant attributes and their high efficiency. The way that isolation forests work is by constructing multiple isolation trees that then average their scores. Each isolation tree is constructed by partitioning the data accross all of its features until each datapoint is located in its own leaf node. Then, the distance to each datapoint is measured. The shorter the distance, the more likely is the datapoint to be an outlier. This distance is also the score that is being averaged out.\nOnce again, creating an isolation forest is made easy with scikit-learn.\n\nfrom sklearn.ensemble import IsolationForest\nforest = IsolationForest(random_state=42)\nprediction = forest.fit_predict(X)\n\n\nconfusion = confusion_matrix(y, prediction)\nsns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\",\n            xticklabels=['Predicted -1', 'Predicted 1'],\n            yticklabels=['Actual -1', 'Actual 1'])\n\n&lt;Axes: &gt;\n\n\n\n\n\nThis model has perfomed significantly better. We were able to find much higher percentage of fraudulant transactions. It is true that there were also much more false possitives outputed by this model. However, in cases such as this, it is much better to have a high false positive rate (also called low precision) since it is better to be overly cautios in these scenarios. This is why credit companies would sometimes block your credit card transactions due to suspected fraud. Thier model most likely has had a false positive, but it was much better to be safe than sorry."
  }
]